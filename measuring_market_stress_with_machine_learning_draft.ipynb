{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3eQc70WsVdx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# Algorithmic Monitoring: Measuring Market Stress with Machine Learning\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2602.07066-b31b1b.svg)](https://arxiv.org/abs/2602.07066)\n",
        "[![Journal](https://img.shields.io/badge/Journal-ArXiv%20Preprint-003366)](https://arxiv.org/abs/2602.07066)\n",
        "[![Year](https://img.shields.io/badge/Year-2026-purple)](https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Financial%20Econometrics%20%7C%20Machine%20Learning-00529B)](https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-CRSP%20%7C%20WRDS-lightgrey)](https://wrds-www.wharton.upenn.edu/)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Lasso--Logit%20%7C%20Expanding%20Window-orange)](https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Probabilistic%20Forecasting-red)](https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning)\n",
        "[![Validation](https://img.shields.io/badge/Validation-Brier%20Score%20%7C%20ECE%20%7C%20AUC-green)](https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning)\n",
        "[![Robustness](https://img.shields.io/badge/Robustness-Block%20Bootstrap%20%7C%20Nonlinear%20Benchmarks-yellow)](https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![YAML](https://img.shields.io/badge/YAML-%23CB171E.svg?style=flat&logo=yaml&logoColor=white)](https://yaml.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![Open Source](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen)](https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning`\n",
        "\n",
        "**Owner:** 2026 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2026 paper entitled **\"Algorithmic Monitoring: Measuring Market Stress with Machine Learning\"** by:\n",
        "\n",
        "*   **Marc Schmitt** (University of Oxford)\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from the ingestion and rigorous validation of CRSP micro-data to the construction of the Market Stress Probability Index (MSPI) using L1-regularized logistic regression, culminating in comprehensive out-of-sample evaluation and structural economic analysis.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `execute_complete_mspi_research_pipeline`](#key-callable-execute_complete_mspi_research_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Schmitt (2026). The core of this repository is the iPython Notebook `measuring_market_stress_with_machine_learning_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline addresses the critical challenge of **real-time market stress monitoring** in modern algorithmic markets, where fragility manifests in the cross-section of returns and trading activity.\n",
        "\n",
        "The paper proposes the **Market Stress Probability Index (MSPI)**, a forward-looking probability measure constructed from interpretable cross-sectional fragility signals. This codebase operationalizes the proposed solution:\n",
        "-   **Validates** data integrity using strict schema checks and temporal consistency enforcement.\n",
        "-   **Engineers** cross-sectional fragility features (moments, tails, liquidity) from high-frequency daily data.\n",
        "-   **Learns** a sparse probability mapping using Lasso-Logit in a strict real-time expanding window.\n",
        "-   **Evaluates** performance via proper scoring rules (Brier, Log Loss), calibration diagnostics, and robustness horse races against nonlinear learners (Random Forest, Gradient Boosting).\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods combine techniques from Financial Econometrics, Machine Learning, and Probabilistic Forecasting.\n",
        "\n",
        "**1. Cross-Sectional Fragility Signals:**\n",
        "Stress leaves a footprint in the cross-section of returns. The model aggregates daily statistics into monthly features $X_t$:\n",
        "$$ Z_t \\equiv \\frac{1}{D_t} \\sum_{d \\in t} z_d $$\n",
        "where $z_d$ includes dispersion $\\sigma^{xs}_d$, skewness, kurtosis, and tail participation $\\text{Frac}^{dn}_d$.\n",
        "\n",
        "**2. Latent Stress Definition:**\n",
        "A month is labeled as stress ($S_t=1$) if market returns crash or realized volatility spikes relative to a dynamic history:\n",
        "$$ S_t \\equiv \\mathbb{I} \\{ R^{mkt}_t \\le c_R \\} \\lor \\mathbb{I} \\{ \\sigma^{mkt}_t \\ge q_{t-1}(\\alpha) \\} $$\n",
        "\n",
        "**3. Sparse Probability Modeling (Lasso-Logit):**\n",
        "The probability of future stress is modeled using L1-regularized logistic regression to ensure parsimony and interpretability:\n",
        "$$ MSPI_t \\equiv \\Pr(Y_{t+1} = 1 | X_t) = \\Lambda(\\beta_0 + X'_t\\beta) $$\n",
        "$$ (\\hat{\\beta}_0, \\hat{\\beta}) \\in \\arg\\min_{\\beta_0,\\beta} \\left\\{ - \\ell(\\beta) + \\lambda\\|\\beta\\|_1 \\right\\} $$\n",
        "\n",
        "**4. Real-Time Discipline:**\n",
        "The pipeline enforces a strict **expanding window** protocol. At time $t$, the model is trained only on information available up to $t$, preventing look-ahead bias in both feature engineering (standardization) and model estimation.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`measuring_market_stress_with_machine_learning_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The pipeline is decomposed into 22 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters (grids, splits, hyperparameters) are managed in an external `config.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks schema integrity, temporal monotonicity, and return plausibility.\n",
        "-   **Deterministic Execution:** Enforces reproducibility through seed control, strict causality checks, and frozen parameter sets.\n",
        "-   **Comprehensive Audit Logging:** Generates detailed logs of every processing step, including invariant checks and benchmark comparisons.\n",
        "-   **Reproducible Artifacts:** Generates structured results containing raw time-series, aggregated metrics, and economic analysis outputs.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Configuration & Validation (Task 1):** Loads and validates the study configuration, enforcing parameter constraints and reproduction modes.\n",
        "\n",
        "2.  **Data Ingestion & Cleansing (Tasks 2-5):** Validates CRSP micro and macro schema, enforces strict monotonicity, handles missingness, and aligns calendars.\n",
        "\n",
        "3.  **Universe Construction (Task 6):** Filters for common shares on major exchanges with price > $1.\n",
        "\n",
        "4.  **Feature Engineering (Tasks 7-10):** Computes daily cross-sectional moments, tail measures, and trading proxies, aggregating them to monthly frequency.\n",
        "\n",
        "5.  **Target Construction (Tasks 11-12):** Computes market aggregates and constructs the binary stress label $S_t$ using dynamic volatility quantiles.\n",
        "\n",
        "6.  **Standardization (Task 13):** Implements expanding-window z-scoring to prevent data leakage.\n",
        "\n",
        "7.  **Hyperparameter Tuning (Task 14):** Optimizes regularization parameters ($\\lambda$) using time-series cross-validation in the initial window.\n",
        "\n",
        "8.  **Forecasting (Tasks 15-16):** Generates out-of-sample probability forecasts for MSPI (Lasso) and the Benchmark (Ridge).\n",
        "\n",
        "9.  **Robustness (Task 18):** Runs a horse race against Random Forest and Gradient Boosting models with real-time Platt calibration.\n",
        "\n",
        "10. **Evaluation (Tasks 19-21):** Computes AUC, PR-AUC, Brier Score, Log Loss, ECE, and performs block-bootstrap inference.\n",
        "\n",
        "11. **Economic Analysis (Task 22):** Estimates predictive regressions for volatility and impulse response functions via local projections.\n",
        "\n",
        "12. **Final Orchestration (Task 22-Plus):** Manages the end-to-end flow and enforces anti-look-ahead safety checks.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The notebook is structured as a logical pipeline with modular orchestrator functions for each of the 22 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `execute_complete_mspi_research_pipeline`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_complete_mspi_research_pipeline`:** This master orchestrator function runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, managing data flow between validation, feature engineering, modeling, evaluation, and economic analysis modules.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `pyyaml`, `scikit-learn`, `statsmodels`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning.git\n",
        "    cd measuring_market_stress_with_machine_learning\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy pyyaml scikit-learn statsmodels\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two primary DataFrames:\n",
        "\n",
        "1.  **`df_crsp_daily` (Micro-Data)**:\n",
        "    -   `PERMNO` (Int64): Security identifier.\n",
        "    -   `DATE` (datetime64[ns]): Trading date.\n",
        "    -   `SHRCD`, `EXCHCD` (Int64): Share and exchange codes.\n",
        "    -   `PRC`, `RET`, `VOL`, `SHROUT` (float64): Price, return, volume, shares outstanding.\n",
        "\n",
        "2.  **`df_crsp_index` (Macro-Data)**:\n",
        "    -   `DATE` (datetime64[ns]): Trading date.\n",
        "    -   `vwretd` (float64): Value-weighted market return.\n",
        "\n",
        "*Note: The pipeline includes a synthetic data generator for testing purposes if access to CRSP is unavailable.*\n",
        "\n",
        "## Usage\n",
        "\n",
        "The notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell, which demonstrates how to use the top-level `execute_complete_mspi_research_pipeline` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load the master configuration from the YAML file.\n",
        "    # (Assumes config.yaml is in the working directory)\n",
        "    with open(\"config.yaml\", \"r\") as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    \n",
        "    # 2. Load raw datasets (Example using synthetic generator provided in the notebook)\n",
        "    # In production, load from CSV/Parquet: pd.read_csv(...)\n",
        "    df_daily, df_index = generate_synthetic_crsp_data()\n",
        "\n",
        "    # 3. Execute the entire replication study.\n",
        "    results = execute_complete_mspi_research_pipeline(df_daily, df_index, config)\n",
        "    \n",
        "    # 4. Access results\n",
        "    print(results[\"mspi_forecasts\"].head())\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns a dictionary containing:\n",
        "-   **`mspi_forecasts`**: DataFrame of out-of-sample stress probabilities ($MSPI_t$) and targets.\n",
        "-   **`discrimination_metrics`**: DataFrame comparing AUC and PR-AUC across models.\n",
        "-   **`probability_metrics`**: DataFrame with Brier Score, Log Loss, and ECE.\n",
        "-   **`economic_analysis`**: Dictionary containing predictive regression stats and IRF DataFrames.\n",
        "-   **`audit_log`**: A detailed record of data cleansing stats, universe counts, and hyperparameter choices.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "measuring_market_stress_with_machine_learning/\n",
        "│\n",
        "├── measuring_market_stress_with_machine_learning_draft.ipynb   # Main implementation notebook\n",
        "├── config.yaml                                                 # Master configuration file\n",
        "├── requirements.txt                                            # Python package dependencies\n",
        "│\n",
        "├── LICENSE                                                     # MIT Project License File\n",
        "└── README.md                                                   # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify study parameters such as:\n",
        "-   **Stress Definition:** `vol_quantile_alpha` (e.g., 0.90 or 0.95), `return_cutoff_c_R`.\n",
        "-   **Feature Engineering:** `tail_threshold_tau` (e.g., 0.05).\n",
        "-   **Learning Protocol:** `initial_training_window_months`, `cv_n_splits`.\n",
        "-   **Models:** Hyperparameter grids for Lasso, Ridge, RF, and GB.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Stress Definitions:** Incorporating liquidity or credit spreads into the stress label.\n",
        "-   **Intraday Features:** Using high-frequency TAQ data for realized measures.\n",
        "-   **Deep Learning Models:** Benchmarking against LSTM or Transformer architectures (with proper calibration).\n",
        "-   **International Markets:** Extending the analysis to global equity markets.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{schmitt2026algorithmic,\n",
        "  title={Algorithmic Monitoring: Measuring Market Stress with Machine Learning},\n",
        "  author={Schmitt, Marc},\n",
        "  journal={arXiv preprint arXiv:2602.07066},\n",
        "  year={2026}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2026). Algorithmic Monitoring: Measuring Market Stress with Machine Learning: An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/measuring_market_stress_with_machine_learning\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Marc Schmitt** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, Scikit-Learn, and Statsmodels**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `measuring_market_stress_with_machine_learning_draft.ipynb` notebook and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "sXdyIYJKr85C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Algorithmic Monitoring: Measuring Market Stress with Machine Learning*\"\n",
        "\n",
        "Authors: Marc Schmitt\n",
        "\n",
        "E-Journal Submission Date: 5 February 2026\n",
        "\n",
        "Link: https://arxiv.org/abs/2602.07066\n",
        "\n",
        "Modified Abstract:\n",
        "\n",
        "The author constructs a Market Stress Probability Index (MSPI) that estimates the probability of high stress in the U.S. equity market one month ahead using information from the cross-section of individual stocks. Using CRSP daily data, each month is summarized by a set of interpretable cross-sectional fragility signals and mapped into a forward-looking stress probability via an L1-regularized logistic regression in a real-time expanding-window design. Out of sample, MSPI tracks major stress episodes and improves discrimination and accuracy relative to a parsimonious benchmark based on lagged market return and realized volatility, delivering calibrated stress probabilities on an economically meaningful scale. Further, I illustrate how MSPI can be used as a probability-based measurement object in financial econometrics. The resulting index provides a transparent and easily updated measure of near-term equity-market stress risk."
      ],
      "metadata": {
        "id": "4xvRoq8wse6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Executive Summary**\n",
        "This paper introduces the **Market Stress Probability Index (MSPI)**, a novel financial-econometric instrument designed to estimate the one-month-ahead probability of a high-stress regime in U.S. equity markets. Departing from composite indices that rely on multi-asset data or option-implied volatility, MSPI is an **equity-only** measure derived strictly from the cross-sectional statistics of individual stock returns (CRSP data).\n",
        "\n",
        "The author employs an **L1-regularized logistic regression (Lasso-logit)** within a strict real-time expanding-window protocol. The central empirical finding is that this parsimonious, interpretable linear mapping outperforms a benchmark of lagged market aggregates and remains highly competitive against flexible non-linear learners (Random Forests and Gradient Boosting) in terms of discrimination and probability calibration. The paper frames this approach as **\"Algorithmic Monitoring\"**: the use of transparent statistical learning to compress high-dimensional market microstructure signals into actionable, calibrated state variables.\n",
        "\n",
        "### **The Econometric Challenge: Monitoring in Algorithmic Markets**\n",
        "The paper posits that the structural transformation of markets—characterized by high-frequency algorithmic trading and electronic liquidity provision—requires a commensurate shift in monitoring technologies.\n",
        "*   **The Problem:** Financial stress is a latent state. Traditional manual oversight struggles to aggregate weak, noisy signals across thousands of securities in real time. Furthermore, stress often arises from endogenous feedback loops (liquidity withdrawal, correlated deleveraging) rather than solely macroeconomic fundamentals.\n",
        "*   **The Objective:** To construct a measurement object that is (i) strictly real-time (no look-ahead bias), (ii) interpretable, (iii) reproducible using standard data, and (iv) output as a well-calibrated probability rather than an arbitrary index level.\n",
        "\n",
        "### **Data Construction and Feature Engineering**\n",
        "The study utilizes CRSP daily stock data for U.S. common stocks. The core hypothesis is that stress leaves a distinctive \"footprint\" in the cross-section of returns before it manifests in aggregate market crashes.\n",
        "\n",
        "**The Feature Set ($X_t$):**\n",
        "The model aggregates daily cross-sectional statistics into monthly predictors, categorized into three fragility signals:\n",
        "1.  **Distributional Moments:** Cross-sectional dispersion ($\\sigma^{xs}$), skewness, and kurtosis. (Logic: Stress correlates with return fragmentation and fat tails).\n",
        "2.  **Tail Participation:** The fraction of stocks experiencing daily returns $\\le -5\\%$ or $\\ge +5\\%$.\n",
        "3.  **Trading Intensity:** Average log volume, dollar volume, and turnover.\n",
        "\n",
        "**The Target Variable ($Y_{t+1}$):**\n",
        "A month is labeled as a \"Stress\" month ($S_t=1$) if:\n",
        "*   The CRSP value-weighted market return is $\\le -5\\%$; **OR**\n",
        "*   Realized market volatility exceeds its historical 90th percentile ($q_{t-1}(0.90)$).\n",
        "*   *Crucial Note:* The volatility threshold is expanding and computed only on past data to ensure the label itself is available in real time.\n",
        "\n",
        "### **The Learning Protocol**\n",
        "The paper treats statistical learning as a measurement tool rather than a \"black box\" forecasting engine.\n",
        "\n",
        "*   **Primary Model:** L1-regularized Logistic Regression (Lasso-logit). The $L_1$ penalty induces sparsity, selecting only the most relevant cross-sectional signals.\n",
        "    $$ \\text{MSPI}_t \\equiv \\text{Pr}(Y_{t+1}=1 | X_t) = \\Lambda(\\beta_0 + X_t'\\beta) $$\n",
        "*   **Benchmarks:**\n",
        "    1.  **Ridge-Logit Benchmark:** Uses only lagged aggregate market return and realized volatility.\n",
        "    2.  **Non-Linear ML:** Random Forests (RF) and Gradient Boosted Trees (GB).\n",
        "*   **Evaluation Protocol:**\n",
        "    *   **Expanding Window:** Models are re-trained monthly.\n",
        "    *   **Hyperparameter Tuning:** Performed on an initial 120-month training set, then fixed to avoid implicit look-ahead.\n",
        "    *   **Calibration:** The non-linear models (RF, GB) are subjected to **Platt scaling** (real-time calibration) to ensure their outputs are valid probabilities, not just ranking scores.\n",
        "\n",
        "### **Empirical Performance (Out-of-Sample 2005–2024)**\n",
        "The evaluation focuses on both discrimination (ranking) and calibration (probability accuracy).\n",
        "\n",
        "*   **Discrimination (AUC & PR-AUC):**\n",
        "    *   **MSPI (Lasso-logit):** AUC **0.800** / PR-AUC **0.538**.\n",
        "    *   **Benchmark:** AUC 0.752 / PR-AUC 0.444.\n",
        "    *   **Gradient Boosting:** AUC 0.756.\n",
        "    *   **Random Forest:** AUC 0.727.\n",
        "    *   *Result:* The sparse linear model outperforms the benchmark and the non-linear learners. This suggests that in monthly macro-finance data (low signal-to-noise), the bias reduction from non-linear models is outweighed by their estimation variance.\n",
        "\n",
        "*   **Probability Accuracy (Proper Scoring Rules):**\n",
        "    *   **Brier Score:** MSPI achieves the lowest (best) score of **0.106**, compared to 0.116 for the benchmark.\n",
        "    *   **Calibration:** MSPI shows a low Expected Calibration Error (ECE) of **0.062**. The predicted probabilities closely match realized stress frequencies.\n",
        "\n",
        "*   **Time-Series Dynamics:**\n",
        "    *   MSPI correctly identifies precursors to the 2008 Global Financial Crisis and the 2020 COVID crash.\n",
        "    *   Compared to the \"jagged\" predictions of Random Forests, MSPI provides a smoother, more stable state variable suitable for monitoring.\n",
        "\n",
        "### **Economic Interpretation and Utility**\n",
        "The paper demonstrates that MSPI is not merely a binary classifier but a continuous state variable for financial econometrics.\n",
        "\n",
        "*   **Predictive Content:** Higher MSPI levels strongly predict next-month realized volatility and downside tail risk (crashes).\n",
        "*   **Stress-Risk Innovations:** The author proposes using local projections to study shocks. By orthogonalizing MSPI against lagged market data ($u_t$), one can isolate \"news\" regarding stress risk.\n",
        "    $$ \\text{MSPI}_t = \\delta_0 + \\delta_1 \\text{MSPI}_{t-1} + \\Delta' Z_{t-1} + u_t $$\n",
        "*   **Transparency vs. Complexity:** The results validate the \"transparency-flexibility trade-off.\" In this domain, a transparent, linear aggregation of cross-sectional fragility signals is superior to opaque non-linear models, satisfying the regulatory and governance requirements for \"Algorithmic Monitoring.\"\n",
        "\n",
        "### **Conclusion**\n",
        "Schmitt successfully argues that the cross-section of equity returns contains rich, latent information about future market stress that is not captured by aggregate market history alone. The **MSPI** serves as a robust, reproducible, and economically interpretable \"thermometer\" for market fragility, proving that in financial time-series forecasting, disciplined regularization (Lasso) often trumps model complexity."
      ],
      "metadata": {
        "id": "pyj-UKkLvwOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "ZnykXbYKgwXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Algorithmic Monitoring: Measuring Market Stress with Machine Learning\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  Market Stress Probability Index (MSPI) framework presented in \"Algorithmic\n",
        "#  Monitoring: Measuring Market Stress with Machine Learning\" by Marc Schmitt\n",
        "#  (2026). It delivers a computationally tractable system for real-time,\n",
        "#  equity-only stress monitoring, enabling robust, state-dependent risk\n",
        "#  assessment, probabilistic forecasting, and structural analysis of market\n",
        "#  fragility using high-dimensional cross-sectional signals.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Cross-sectional fragility signal extraction (moments, tails, liquidity)\n",
        "#  • Real-time expanding-window stress labeling via dynamic volatility quantiles\n",
        "#  • L1-regularized logistic regression (Lasso-Logit) for sparse probability modeling\n",
        "#  • Strict real-time discipline (no look-ahead bias) in feature engineering and training\n",
        "#  • Robustness benchmarking against nonlinear learners (Random Forest, Gradient Boosting)\n",
        "#  • Probability calibration via Platt scaling for reliable risk estimates\n",
        "#  • Block-bootstrap inference for rigorous out-of-sample performance comparison\n",
        "#  • Structural economic analysis via stress-risk innovations and local projections\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Vectorized processing of large-scale CRSP daily micro-data\n",
        "#  • Deterministic deduplication and type coercion for financial time series\n",
        "#  • Expanding-window standardization and hyperparameter tuning\n",
        "#  • Comprehensive audit logging for data lineage and pipeline integrity\n",
        "#  • Integration with statsmodels for HAC-robust econometric inference\n",
        "#  • Modular architecture facilitating component reuse and testing\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Schmitt, M. (2026). Algorithmic Monitoring: Measuring Market Stress with\n",
        "#  Machine Learning. arXiv preprint arXiv:2602.07066.\n",
        "#  https://arxiv.org/abs/2602.07066\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union, Set, Callable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from pandas.api.types import is_datetime64_any_dtype, is_numeric_dtype, is_integer_dtype\n",
        "from scipy.special import expit\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score,\n",
        "    brier_score_loss,\n",
        "    log_loss,\n",
        "    roc_auc_score,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZnpmSsTJg0w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "rW1mdRyjg2iN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draft 1\n",
        "\n",
        "## **Discussion of the Inputs, Processes and Outputs of Key Callables**\n",
        "\n",
        "Here is the granular, step-by-step assessment and documentation of the 22 task-specific orchestrator callables that constitute the complete \"Algorithmic Monitoring\" research pipeline.\n",
        "\n",
        "### 1. `validate_study_config` (Task 1 Orchestrator)\n",
        "*   **Inputs:** `raw_study_config` (Dictionary).\n",
        "*   **Processes:** Injects strict drift-control defaults (e.g., `realized_vol_ddof`, `vol_quantile_interpolation`), validates the presence of all required schema keys, asserts the six mandatory anti-look-ahead constraints, and logs warnings for unspecified optional fields.\n",
        "*   **Outputs:** `validated_config` (Dictionary), `validation_report` (Dictionary).\n",
        "*   **Data Transformation:** Augments the input dictionary with explicit technical parameters and evaluates boolean logic across nested dictionary paths to generate a structured validation report.\n",
        "*   **Role in Research Pipeline:** Enforces the foundational methodological guardrails. It ensures the pipeline strictly adheres to the \"expanding window, no look-ahead, reproducible updating\" protocol mandated in Box 2 of the LaTeX context.\n",
        "\n",
        "### 2. `validate_crsp_daily` (Task 2 Orchestrator)\n",
        "*   **Inputs:** `df_crsp_daily` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Verifies the presence and data types of required columns (`PERMNO`, `DATE`, `SHRCD`, `EXCHCD`, `PRC`, `RET`, `VOL`, `SHROUT`), checks value plausibility (e.g., non-negative volume, valid share codes), and computes missingness statistics.\n",
        "*   **Outputs:** Validation report (Dictionary).\n",
        "*   **Data Transformation:** Applies read-only vectorized boolean masking and aggregation functions (`.isna().sum()`, `.nunique()`) to generate summary statistics without mutating the underlying micro-data.\n",
        "*   **Role in Research Pipeline:** Validates the integrity of the \"CRSP daily stock data accessed via WRDS for U.S. common stocks,\" ensuring the raw inputs can support the rigorous cross-sectional computations required downstream.\n",
        "\n",
        "### 3. `validate_crsp_index` (Task 3 Orchestrator)\n",
        "*   **Inputs:** `df_crsp_daily` (DataFrame), `df_crsp_index` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Validates the schema of the index data, normalizes dates to midnight to verify temporal coverage against the micro-data, and checks the magnitude of `vwretd` to ensure decimal unit compliance.\n",
        "*   **Outputs:** Validation report (Dictionary).\n",
        "*   **Data Transformation:** Performs read-only date normalization and set-difference operations (`np.setdiff1d`) to identify calendar gaps between the micro and macro datasets.\n",
        "*   **Role in Research Pipeline:** Ensures the \"CRSP daily value-weighted market index return\" is structurally sound and temporally aligned, which is a strict prerequisite for constructing the stress label $S_t$.\n",
        "\n",
        "### 4. `cleanse_crsp_daily` (Task 4 Orchestrator)\n",
        "*   **Inputs:** `df_crsp_daily` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Performs deterministic deduplication (stable sort by `PERMNO` and `DATE`, keeping the last observation), coerces target columns to numeric types, creates an absolute price column, drops rows with missing returns or prices, and normalizes dates to create a canonical `year_month` key.\n",
        "*   **Outputs:** `df_clean` (DataFrame), `audit_log` (Dictionary).\n",
        "*   **Data Transformation:** Transforms the raw panel via row-filtering (`dropna`), type casting (`to_numeric`), and column mutation ($|PRC|$), fundamentally altering the shape and type-safety of the data structure.\n",
        "*   **Role in Research Pipeline:** Implements the data preparation requirements: \"(iii) non-missing returns and prices.\" It explicitly handles the CRSP signed-price convention by computing $|P_{i,d}|$ for subsequent microstructure filtering.\n",
        "\n",
        "### 5. `cleanse_crsp_index` (Task 5 Orchestrator)\n",
        "*   **Inputs:** `df_crsp_daily_clean` (DataFrame), `df_crsp_index` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Deduplicates index rows, coerces `vwretd` to numeric, drops missing observations, normalizes dates, creates the `year_month` key, and asserts that every month in the micro-data exists in the index data.\n",
        "*   **Outputs:** `df_index_clean` (DataFrame), `audit_log` (Dictionary).\n",
        "*   **Data Transformation:** Applies row-filtering and date normalization, followed by a strict set-subset assertion between the unique `year_month` values of the micro and macro panels.\n",
        "*   **Role in Research Pipeline:** Guarantees that the aggregate market variables can be computed for every month present in the cross-sectional feature matrix, preventing undefined states in the target variable $Y_{t+1}$.\n",
        "\n",
        "### 6. `construct_eligible_universe` (Task 6 Orchestrator)\n",
        "*   **Inputs:** `df_crsp_daily_clean` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Filters the cleansed panel to retain only ordinary common shares (codes 10, 11), major exchanges (codes 1, 2, 3), and stocks with an absolute price $\\ge \\$1$. Computes the daily cross-section size $N_d$.\n",
        "*   **Outputs:** `df_eligible` (DataFrame), `nd_series` (Series), `audit_log` (Dictionary).\n",
        "*   **Data Transformation:** Applies sequential boolean indexing to slice the DataFrame, followed by a `groupby(\"DATE\")[\"PERMNO\"].nunique()` aggregation to generate the $N_d$ vector.\n",
        "*   **Role in Research Pipeline:** Accurately implements the paper's universe filters: \"(i) ordinary common shares (CRSP share codes 10 and 11), (ii) NYSE/AMEX/NASDAQ listings... and (iv) absolute price at least $1.\"\n",
        "\n",
        "### 7. `compute_daily_return_moments` (Task 7 Orchestrator)\n",
        "*   **Inputs:** `df_eligible` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Computes the daily cross-sectional mean, population dispersion, skewness, and raw kurtosis. Enforces a zero-variance policy to prevent division by zero.\n",
        "*   **Outputs:** DataFrame of daily moments indexed by `DATE`.\n",
        "*   **Data Transformation:** Groups the panel by `DATE` and applies custom aggregations to compute higher-order central moments, broadcasting the daily mean back to the panel to center returns before exponentiation.\n",
        "*   **Role in Research Pipeline:** Implements equations (1) through (4) from the LaTeX context, capturing the \"fragmentation of returns\" and \"extreme cross-sectional moves\":\n",
        "    $\\bar{r}_d \\equiv \\frac{1}{N_d} \\sum_{i=1}^{N_d} r_{i,d}$\n",
        "    $\\sigma^{xs}_d \\equiv \\sqrt{\\frac{1}{N_d} \\sum_{i=1}^{N_d} (r_{i,d} - \\bar{r}_d)^2}$\n",
        "    $\\text{Skew}^{xs}_d \\equiv \\frac{\\frac{1}{N_d} \\sum_{i=1}^{N_d} (r_{i,d} - \\bar{r}_d)^3}{(\\sigma^{xs}_d)^3}$\n",
        "    $\\text{Kurt}^{xs}_d \\equiv \\frac{\\frac{1}{N_d} \\sum_{i=1}^{N_d} (r_{i,d} - \\bar{r}_d)^4}{(\\sigma^{xs}_d)^4}$\n",
        "\n",
        "### 8. `compute_daily_tail_measures` (Task 8 Orchestrator)\n",
        "*   **Inputs:** `df_eligible` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Computes the cross-sectional mean absolute return, and the fraction of stocks experiencing returns $\\le -5\\%$ and $\\ge +5\\%$.\n",
        "*   **Outputs:** DataFrame of daily tail measures indexed by `DATE`.\n",
        "*   **Data Transformation:** Creates boolean indicator series for tail events, then applies a `groupby(\"DATE\").mean()` operation to compute the exact fraction of the cross-section meeting the criteria.\n",
        "*   **Role in Research Pipeline:** Implements equations (5) and (6) to capture \"tail-like behavior in a robust way\":\n",
        "    $\\text{Frac}^{dn}_d(\\tau) \\equiv \\frac{1}{N_d} \\sum_{i=1}^{N_d} \\mathbb{I} \\{r_{i,d} \\le -\\tau\\}$\n",
        "    $\\text{Frac}^{up}_d(\\tau) \\equiv \\frac{1}{N_d} \\sum_{i=1}^{N_d} \\mathbb{I} \\{r_{i,d} \\ge \\tau\\}$\n",
        "\n",
        "### 9. `compute_daily_trading_proxies` (Task 9 Orchestrator)\n",
        "*   **Inputs:** `df_eligible` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Computes the cross-sectional average of $\\log(1 + \\text{Vol})$, dollar volume ($|P| \\times \\text{Vol}$), and turnover ($\\text{Vol} / (1000 \\times \\text{SHROUT})$), filtering out missing or invalid denominators.\n",
        "*   **Outputs:** DataFrame of daily trading proxies indexed by `DATE`.\n",
        "*   **Data Transformation:** Performs row-wise arithmetic transformations (logarithms, multiplication, division) on valid subsets of the data, followed by `groupby` averaging.\n",
        "*   **Role in Research Pipeline:** Implements the liquidity usage and trading intensity signals defined in Table 1: \"average log volume, average dollar volume, and average turnover (volume scaled by shares outstanding).\"\n",
        "\n",
        "### 10. `aggregate_monthly_features` (Task 10 Orchestrator)\n",
        "*   **Inputs:** `daily_stats_all` (DataFrame), `nd_series` (Series), `study_config` (Dictionary).\n",
        "*   **Processes:** Averages all daily statistics within each calendar month, renames columns to the canonical feature set, and drops months with insufficient trading days.\n",
        "*   **Outputs:** `X_t` (DataFrame), `audit_log` (Dictionary).\n",
        "*   **Data Transformation:** Groups the daily time series by `year_month` and applies a `.mean()` reduction, transforming the data from a daily frequency to a monthly panel $X_t$.\n",
        "*   **Role in Research Pipeline:** Implements equation (7), compressing daily fragility signals into the monthly feature vector used for prediction:\n",
        "    $Z_t \\equiv \\frac{1}{D_t} \\sum_{d \\in t} z_d$\n",
        "\n",
        "### 11. `construct_market_aggregates` (Task 11 Orchestrator)\n",
        "*   **Inputs:** `df_crsp_index_clean` (DataFrame), `X_t` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Compounds daily market returns to compute $R^{mkt}_t$, computes the annualized within-month standard deviation to yield $\\sigma^{mkt}_t$, and aligns these aggregates with $X_t$ via an inner join.\n",
        "*   **Outputs:** `aligned_panel` (DataFrame), `audit_log` (Dictionary).\n",
        "*   **Data Transformation:** Applies a custom product aggregation ($\\prod(1+r) - 1$) and standard deviation aggregation to the daily index data, followed by a relational join on the `year_month` index.\n",
        "*   **Role in Research Pipeline:** Constructs the macroeconomic state variables required for both the stress definition and the parsimonious benchmark model.\n",
        "\n",
        "### 12. `construct_stress_labels` (Task 12 Orchestrator)\n",
        "*   **Inputs:** `aligned_panel` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Computes the expanding 90th percentile of realized volatility strictly up to $t-1$, evaluates the logical OR condition for stress at month $t$, and shifts the result backward to create the target $Y_{t+1}$.\n",
        "*   **Outputs:** `modeling_panel` (DataFrame), `audit_log` (Dictionary).\n",
        "*   **Data Transformation:** Utilizes an `.expanding().quantile().shift(1)` operation to prevent look-ahead bias, applies boolean logic to create a binary state variable, and uses `.shift(-1)` to align the future state with current features.\n",
        "*   **Role in Research Pipeline:** Implements equations (8) and (9), defining the latent stress state and the supervised learning target:\n",
        "    $S_t \\equiv \\mathbb{I} \\{ R^{mkt}_t \\le c_R \\} \\lor \\mathbb{I} \\{ \\sigma^{mkt}_t \\ge q_{t-1}(\\alpha) \\}$\n",
        "    $Y_{t+1} \\equiv S_{t+1}$\n",
        "\n",
        "### 13. `expanding_window_standardizer` (Task 13 Orchestrator)\n",
        "*   **Inputs:** `x_panel` (DataFrame), `train_end_idx` (Any), `study_config` (Dictionary).\n",
        "*   **Processes:** Slices the feature panel up to the training boundary, computes the mean and population standard deviation on complete cases, and applies the z-score transformation to the training data.\n",
        "*   **Outputs:** `x_train_std` (DataFrame), `scaler_params` (Dictionary).\n",
        "*   **Data Transformation:** Extracts cross-sectional moments from a temporal slice and broadcasts these scalars across the DataFrame to normalize the feature distributions.\n",
        "*   **Role in Research Pipeline:** Implements the strict data hygiene requirement: \"Predictors are standardized (z-scored) using information available within each training window.\"\n",
        "\n",
        "### 14. `tune_hyperparameters` (Task 14 Orchestrator)\n",
        "*   **Inputs:** `modeling_panel` (DataFrame), `standardizer_func` (Callable), `study_config` (Dictionary).\n",
        "*   **Processes:** Generates forward-chaining time-series splits within the initial 120-month window, performs a grid search for the L1 (Lasso) and L2 (Ridge) penalty parameters minimizing log loss, and freezes the optimal values.\n",
        "*   **Outputs:** `frozen_hyperparams` (Dictionary).\n",
        "*   **Data Transformation:** Iteratively slices the panel into expanding train/validation folds, standardizes features *within* each fold, fits logistic models, and aggregates validation scores.\n",
        "*   **Role in Research Pipeline:** Implements the hyperparameter selection protocol: \"The regularization parameter $\\lambda$ is selected via time-series cross-validation within this initial window and then held fixed...\"\n",
        "\n",
        "### 15. `forecast_mspi` (Task 15 Orchestrator)\n",
        "*   **Inputs:** `modeling_panel` (DataFrame), `frozen_hyperparams` (Dictionary), `standardizer_func` (Callable), `study_config` (Dictionary).\n",
        "*   **Processes:** Iterates over out-of-sample months. For each month $t$, defines a training set strictly where $\\tau+1 \\le t$, standardizes data, fits an L1-regularized logistic regression, and predicts the probability of stress for $t+1$.\n",
        "*   **Outputs:** `mspi_forecasts` (DataFrame).\n",
        "*   **Data Transformation:** Sequentially expands a temporal slice, fits a convex optimization routine (Coordinate Descent via `liblinear`), and maps the linear predictor through a sigmoid function to yield probabilities.\n",
        "*   **Role in Research Pipeline:** Implements the core MSPI algorithm (equations 10 and 11):\n",
        "    $MSPI_t \\equiv \\Pr(Y_{t+1} = 1 | X_t) = \\Lambda(\\beta_0 + X'_t\\beta)$\n",
        "    $(\\hat{\\beta}_0, \\hat{\\beta}) \\in \\arg\\min_{\\beta_0,\\beta} \\left\\{ - \\sum_{t \\in \\mathcal{T}} \\left[ Y_{t+1} \\log p_t + \\dots \\right] + \\lambda\\|\\beta\\|_1 \\right\\}$\n",
        "\n",
        "### 16. `forecast_benchmark` (Task 16 Orchestrator)\n",
        "*   **Inputs:** `modeling_panel` (DataFrame), `frozen_hyperparams` (Dictionary), `standardizer_func` (Callable), `study_config` (Dictionary).\n",
        "*   **Processes:** Executes the identical expanding-window protocol as Task 15, but restricts features to $R^{mkt}_t$ and $\\sigma^{mkt}_t$ and applies an L2 (Ridge) penalty.\n",
        "*   **Outputs:** `benchmark_forecasts` (DataFrame).\n",
        "*   **Data Transformation:** Identical to Task 15, but operating on a highly restricted, two-dimensional feature space.\n",
        "*   **Role in Research Pipeline:** Implements the parsimonious benchmark model (equation 12):\n",
        "    $p^B_t = \\Lambda(\\alpha_0 + \\alpha_1 R^{mkt}_t + \\alpha_2 \\sigma^{mkt}_t)$\n",
        "\n",
        "### 17. `run_robustness_horse_race` (Task 18 Orchestrator)\n",
        "*   **Inputs:** `modeling_panel`, `mspi_forecasts`, `benchmark_forecasts`, `frozen_hyperparams`, `standardizer_func`, `study_config`.\n",
        "*   **Processes:** Fits Random Forest and Gradient Boosting models in expanding windows. Extracts raw scores, fits a Platt scaling (logistic) calibrator on the training scores, applies it to the test scores, and unifies all model forecasts into a single panel.\n",
        "*   **Outputs:** `unified_evaluation_panel` (DataFrame).\n",
        "*   **Data Transformation:** Fits non-parametric ensemble trees, extracts decision functions, and maps them through a secondary logistic regression layer to produce calibrated probabilities.\n",
        "*   **Role in Research Pipeline:** Implements the nonlinear machine-learning benchmarks and explicit probability calibration: \"I apply a real-time calibration map (Platt scaling) within each training window to obtain the probability forecast $\\hat{p}^{RF}_t = g_{RF}(s^{RF}_t)$.\"\n",
        "\n",
        "### 18. `compute_discrimination_metrics` (Task 19 Orchestrator)\n",
        "*   **Inputs:** `unified_evaluation_panel` (DataFrame).\n",
        "*   **Processes:** Computes the Area Under the ROC Curve (AUC) and the Area Under the Precision-Recall Curve (PR-AUC) for all models using their raw, uncalibrated scores.\n",
        "*   **Outputs:** `discrimination_metrics` (DataFrame).\n",
        "*   **Data Transformation:** Computes rank-based integration metrics over the aligned arrays of predictions and ground-truth labels.\n",
        "*   **Role in Research Pipeline:** Implements the rank-based evaluation: \"I next quantify these comparisons using rank-based discrimination metrics (ROC and precision–recall) computed from raw scores...\"\n",
        "\n",
        "### 19. `compute_probability_metrics` (Task 20 Orchestrator)\n",
        "*   **Inputs:** `unified_evaluation_panel`, `discrimination_metrics`, `study_config`.\n",
        "*   **Processes:** Computes the Brier Score, Log Loss (with probability clipping), and Expected Calibration Error (ECE) using equal-mass quantile binning. Merges these with the discrimination metrics.\n",
        "*   **Outputs:** `probability_metrics` (DataFrame).\n",
        "*   **Data Transformation:** Applies proper scoring rule formulas and performs quantile discretization (`pd.qcut`) to compute weighted absolute calibration errors.\n",
        "*   **Role in Research Pipeline:** Implements the probability accuracy evaluation: \"The Brier score and log loss (negative log-likelihood) reward sharp but accurate probabilities and penalize overconfident errors.\"\n",
        "\n",
        "### 20. `perform_bootstrap_inference` (Task 21 Orchestrator)\n",
        "*   **Inputs:** `unified_evaluation_panel` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Generates 2,000 moving block bootstrap resamples (block length 12), computes the difference in metrics (MSPI vs Benchmark) for each resample, and calculates the mean difference and 95% confidence intervals.\n",
        "*   **Outputs:** `bootstrap_inference` (DataFrame).\n",
        "*   **Data Transformation:** Performs resampling with replacement on integer indices, recalculates metrics iteratively, and applies percentile aggregations to the resulting distributions.\n",
        "*   **Role in Research Pipeline:** Implements the statistical significance testing: \"To quantify sampling uncertainty... I implement a block bootstrap over months (12-month blocks; 2,000 replications).\"\n",
        "\n",
        "### 21. `perform_economic_analysis` (Task 22 Orchestrator)\n",
        "*   **Inputs:** `mspi_forecasts` (DataFrame), `market_aggregates` (DataFrame), `study_config` (Dictionary).\n",
        "*   **Processes:** Estimates a predictive OLS regression for next-month volatility, constructs stress-risk innovations by orthogonalizing MSPI against its lags and market controls, and estimates local projections (IRFs) using HAC standard errors.\n",
        "*   **Outputs:** `economic_analysis` (Dictionary).\n",
        "*   **Data Transformation:** Performs temporal shifting to align variables, fits OLS regressions, and computes Heteroskedasticity and Autocorrelation Consistent (Newey-West) covariance matrices.\n",
        "*   **Role in Research Pipeline:** Implements the financial econometrics applications (equations 13, 15, 16):\n",
        "    $\\sigma^{mkt}_{t+1} = \\alpha + \\gamma MSPI_t + \\phi' Z_t + \\eta_{t+1}$\n",
        "    $MSPI_t = \\delta_0 + \\delta_1 MSPI_{t-1} + \\Delta' Z_{t-1} + u_t$\n",
        "    $y_{t+h} = a_h + b_h u_t + \\Gamma'_h W_{t-1} + \\varepsilon_{t+h}$\n",
        "\n",
        "### 22. `execute_complete_mspi_research_pipeline` (Top-Level Orchestrator)\n",
        "*   **Inputs:** `df_crsp_daily` (DataFrame), `df_crsp_index` (DataFrame), `raw_study_config` (Dictionary).\n",
        "*   **Processes:** Sequentially executes all 21 upstream orchestrators in strict dependency order. It manages the flow of intermediate DataFrames, passes the standardizer callable to modeling functions, executes final anti-look-ahead safety checks, and compiles a master results dictionary.\n",
        "*   **Outputs:** `results` (Dictionary containing all features, labels, forecasts, metrics, economic analyses, and audit logs).\n",
        "*   **Data Transformation:** Acts as the master composition function, threading the outputs of data cleansing into feature engineering, into labeling, into modeling, and finally into evaluation and inference.\n",
        "*   **Role in Research Pipeline:** Operationalizes the entire \"Algorithmic Monitoring\" framework. It guarantees that the \"expanding window, no look-ahead, reproducible updating\" protocol defined in Box 2 is strictly enforced from raw data ingestion to final econometric output.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Usage Example**\n",
        "\n",
        "Here is the granular, step-by-step guide to executing the end-to-end pipeline for **\"Algorithmic Monitoring: Measuring Market Stress with Machine Learning\"**. This example demonstrates how to synthetically generate the required CRSP micro and macro data, load the study configuration from a YAML file, and execute the full research pipeline using the `execute_complete_mspi_research_pipeline` orchestrator.\n",
        "\n",
        "**Note:** This example assumes that the required Python modules and all callables defined in this workbook (from `validate_study_config` to `execute_complete_mspi_research_pipeline`) are available in the current namespace (e.g., defined in a single Jupyter notebook). It also assumes a `config.yaml` file exists in the working directory.\n",
        "\n",
        "### **Step 1: Synthetic Data Generation**\n",
        "\n",
        "The pipeline requires two specific DataFrames: `df_crsp_daily` (micro-data) and `df_crsp_index` (market aggregate). We generate high-fidelity synthetic versions of these datasets using `Faker` and `numpy` to mimic the schema, data types, and statistical properties required by the pipeline's validation logic.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Micro-Data (`df_crsp_daily`):** We simulate a panel of 50 stocks over a 20-year period. We include all required columns (`PERMNO`, `DATE`, `SHRCD`, `EXCHCD`, `PRC`, `RET`, `VOL`, `SHROUT`). We intentionally introduce some missing values and negative prices (CRSP convention) to test the cleansing logic.\n",
        "2.  **Index Data (`df_crsp_index`):** We simulate a value-weighted market return (`vwretd`) corresponding to the same dates.\n",
        "3.  **Schema Enforcement:** We explicitly cast columns to the strict types (`Int64`, `float64`, `datetime64[ns]`) expected by the validators.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import os\n",
        "from faker import Faker\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Initialize Faker and RNG for reproducibility\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_synthetic_crsp_data(\n",
        "    start_date: str = \"2000-01-01\",\n",
        "    end_date: str = \"2024-12-31\",\n",
        "    n_stocks: int = 50\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Generates synthetic CRSP Daily Stock File (micro) and Index File (macro) DataFrames.\n",
        "\n",
        "    Purpose:\n",
        "        To create high-fidelity mock datasets that mimic the schema and statistical\n",
        "        properties of CRSP data, enabling the execution of the MSPI pipeline without\n",
        "        proprietary data access.\n",
        "\n",
        "    Inputs:\n",
        "        start_date (str): Start date 'YYYY-MM-DD'.\n",
        "        end_date (str): End date 'YYYY-MM-DD'.\n",
        "        n_stocks (int): Number of unique stocks to simulate in the micro-panel.\n",
        "\n",
        "    Returns:\n",
        "        tuple[pd.DataFrame, pd.DataFrame]: (df_crsp_daily, df_crsp_index)\n",
        "    \"\"\"\n",
        "    # 1. Generate Trading Days (Business Days)\n",
        "    dates = pd.date_range(start=start_date, end=end_date, freq='B')\n",
        "    n_days = len(dates)\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # A. Generate Micro-Data (df_crsp_daily)\n",
        "    # -------------------------------------------------------------------------\n",
        "    permnos = np.arange(10000, 10000 + n_stocks)\n",
        "    \n",
        "    # Create a long-format panel\n",
        "    # Repeat dates for each permno\n",
        "    panel_dates = np.tile(dates, n_stocks)\n",
        "    panel_permnos = np.repeat(permnos, n_days)\n",
        "    \n",
        "    n_rows = len(panel_dates)\n",
        "    \n",
        "    # Simulate Returns (Normal distribution, mean 0, vol 2%)\n",
        "    ret = np.random.normal(0.0004, 0.02, n_rows)\n",
        "    \n",
        "    # Simulate Prices (Random walk approx, some negative for CRSP convention)\n",
        "    # Base price ~ $50\n",
        "    prc_raw = np.random.uniform(5, 100, n_rows)\n",
        "    # Make 5% of prices negative (bid/ask flag)\n",
        "    neg_mask = np.random.rand(n_rows) < 0.05\n",
        "    prc = prc_raw.copy()\n",
        "    prc[neg_mask] = -prc[neg_mask]\n",
        "    \n",
        "    # Simulate Volume and Shares\n",
        "    vol = np.random.randint(1000, 1000000, n_rows)\n",
        "    shrout = np.random.randint(5000, 500000, n_rows) # In thousands\n",
        "    \n",
        "    # Simulate Codes (Mostly valid, some invalid to test filters)\n",
        "    # SHRCD: 90% are 10 or 11\n",
        "    shrcd = np.random.choice([10, 11, 12, 31], size=n_rows, p=[0.45, 0.45, 0.05, 0.05])\n",
        "    # EXCHCD: 90% are 1, 2, 3\n",
        "    exchcd = np.random.choice([1, 2, 3, 4], size=n_rows, p=[0.3, 0.3, 0.3, 0.1])\n",
        "    \n",
        "    df_crsp_daily = pd.DataFrame({\n",
        "        \"PERMNO\": panel_permnos,\n",
        "        \"DATE\": panel_dates,\n",
        "        \"SHRCD\": shrcd,\n",
        "        \"EXCHCD\": exchcd,\n",
        "        \"PRC\": prc,\n",
        "        \"RET\": ret,\n",
        "        \"VOL\": vol,\n",
        "        \"SHROUT\": shrout\n",
        "    })\n",
        "    \n",
        "    # Enforce Types\n",
        "    df_crsp_daily[\"PERMNO\"] = df_crsp_daily[\"PERMNO\"].astype(\"Int64\")\n",
        "    df_crsp_daily[\"SHRCD\"] = df_crsp_daily[\"SHRCD\"].astype(\"Int64\")\n",
        "    df_crsp_daily[\"EXCHCD\"] = df_crsp_daily[\"EXCHCD\"].astype(\"Int64\")\n",
        "    df_crsp_daily[\"DATE\"] = df_crsp_daily[\"DATE\"].astype(\"datetime64[ns]\")\n",
        "    df_crsp_daily[\"PRC\"] = df_crsp_daily[\"PRC\"].astype(\"float64\")\n",
        "    df_crsp_daily[\"RET\"] = df_crsp_daily[\"RET\"].astype(\"float64\")\n",
        "    df_crsp_daily[\"VOL\"] = df_crsp_daily[\"VOL\"].astype(\"float64\")\n",
        "    df_crsp_daily[\"SHROUT\"] = df_crsp_daily[\"SHROUT\"].astype(\"float64\")\n",
        "    \n",
        "    # Introduce some missingness (1% of RET)\n",
        "    mask_missing = np.random.rand(n_rows) < 0.01\n",
        "    df_crsp_daily.loc[mask_missing, \"RET\"] = np.nan\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # B. Generate Index Data (df_crsp_index)\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Simulate market return (correlated with mean stock return but lower vol)\n",
        "    # Mean 0.04%, Vol 1%\n",
        "    vwretd = np.random.normal(0.0004, 0.01, n_days)\n",
        "    \n",
        "    df_crsp_index = pd.DataFrame({\n",
        "        \"DATE\": dates,\n",
        "        \"vwretd\": vwretd\n",
        "    })\n",
        "    \n",
        "    # Enforce Types\n",
        "    df_crsp_index[\"DATE\"] = df_crsp_index[\"DATE\"].astype(\"datetime64[ns]\")\n",
        "    df_crsp_index[\"vwretd\"] = df_crsp_index[\"vwretd\"].astype(\"float64\")\n",
        "    \n",
        "    return df_crsp_daily, df_crsp_index\n",
        "\n",
        "# Generate datasets\n",
        "print(\"Generating synthetic CRSP data...\")\n",
        "df_daily, df_index = generate_synthetic_crsp_data()\n",
        "print(\"Data generation complete.\")\n",
        "print(f\"Micro-data shape: {df_daily.shape}\")\n",
        "print(f\"Index-data shape: {df_index.shape}\")\n",
        "```\n",
        "<br>\n",
        "\n",
        "### **Step 2: Loading the Configuration (`config.yaml`)**\n",
        "\n",
        "We read the `config.yaml` file (assumed to be in the working directory) into a Python dictionary. This dictionary serves as the single source of truth for all study parameters.\n",
        "\n",
        "```python\n",
        "def load_config(filepath: str = \"config.yaml\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Loads the study configuration parameters from a YAML file into a Python dictionary.\n",
        "\n",
        "    Purpose:\n",
        "        To ingest the deterministic hyperparameters, data split definitions, and\n",
        "        evaluation metrics defined in the external configuration file. This ensures\n",
        "        reproducibility by separating code from configuration.\n",
        "\n",
        "    Inputs:\n",
        "        filepath (str): The relative or absolute path to the YAML configuration file.\n",
        "                        Default is \"config.yaml\".\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the study configuration.\n",
        "                        Returns an empty dictionary if the file is not found (fallback).\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If filepath is not a string.\n",
        "        yaml.YAMLError: If the file contains invalid YAML syntax.\n",
        "    \"\"\"\n",
        "    # Validate input type\n",
        "    if not isinstance(filepath, str):\n",
        "        raise TypeError(f\"filepath must be a string, got {type(filepath)}.\")\n",
        "\n",
        "    try:\n",
        "        # Open the file stream\n",
        "        with open(filepath, \"r\") as file:\n",
        "            # Parse the YAML content safely\n",
        "            config = yaml.safe_load(file)\n",
        "\n",
        "        # Log success to console\n",
        "        print(f\"\\nSuccessfully loaded configuration from {filepath}\")\n",
        "\n",
        "        return config\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # Fallback for demonstration if file doesn't exist in this specific run environment\n",
        "        # In a real scenario, this would raise an error, but per instructions we handle it gracefully.\n",
        "        print(f\"\\nWarning: {filepath} not found. Please ensure the file exists.\")\n",
        "        return {}\n",
        "    except yaml.YAMLError as e:\n",
        "        # Handle parsing errors explicitly\n",
        "        print(f\"\\nError parsing YAML file {filepath}: {e}\")\n",
        "        raise\n",
        "\n",
        "# Load the configuration\n",
        "# Note: Ensure 'config.yaml' is in your working directory.\n",
        "raw_config = load_config()\n",
        "\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Step 3: Executing the Pipeline (`execute_complete_mspi_research_pipeline`)**\n",
        "\n",
        "We now invoke the top-level orchestrator. This function will validate inputs, cleanse data, engineer features, train models, and produce the final MSPI index and analysis.\n",
        "\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STARTING MSPI PIPELINE EXECUTION\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Execute the pipeline\n",
        "    try:\n",
        "        results = execute_complete_mspi_research_pipeline(\n",
        "            df_crsp_daily=df_daily,\n",
        "            df_crsp_index=df_index,\n",
        "            raw_study_config=raw_config\n",
        "        )\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PIPELINE EXECUTION SUCCESSFUL\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # ---------------------------------------------------------------------\n",
        "        # Inspecting Artifacts\n",
        "        # ---------------------------------------------------------------------\n",
        "        \n",
        "        # 1. MSPI Forecasts\n",
        "        print(\"\\n[1] MSPI Forecasts (Head):\")\n",
        "        print(results[\"mspi_forecasts\"].head())\n",
        "        \n",
        "        # 2. Discrimination Metrics\n",
        "        print(\"\\n[2] Discrimination Metrics (AUC/PR-AUC):\")\n",
        "        print(results[\"discrimination_metrics\"])\n",
        "        \n",
        "        # 3. Probability Metrics\n",
        "        print(\"\\n[3] Probability Metrics (Brier/LogLoss/ECE):\")\n",
        "        print(results[\"probability_metrics\"])\n",
        "        \n",
        "        # 4. Economic Analysis (Predictive Regression)\n",
        "        print(\"\\n[4] Predictive Regression Results (Next-Month Volatility):\")\n",
        "        reg_stats = results[\"economic_analysis\"][\"predictive_regression_vol\"]\n",
        "        print(pd.DataFrame([reg_stats]))\n",
        "        \n",
        "        # 5. Audit Log (Sample)\n",
        "        print(\"\\n[5] Audit Log (Data Cleansing):\")\n",
        "        print(results[\"audit_log\"][\"cleansing_daily\"])\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nCRITICAL FAILURE: Pipeline execution failed with error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Summary of Outputs**\n",
        "\n",
        "The `results` dictionary contains the complete set of research artifacts:\n",
        "*   **`mspi_forecasts`**: The time series of stress probabilities ($MSPI_t$) and realized targets ($Y_{t+1}$).\n",
        "*   **`discrimination_metrics`**: A DataFrame comparing AUC and PR-AUC across MSPI, Benchmark, RF, and GB.\n",
        "*   **`probability_metrics`**: Calibration diagnostics including Brier Score and ECE.\n",
        "*   **`economic_analysis`**: Results from the predictive volatility regression and impulse response functions.\n",
        "*   **`audit_log`**: A detailed record of rows dropped, duplicates removed, and model hyperparameters used, ensuring full reproducibility.\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Implemented Callables**"
      ],
      "metadata": {
        "id": "uC2VEboE7QzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Validate `STUDY_CONFIG` dictionary completeness and internal coherence\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate and parse the study configuration dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "def apply_strict_defaults(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Injects strict, reproducible defaults for technical parameters that may be\n",
        "    missing from the high-level study configuration. This ensures no silent\n",
        "    numerical drift occurs due to unspecified library defaults.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The input study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The configuration dictionary with explicit drift-control keys added.\n",
        "    \"\"\"\n",
        "    # Create a deep copy to avoid mutating the input in place unexpectedly\n",
        "    # (In a full implementation, use copy.deepcopy; here we assume dict structure)\n",
        "    clean_config = config.copy()\n",
        "\n",
        "    # 1. Market Aggregates Defaults\n",
        "    if \"market_aggregates\" in clean_config:\n",
        "        # Default to sample standard deviation (unbiased) for realized vol\n",
        "        clean_config[\"market_aggregates\"].setdefault(\"realized_vol_ddof\", 1)\n",
        "\n",
        "    # 2. Target Definition Defaults\n",
        "    if \"target_definition\" in clean_config:\n",
        "        # Default to linear interpolation for quantiles (standard in finance)\n",
        "        clean_config[\"target_definition\"].setdefault(\"vol_quantile_interpolation\", \"linear\")\n",
        "        # Default to 12 months history before first label is generated\n",
        "        clean_config[\"target_definition\"].setdefault(\"min_vol_history_months\", 12)\n",
        "\n",
        "    # 3. Feature Engineering Defaults\n",
        "    if \"feature_engineering\" in clean_config:\n",
        "        # Default to Period['M'] for strict monthly alignment\n",
        "        clean_config[\"feature_engineering\"].setdefault(\"month_index_type\", \"period_M\")\n",
        "        # Default epsilon for zero-variance checks\n",
        "        clean_config[\"feature_engineering\"].setdefault(\"std_epsilon\", 1e-8)\n",
        "\n",
        "    # 4. Evaluation Defaults\n",
        "    if \"evaluation_inference\" in clean_config:\n",
        "        # Default probability clipping for log loss stability\n",
        "        clean_config[\"evaluation_inference\"].setdefault(\"proba_clip_epsilon\", 1e-6)\n",
        "        # Default calibration protocol\n",
        "        clean_config[\"evaluation_inference\"].setdefault(\"calibration_protocol\", \"in_sample_within_window\")\n",
        "        # Default bootstrap settings if missing\n",
        "        if \"block_bootstrap\" in clean_config[\"evaluation_inference\"]:\n",
        "            clean_config[\"evaluation_inference\"][\"block_bootstrap\"].setdefault(\"variant\", \"moving\")\n",
        "            clean_config[\"evaluation_inference\"][\"block_bootstrap\"].setdefault(\"ci_method\", \"percentile\")\n",
        "            clean_config[\"evaluation_inference\"][\"block_bootstrap\"].setdefault(\"degenerate_sample_policy\", \"skip\")\n",
        "\n",
        "    # 5. Learning Protocol Defaults\n",
        "    if \"learning_protocol\" in clean_config:\n",
        "        if \"hyperparameter_tuning\" in clean_config[\"learning_protocol\"]:\n",
        "            # Default CV objective\n",
        "            clean_config[\"learning_protocol\"][\"hyperparameter_tuning\"].setdefault(\"cv_objective\", \"neg_log_loss\")\n",
        "            # Default CV geometry (5 splits is standard for this sample size)\n",
        "            clean_config[\"learning_protocol\"][\"hyperparameter_tuning\"].setdefault(\"cv_n_splits\", 5)\n",
        "            clean_config[\"learning_protocol\"][\"hyperparameter_tuning\"].setdefault(\"cv_val_size_months\", 12)\n",
        "            clean_config[\"learning_protocol\"][\"hyperparameter_tuning\"].setdefault(\"cv_step_size_months\", 12)\n",
        "\n",
        "    return clean_config\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Verify all required top-level keys exist and validate schema structure.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_schema_completeness(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates that the configuration dictionary contains all required top-level keys\n",
        "    and critical sub-keys necessary for the MSPI pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A report dictionary with keys 'passed' (bool), 'missing_keys' (List[str]),\n",
        "        and 'errors' (List[str]).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any top-level key is missing, as this prevents pipeline initialization.\n",
        "    \"\"\"\n",
        "    # List of mandatory top-level keys derived from the task specification\n",
        "    required_top_level = [\n",
        "        \"raw_data_schema\",\n",
        "        \"universe_construction\",\n",
        "        \"feature_engineering\",\n",
        "        \"market_aggregates\",\n",
        "        \"target_definition\",\n",
        "        \"learning_protocol\",\n",
        "        \"models\",\n",
        "        \"evaluation_inference\",\n",
        "        \"economic_analysis\",\n",
        "        \"reproducibility\"\n",
        "    ]\n",
        "\n",
        "    missing_keys = [key for key in required_top_level if key not in config]\n",
        "\n",
        "    if missing_keys:\n",
        "        error_msg = f\"Critical Error: Missing top-level configuration keys: {missing_keys}\"\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # Validate critical sub-keys for drift control (must be present after default injection)\n",
        "    # We check paths like \"target_definition.vol_quantile_alpha\"\n",
        "    critical_subpaths = [\n",
        "        (\"target_definition\", \"vol_quantile_alpha\"),\n",
        "        (\"target_definition\", \"vol_quantile_interpolation\"), # Drift control\n",
        "        (\"market_aggregates\", \"realized_vol_ddof\"),          # Drift control\n",
        "        (\"feature_engineering\", \"month_index_type\"),         # Drift control\n",
        "        (\"learning_protocol\", \"window_type\"),\n",
        "        (\"reproducibility\", \"random_seed\")\n",
        "    ]\n",
        "\n",
        "    errors = []\n",
        "    for parent, child in critical_subpaths:\n",
        "        if child not in config.get(parent, {}):\n",
        "            errors.append(f\"Missing critical sub-key: ['{parent}']['{child}']\")\n",
        "\n",
        "    return {\n",
        "        \"passed\": len(errors) == 0,\n",
        "        \"missing_keys\": missing_keys,\n",
        "        \"errors\": errors\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Assert anti-look-ahead constraints.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def assert_real_time_discipline(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Enforces strict anti-look-ahead constraints mandated by the research protocol.\n",
        "    Violations are treated as fatal errors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A report dictionary with 'passed' (bool) and 'violations' (List[str]).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any real-time constraint is violated.\n",
        "    \"\"\"\n",
        "    violations = []\n",
        "\n",
        "    # 1. Volatility Quantile Window must be expanding\n",
        "    # Source: \"qt-1(alpha) is the expanding, real-time alpha-quantile\"\n",
        "    if config[\"target_definition\"][\"vol_quantile_window\"] != \"expanding\":\n",
        "        violations.append(\n",
        "            f\"Constraint Violation: target_definition.vol_quantile_window must be 'expanding', \"\n",
        "            f\"got '{config['target_definition']['vol_quantile_window']}'\"\n",
        "        )\n",
        "\n",
        "    # 2. Quantile Lag must be 1 month\n",
        "    # Source: \"qt-1(alpha)... computed using data available up to month t-1\"\n",
        "    if config[\"target_definition\"][\"vol_quantile_lag_months\"] != 1:\n",
        "        violations.append(\n",
        "            f\"Constraint Violation: target_definition.vol_quantile_lag_months must be 1, \"\n",
        "            f\"got {config['target_definition']['vol_quantile_lag_months']}\"\n",
        "        )\n",
        "\n",
        "    # 3. Learning Window Type must be expanding\n",
        "    # Source: \"re-estimated in an expanding-window design\"\n",
        "    if config[\"learning_protocol\"][\"window_type\"] != \"expanding\":\n",
        "        violations.append(\n",
        "            f\"Constraint Violation: learning_protocol.window_type must be 'expanding', \"\n",
        "            f\"got '{config['learning_protocol']['window_type']}'\"\n",
        "        )\n",
        "\n",
        "    # 4. Standardization Scope must be training window only\n",
        "    # Source: \"Predictors are standardized (z-scored) using information available within each training window\"\n",
        "    std_scope = config[\"learning_protocol\"][\"standardization\"][\"scope\"]\n",
        "    if std_scope != \"training_window_only\":\n",
        "        violations.append(\n",
        "            f\"Constraint Violation: learning_protocol.standardization.scope must be 'training_window_only', \"\n",
        "            f\"got '{std_scope}'\"\n",
        "        )\n",
        "\n",
        "    # 5. Hyperparameter Tuning Scope\n",
        "    # Source: \"selected via time-series cross-validation within this initial window\"\n",
        "    tune_init = config[\"learning_protocol\"][\"hyperparameter_tuning\"][\"tune_in_initial_window_only\"]\n",
        "    if tune_init is not True:\n",
        "        violations.append(\n",
        "            f\"Constraint Violation: learning_protocol.hyperparameter_tuning.tune_in_initial_window_only must be True, \"\n",
        "            f\"got {tune_init}\"\n",
        "        )\n",
        "\n",
        "    # 6. Hyperparameter Fixing\n",
        "    # Source: \"held fixed for the subsequent expanding-window evaluation\"\n",
        "    hold_fixed = config[\"learning_protocol\"][\"hyperparameter_tuning\"][\"hold_fixed_after_tuning\"]\n",
        "    if hold_fixed is not True:\n",
        "        violations.append(\n",
        "            f\"Constraint Violation: learning_protocol.hyperparameter_tuning.hold_fixed_after_tuning must be True, \"\n",
        "            f\"got {hold_fixed}\"\n",
        "        )\n",
        "\n",
        "    if violations:\n",
        "        error_msg = \"\\n\".join(violations)\n",
        "        raise ValueError(f\"Real-Time Discipline Check Failed:\\n{error_msg}\")\n",
        "\n",
        "    return {\n",
        "        \"passed\": True,\n",
        "        \"violations\": []\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Log unspecified fields and drift controls.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def check_unspecified_fields(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Identifies fields that are explicitly None or missing, which require user resolution\n",
        "    or strict default injection to prevent numerical drift.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A report dictionary with 'warnings' (List[str]) detailing unspecified fields.\n",
        "    \"\"\"\n",
        "    warnings = []\n",
        "\n",
        "    # Check for None values in fields that require explicit settings\n",
        "    # 1. CV Objective\n",
        "    cv_obj = config[\"learning_protocol\"][\"hyperparameter_tuning\"].get(\"cv_objective\")\n",
        "    if cv_obj is None:\n",
        "        warnings.append(\"Warning: 'cv_objective' is None. Defaulting to 'neg_log_loss' is recommended.\")\n",
        "\n",
        "    # 2. Random Forest Hyperparameters\n",
        "    rf_params = config[\"models\"][\"random_forest\"].get(\"hyperparameters\")\n",
        "    if rf_params is None:\n",
        "        warnings.append(\"Warning: Random Forest 'hyperparameters' is None. Full tuning grid must be defined in initial window.\")\n",
        "\n",
        "    # 3. Gradient Boosting Hyperparameters\n",
        "    gb_params = config[\"models\"][\"gradient_boosted_trees\"].get(\"hyperparameters\")\n",
        "    if gb_params is None:\n",
        "        warnings.append(\"Warning: Gradient Boosting 'hyperparameters' is None. Full tuning grid must be defined in initial window.\")\n",
        "\n",
        "    # 4. Local Projections Horizons\n",
        "    lp_h = config[\"economic_analysis\"][\"local_projections\"].get(\"horizons_h\")\n",
        "    if lp_h is None:\n",
        "        warnings.append(\"Warning: Local Projections 'horizons_h' is None. User must specify horizons (e.g., 12).\")\n",
        "\n",
        "    # 5. Local Projections Controls\n",
        "    lp_w = config[\"economic_analysis\"][\"local_projections\"].get(\"controls_W\")\n",
        "    if lp_w is None:\n",
        "        warnings.append(\"Warning: Local Projections 'controls_W' is None. User must specify control variables.\")\n",
        "\n",
        "    return {\n",
        "        \"warnings\": warnings\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_study_config(raw_config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the study configuration. It applies strict defaults\n",
        "    to prevent drift, verifies schema completeness, enforces real-time discipline,\n",
        "    and reports warnings for unspecified fields.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_config : Dict[str, Any]\n",
        "        The raw input configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing:\n",
        "        - 'validated_config': The configuration with strict defaults applied.\n",
        "        - 'validation_report': A summary of checks, errors, and warnings.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If critical schema keys are missing or real-time constraints are violated.\n",
        "    \"\"\"\n",
        "    # 1. Apply strict defaults to ensure drift control keys exist\n",
        "    # This transforms the raw input into a \"canonical\" config\n",
        "    validated_config = apply_strict_defaults(raw_config)\n",
        "\n",
        "    # 2. Validate Schema Completeness\n",
        "    # This will raise ValueError if top-level keys are missing\n",
        "    schema_report = validate_schema_completeness(validated_config)\n",
        "    if not schema_report[\"passed\"]:\n",
        "        # This block might be unreachable due to raise in function, but good for robustness\n",
        "        raise ValueError(f\"Schema Validation Failed: {schema_report['errors']}\")\n",
        "\n",
        "    # 3. Assert Real-Time Discipline\n",
        "    # This will raise ValueError if constraints are violated\n",
        "    discipline_report = assert_real_time_discipline(validated_config)\n",
        "\n",
        "    # 4. Check for Unspecified Fields (Warnings)\n",
        "    warnings_report = check_unspecified_fields(validated_config)\n",
        "\n",
        "    # Compile final report\n",
        "    final_report = {\n",
        "        \"schema_validation\": schema_report,\n",
        "        \"discipline_validation\": discipline_report,\n",
        "        \"warnings\": warnings_report[\"warnings\"],\n",
        "        \"status\": \"PASSED\"\n",
        "    }\n",
        "\n",
        "    # Log warnings to console/logger\n",
        "    if warnings_report[\"warnings\"]:\n",
        "        print(\"Configuration Warnings:\")\n",
        "        for w in warnings_report[\"warnings\"]:\n",
        "            print(f\"  - {w}\")\n",
        "\n",
        "    return {\n",
        "        \"validated_config\": validated_config,\n",
        "        \"validation_report\": final_report\n",
        "    }\n"
      ],
      "metadata": {
        "id": "_qNVAL4Lg72X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 — Validate df_crsp_daily schema, types, and value ranges\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate df_crsp_daily schema, types, and value ranges\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Verify required columns are present and correctly typed.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _check_column_presence_and_types(\n",
        "    df: pd.DataFrame,\n",
        "    schema_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates that the CRSP Daily DataFrame contains all required columns and that\n",
        "    they conform to the expected data types (datetime, integer-like, numeric).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The CRSP Daily Stock File (micro-data).\n",
        "    schema_config : Dict[str, Any]\n",
        "        The 'crsp_daily_stock_file' section of the study configuration, containing\n",
        "        'required_columns'.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any required column is missing or has a fundamentally incompatible type.\n",
        "    \"\"\"\n",
        "    required_cols = schema_config[\"required_columns\"]\n",
        "    missing = [c for c in required_cols if c not in df.columns]\n",
        "\n",
        "    if missing:\n",
        "        # Critical failure: missing columns prevent any downstream processing.\n",
        "        raise ValueError(f\"CRSP Daily validation failed. Missing columns: {missing}\")\n",
        "\n",
        "    # 1. Validate DATE type\n",
        "    # Must be datetime64[ns] to ensure correct temporal sorting and grouping.\n",
        "    if not is_datetime64_any_dtype(df[\"DATE\"]):\n",
        "        raise TypeError(\n",
        "            f\"Column 'DATE' must be datetime64[ns], got {df['DATE'].dtype}. \"\n",
        "            \"Please parse dates during ingestion.\"\n",
        "        )\n",
        "\n",
        "    # 2. Validate Integer-like Identifiers (PERMNO, SHRCD, EXCHCD)\n",
        "    # These define the panel structure and universe. Float representation is acceptable\n",
        "    # if lossless, but object/string is dangerous.\n",
        "    int_cols = [\"PERMNO\", \"SHRCD\", \"EXCHCD\"]\n",
        "    for col in int_cols:\n",
        "        if not is_numeric_dtype(df[col]):\n",
        "            # If object, check if it's coercible to integer (e.g. string \"10\")\n",
        "            # This is a strict check to prevent \"10.0\" strings or mixed types.\n",
        "            try:\n",
        "                pd.to_numeric(df[col], errors='raise').astype('Int64')\n",
        "            except Exception:\n",
        "                raise TypeError(\n",
        "                    f\"Column '{col}' must be integer-like, got {df[col].dtype} \"\n",
        "                    \"and is not safely coercible.\"\n",
        "                )\n",
        "\n",
        "    # 3. Validate Numeric Signals (PRC, RET, VOL, SHROUT)\n",
        "    # These are the basis of all fragility signals.\n",
        "    num_cols = [\"PRC\", \"RET\", \"VOL\", \"SHROUT\"]\n",
        "    for col in num_cols:\n",
        "        # We allow object type here ONLY if it contains coercible numbers (common in CRSP\n",
        "        # where error codes like 'C' are mixed with floats). Task 4 will cleanse them.\n",
        "        # Here we just ensure the column isn't purely garbage.\n",
        "        if not is_numeric_dtype(df[col]):\n",
        "            # Attempt to coerce a sample to verify feasibility\n",
        "            sample = df[col].dropna().head(100)\n",
        "            try:\n",
        "                pd.to_numeric(sample, errors='coerce')\n",
        "            except Exception:\n",
        "                raise TypeError(\n",
        "                    f\"Column '{col}' has incompatible type {df[col].dtype} and \"\n",
        "                    \"sample could not be coerced to numeric.\"\n",
        "                )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Verify value-range plausibility (sanity bounds).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _check_value_plausibility(df: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Performs vectorized sanity checks on data values to detect gross data corruption\n",
        "    or ingestion errors (e.g., negative shares, impossible returns).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The CRSP Daily Stock File.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "        A list of warning messages describing any plausibility violations.\n",
        "    \"\"\"\n",
        "    warnings = []\n",
        "\n",
        "    # 1. SHRCD Plausibility\n",
        "    # CRSP share codes are typically 10, 11, 12, etc.\n",
        "    # We check if the dataset contains *any* valid common share codes (10, 11).\n",
        "    # If not, the universe filter will result in an empty set.\n",
        "    valid_shrcd_mask = df[\"SHRCD\"].isin([10, 11])\n",
        "    if not valid_shrcd_mask.any():\n",
        "        warnings.append(\"CRITICAL: No rows with SHRCD 10 or 11 found. Universe will be empty.\")\n",
        "\n",
        "    # 2. EXCHCD Plausibility\n",
        "    # We check for presence of NYSE/AMEX/NASDAQ codes (1, 2, 3).\n",
        "    valid_exch_mask = df[\"EXCHCD\"].isin([1, 2, 3])\n",
        "    if not valid_exch_mask.any():\n",
        "        warnings.append(\"CRITICAL: No rows with EXCHCD 1, 2, or 3 found. Universe will be empty.\")\n",
        "\n",
        "    # 3. RET Plausibility\n",
        "    # Coerce to numeric for checking (handling CRSP error codes as NaN temporarily)\n",
        "    # Returns < -1.0 are impossible for simple holding periods (limited liability).\n",
        "    # Returns > 20.0 (2000%) are extremely rare daily events, likely errors if frequent.\n",
        "    ret_numeric = pd.to_numeric(df[\"RET\"], errors='coerce')\n",
        "    if (ret_numeric < -1.0).any():\n",
        "        count = (ret_numeric < -1.0).sum()\n",
        "        warnings.append(f\"Found {count} rows with RET < -1.0 (impossible). Check data source.\")\n",
        "\n",
        "    # 4. VOL Non-negativity\n",
        "    # Volume cannot be negative.\n",
        "    vol_numeric = pd.to_numeric(df[\"VOL\"], errors='coerce')\n",
        "    if (vol_numeric < 0).any():\n",
        "        count = (vol_numeric < 0).sum()\n",
        "        warnings.append(f\"Found {count} rows with negative VOL. Check data source.\")\n",
        "\n",
        "    # 5. SHROUT Positivity\n",
        "    # Shares outstanding must be positive for a traded stock.\n",
        "    shrout_numeric = pd.to_numeric(df[\"SHROUT\"], errors='coerce')\n",
        "    # Check strictly negative or zero (where non-missing)\n",
        "    invalid_shrout = (shrout_numeric <= 0)\n",
        "    if invalid_shrout.any():\n",
        "        count = invalid_shrout.sum()\n",
        "        warnings.append(f\"Found {count} rows with SHROUT <= 0. Turnover will be undefined.\")\n",
        "\n",
        "    return warnings\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Report DataFrame shape and missingness summary.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _summarize_dataframe_state(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes summary statistics of the DataFrame to serve as an audit record\n",
        "    before cleansing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The CRSP Daily Stock File.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing shape, date ranges, and missingness fractions.\n",
        "    \"\"\"\n",
        "    # 1. Basic Dimensions\n",
        "    n_rows = len(df)\n",
        "    n_permnos = df[\"PERMNO\"].nunique()\n",
        "    min_date = df[\"DATE\"].min()\n",
        "    max_date = df[\"DATE\"].max()\n",
        "\n",
        "    # 2. Missingness Fractions\n",
        "    # We care about missingness in the critical signal columns.\n",
        "    # Note: We check raw missingness (NaN), not CRSP error codes yet.\n",
        "    missing_stats = {}\n",
        "    for col in [\"RET\", \"PRC\", \"VOL\", \"SHROUT\"]:\n",
        "        # For object columns, we might want to count non-coercible as missing too,\n",
        "        # but strictly speaking, NaN is the structural missingness.\n",
        "        # Task 4 handles the coercion-to-NaN.\n",
        "        missing_count = df[col].isna().sum()\n",
        "        missing_stats[f\"pct_missing_{col}\"] = missing_count / n_rows if n_rows > 0 else 0.0\n",
        "\n",
        "    # 3. Universe Eligibility Preview\n",
        "    # Fraction of rows that are NOT common shares (10, 11) or NOT major exchange (1, 2, 3)\n",
        "    # This helps anticipate how much data will be dropped in Task 6.\n",
        "    # We handle potential non-numeric types by coercing for the check.\n",
        "    shrcd_numeric = pd.to_numeric(df[\"SHRCD\"], errors='coerce')\n",
        "    exchcd_numeric = pd.to_numeric(df[\"EXCHCD\"], errors='coerce')\n",
        "\n",
        "    ineligible_shrcd = (~shrcd_numeric.isin([10, 11])).sum()\n",
        "    ineligible_exchcd = (~exchcd_numeric.isin([1, 2, 3])).sum()\n",
        "\n",
        "    return {\n",
        "        \"n_rows\": n_rows,\n",
        "        \"n_unique_permnos\": n_permnos,\n",
        "        \"min_date\": str(min_date),\n",
        "        \"max_date\": str(max_date),\n",
        "        \"missingness\": missing_stats,\n",
        "        \"universe_preview\": {\n",
        "            \"pct_ineligible_shrcd\": ineligible_shrcd / n_rows if n_rows > 0 else 0.0,\n",
        "            \"pct_ineligible_exchcd\": ineligible_exchcd / n_rows if n_rows > 0 else 0.0\n",
        "        }\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_crsp_daily(\n",
        "    df_crsp_daily: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the CRSP Daily Stock File. It verifies schema\n",
        "    compliance, checks for data plausibility, and generates a summary audit report.\n",
        "    This function does NOT modify the input DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_crsp_daily : pd.DataFrame\n",
        "        The raw CRSP Daily Stock File.\n",
        "    study_config : Dict[str, Any]\n",
        "        The full study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A validation report containing 'status' (str), 'warnings' (List[str]),\n",
        "        and 'summary_stats' (Dict).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If schema validation fails (missing columns).\n",
        "    TypeError\n",
        "        If column types are fundamentally incompatible.\n",
        "    \"\"\"\n",
        "    # Extract relevant schema config\n",
        "    schema = study_config[\"raw_data_schema\"][\"crsp_daily_stock_file\"]\n",
        "\n",
        "    # 1. Schema and Type Validation (Raises on failure)\n",
        "    _check_column_presence_and_types(df_crsp_daily, schema)\n",
        "\n",
        "    # 2. Plausibility Checks (Returns warnings)\n",
        "    warnings = _check_value_plausibility(df_crsp_daily)\n",
        "\n",
        "    # 3. Summary Statistics (Audit log)\n",
        "    summary_stats = _summarize_dataframe_state(df_crsp_daily)\n",
        "\n",
        "    # Log warnings if any\n",
        "    if warnings:\n",
        "        print(\"CRSP Daily Data Warnings:\")\n",
        "        for w in warnings:\n",
        "            print(f\"  - {w}\")\n",
        "\n",
        "    return {\n",
        "        \"status\": \"PASSED\",\n",
        "        \"warnings\": warnings,\n",
        "        \"summary_stats\": summary_stats\n",
        "    }\n"
      ],
      "metadata": {
        "id": "CcNfGwX2i2YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 — Validate df_crsp_index schema, types, and temporal coverage\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Validate df_crsp_index schema, types, and temporal coverage\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Verify required columns and types.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _check_index_schema(\n",
        "    df_index: pd.DataFrame,\n",
        "    schema_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates that the CRSP Index DataFrame contains the required columns and types.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_index : pd.DataFrame\n",
        "        The CRSP Daily Index File.\n",
        "    schema_config : Dict[str, Any]\n",
        "        The 'crsp_daily_index_file' section of the study configuration.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If required columns are missing.\n",
        "    TypeError\n",
        "        If column types are incompatible.\n",
        "    \"\"\"\n",
        "    required_cols = schema_config[\"required_columns\"]\n",
        "    missing = [c for c in required_cols if c not in df_index.columns]\n",
        "\n",
        "    if missing:\n",
        "        raise ValueError(f\"CRSP Index validation failed. Missing columns: {missing}\")\n",
        "\n",
        "    # 1. Validate DATE\n",
        "    if not is_datetime64_any_dtype(df_index[\"DATE\"]):\n",
        "        raise TypeError(\n",
        "            f\"Index 'DATE' must be datetime64[ns], got {df_index['DATE'].dtype}.\"\n",
        "        )\n",
        "\n",
        "    # 2. Validate vwretd (Value-Weighted Return)\n",
        "    # Must be numeric. If object, check coercibility.\n",
        "    if not is_numeric_dtype(df_index[\"vwretd\"]):\n",
        "        try:\n",
        "            pd.to_numeric(df_index[\"vwretd\"], errors='raise')\n",
        "        except Exception:\n",
        "            raise TypeError(\n",
        "                f\"Index 'vwretd' must be numeric, got {df_index['vwretd'].dtype}.\"\n",
        "            )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Verify temporal coverage relative to df_crsp_daily.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _check_temporal_coverage(\n",
        "    df_micro: pd.DataFrame,\n",
        "    df_index: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Verifies that the index data covers the full temporal span of the micro data.\n",
        "    Identifies any trading days present in the micro data but missing from the index.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_micro : pd.DataFrame\n",
        "        The CRSP Daily Stock File.\n",
        "    df_index : pd.DataFrame\n",
        "        The CRSP Daily Index File.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A coverage report containing date ranges and missing date counts.\n",
        "    \"\"\"\n",
        "    # Normalize to date-only (midnight) to ensure fair comparison\n",
        "    micro_dates = pd.to_datetime(df_micro[\"DATE\"]).dt.normalize().unique()\n",
        "    index_dates = pd.to_datetime(df_index[\"DATE\"]).dt.normalize().unique()\n",
        "\n",
        "    micro_min, micro_max = micro_dates.min(), micro_dates.max()\n",
        "    index_min, index_max = index_dates.min(), index_dates.max()\n",
        "\n",
        "    # Check span\n",
        "    span_warning = []\n",
        "    if index_min > micro_min:\n",
        "        span_warning.append(f\"Index starts after micro data ({index_min} > {micro_min})\")\n",
        "    if index_max < micro_max:\n",
        "        span_warning.append(f\"Index ends before micro data ({index_max} < {micro_max})\")\n",
        "\n",
        "    # Check for specific gaps (micro dates missing from index)\n",
        "    # We use set difference for efficiency\n",
        "    missing_dates = np.setdiff1d(micro_dates, index_dates)\n",
        "    n_missing = len(missing_dates)\n",
        "\n",
        "    # Check if missing dates are weekdays (more serious)\n",
        "    missing_weekdays = 0\n",
        "    if n_missing > 0:\n",
        "        missing_dt = pd.to_datetime(missing_dates)\n",
        "        # dayofweek: 0=Mon, 4=Fri, 5=Sat, 6=Sun\n",
        "        missing_weekdays = (missing_dt.dayofweek < 5).sum()\n",
        "\n",
        "    return {\n",
        "        \"micro_range\": (str(micro_min), str(micro_max)),\n",
        "        \"index_range\": (str(index_min), str(index_max)),\n",
        "        \"span_warnings\": span_warning,\n",
        "        \"missing_dates_count\": n_missing,\n",
        "        \"missing_weekdays_count\": missing_weekdays,\n",
        "        \"missing_dates_sample\": [str(d) for d in missing_dates[:5]] if n_missing > 0 else []\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Verify plausibility of vwretd values.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _check_index_plausibility(df_index: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Checks if index returns are in decimal units and within plausible historical bounds.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_index : pd.DataFrame\n",
        "        The CRSP Daily Index File.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "        A list of warning messages.\n",
        "    \"\"\"\n",
        "    warnings = []\n",
        "\n",
        "    # Coerce to numeric for checking\n",
        "    vwretd = pd.to_numeric(df_index[\"vwretd\"], errors='coerce')\n",
        "\n",
        "    # 1. Unit Check (Decimal vs Percent)\n",
        "    # If median absolute return > 0.5, it's likely percent (e.g., 1.0 = 1%).\n",
        "    # Real daily returns are typically ~0.00-0.02.\n",
        "    median_abs = vwretd.abs().median()\n",
        "    if median_abs > 0.10: # Threshold: 10% median daily move is impossible\n",
        "        warnings.append(\n",
        "            f\"CRITICAL: Median absolute index return is {median_abs:.4f}. \"\n",
        "            \"Data appears to be in PERCENT, not DECIMAL. Pipeline requires decimal.\"\n",
        "        )\n",
        "\n",
        "    # 2. Extreme Value Check\n",
        "    # Max daily drop in history (1987) was ~22%.\n",
        "    # We flag anything > 25% magnitude as suspicious.\n",
        "    max_abs = vwretd.abs().max()\n",
        "    if max_abs > 0.25:\n",
        "        warnings.append(\n",
        "            f\"Warning: Max absolute index return is {max_abs:.4f} (> 0.25). \"\n",
        "            \"Verify if this is a valid crash (e.g. 1987) or data error.\"\n",
        "        )\n",
        "\n",
        "    return warnings\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_crsp_index(\n",
        "    df_crsp_daily: pd.DataFrame,\n",
        "    df_crsp_index: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the CRSP Daily Index File. Verifies schema,\n",
        "    temporal alignment with micro data, and value plausibility.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_crsp_daily : pd.DataFrame\n",
        "        The CRSP Daily Stock File (for temporal comparison).\n",
        "    df_crsp_index : pd.DataFrame\n",
        "        The CRSP Daily Index File.\n",
        "    study_config : Dict[str, Any]\n",
        "        The study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A validation report.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If schema validation fails.\n",
        "    \"\"\"\n",
        "    schema = study_config[\"raw_data_schema\"][\"crsp_daily_index_file\"]\n",
        "\n",
        "    # 1. Schema Validation\n",
        "    _check_index_schema(df_crsp_index, schema)\n",
        "\n",
        "    # 2. Temporal Coverage\n",
        "    coverage_report = _check_temporal_coverage(df_crsp_daily, df_crsp_index)\n",
        "\n",
        "    # 3. Plausibility\n",
        "    plausibility_warnings = _check_index_plausibility(df_crsp_index)\n",
        "\n",
        "    # Combine warnings\n",
        "    all_warnings = coverage_report[\"span_warnings\"] + plausibility_warnings\n",
        "\n",
        "    # Log warnings\n",
        "    if all_warnings:\n",
        "        print(\"CRSP Index Data Warnings:\")\n",
        "        for w in all_warnings:\n",
        "            print(f\"  - {w}\")\n",
        "\n",
        "    if coverage_report[\"missing_weekdays_count\"] > 0:\n",
        "        print(f\"  - CRITICAL: {coverage_report['missing_weekdays_count']} weekdays missing from index.\")\n",
        "\n",
        "    return {\n",
        "        \"status\": \"PASSED\",\n",
        "        \"coverage\": coverage_report,\n",
        "        \"warnings\": all_warnings\n",
        "    }\n"
      ],
      "metadata": {
        "id": "nZ0kAluijmsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 — Cleanse df_crsp_daily (micro data)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Cleanse df_crsp_daily (micro data)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Resolve duplicates, coerce types, and handle CRSP signed-price convention.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _deduplicate_and_coerce(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Performs deterministic deduplication and type coercion on the CRSP Daily file.\n",
        "    Handles the CRSP signed-price convention by creating an absolute price column.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Raw CRSP Daily DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, int]]\n",
        "        - Cleansed DataFrame with numeric columns and 'abs_prc'.\n",
        "        - Dictionary of cleansing statistics (duplicates dropped, coercion NaNs).\n",
        "    \"\"\"\n",
        "    # 1. Deterministic Deduplication\n",
        "    # Sort by PERMNO, DATE to ensure 'keep=\"last\"' is deterministic relative to file order\n",
        "    # (assuming file order has some meaning, otherwise this is at least reproducible).\n",
        "    # We use a stable sort.\n",
        "    df_sorted = df.sort_values(by=[\"PERMNO\", \"DATE\"], kind=\"stable\")\n",
        "\n",
        "    initial_count = len(df_sorted)\n",
        "    # Drop exact duplicates first\n",
        "    df_dedup = df_sorted.drop_duplicates()\n",
        "    exact_dupes = initial_count - len(df_dedup)\n",
        "\n",
        "    # Drop duplicates on primary key (PERMNO, DATE), keeping last\n",
        "    df_dedup = df_dedup.drop_duplicates(subset=[\"PERMNO\", \"DATE\"], keep=\"last\")\n",
        "    key_dupes = (initial_count - exact_dupes) - len(df_dedup)\n",
        "\n",
        "    # 2. Type Coercion\n",
        "    # Coerce critical columns to numeric, turning errors (e.g. 'C') into NaN\n",
        "    coercion_stats = {}\n",
        "    for col in [\"RET\", \"PRC\", \"VOL\", \"SHROUT\"]:\n",
        "        # Track pre-existing NaNs to distinguish coercion effects\n",
        "        pre_nans = df_dedup[col].isna().sum()\n",
        "\n",
        "        # Coerce\n",
        "        df_dedup[col] = pd.to_numeric(df_dedup[col], errors=\"coerce\")\n",
        "\n",
        "        post_nans = df_dedup[col].isna().sum()\n",
        "        coercion_stats[f\"nans_induced_{col}\"] = int(post_nans - pre_nans)\n",
        "\n",
        "    # 3. Handle Signed Price Convention\n",
        "    # CRSP reports negative prices if they are bid/ask averages.\n",
        "    # We need absolute price for filters and dollar volume.\n",
        "    df_dedup[\"abs_prc\"] = df_dedup[\"PRC\"].abs()\n",
        "\n",
        "    stats = {\n",
        "        \"exact_duplicates_dropped\": int(exact_dupes),\n",
        "        \"key_duplicates_dropped\": int(key_dupes),\n",
        "        **coercion_stats\n",
        "    }\n",
        "\n",
        "    return df_dedup, stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Drop rows that violate non-missing requirements.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _enforce_non_missing_requirements(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Drops rows where Returns or Prices are missing, as required by the paper's\n",
        "    methodology. Retains rows with missing Volume/SharesOut for return-based signals.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame from Step 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, int]]\n",
        "        - Filtered DataFrame.\n",
        "        - Dictionary of drop counts.\n",
        "    \"\"\"\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # 1. Drop missing RET\n",
        "    # Paper: \"non-missing returns\"\n",
        "    df_ret_valid = df.dropna(subset=[\"RET\"])\n",
        "    dropped_ret = initial_count - len(df_ret_valid)\n",
        "\n",
        "    # 2. Drop missing PRC (abs_prc)\n",
        "    # Paper: \"non-missing... prices\" (implied by price filter and universe definition)\n",
        "    df_valid = df_ret_valid.dropna(subset=[\"abs_prc\"])\n",
        "    dropped_prc = len(df_ret_valid) - len(df_valid)\n",
        "\n",
        "    stats = {\n",
        "        \"rows_dropped_missing_ret\": int(dropped_ret),\n",
        "        \"rows_dropped_missing_prc\": int(dropped_prc),\n",
        "        \"final_row_count\": len(df_valid)\n",
        "    }\n",
        "\n",
        "    return df_valid, stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Normalize DATE and create auxiliary temporal columns.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _normalize_dates_and_create_month_key(\n",
        "    df: pd.DataFrame,\n",
        "    month_index_type: str = \"period_M\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes dates to midnight and creates a canonical 'year_month' column\n",
        "    for aggregation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame from Step 2.\n",
        "    month_index_type : str\n",
        "        Configuration for month key type. Currently supports 'period_M' (pandas Period).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with normalized 'DATE' and new 'year_month' column.\n",
        "    \"\"\"\n",
        "    # 1. Normalize DATE to midnight (remove time component)\n",
        "    # Ensure it's datetime first (Task 2 validated this, but safe to enforce)\n",
        "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"]).dt.normalize()\n",
        "\n",
        "    # 2. Create Canonical Month Key\n",
        "    # This logic must be identical to Task 5 (Index cleansing)\n",
        "    if month_index_type == \"period_M\":\n",
        "        df[\"year_month\"] = df[\"DATE\"].dt.to_period(\"M\")\n",
        "    else:\n",
        "        # Fallback or other types could be implemented here\n",
        "        raise ValueError(f\"Unsupported month_index_type: {month_index_type}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def cleanse_crsp_daily(\n",
        "    df_crsp_daily: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing of the CRSP Daily Stock File.\n",
        "    1. Deduplicates and coerces types.\n",
        "    2. Enforces non-missing return/price requirements.\n",
        "    3. Normalizes dates and creates month keys.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_crsp_daily : pd.DataFrame\n",
        "        Raw CRSP Daily DataFrame.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        - 'df_clean': The cleansed DataFrame.\n",
        "        - 'audit_log': Dictionary of cleansing statistics.\n",
        "    \"\"\"\n",
        "    # Extract config for month type\n",
        "    month_type = study_config[\"feature_engineering\"].get(\"month_index_type\", \"period_M\")\n",
        "\n",
        "    # Step 1: Deduplicate and Coerce\n",
        "    df_step1, stats_step1 = _deduplicate_and_coerce(df_crsp_daily)\n",
        "\n",
        "    # Step 2: Enforce Non-Missing\n",
        "    df_step2, stats_step2 = _enforce_non_missing_requirements(df_step1)\n",
        "\n",
        "    # Step 3: Temporal Normalization\n",
        "    df_final = _normalize_dates_and_create_month_key(df_step2, month_index_type=month_type)\n",
        "\n",
        "    # Compile Audit Log\n",
        "    audit_log = {\n",
        "        **stats_step1,\n",
        "        **stats_step2,\n",
        "        \"month_index_type_used\": month_type\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"df_clean\": df_final,\n",
        "        \"audit_log\": audit_log\n",
        "    }\n"
      ],
      "metadata": {
        "id": "D4bKFdCakPaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 — Cleanse df_crsp_index and enforce cross-dataset calendar alignment\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Cleanse df_crsp_index and enforce cross-dataset calendar alignment\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Resolve duplicates and coerce types.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _deduplicate_and_coerce_index(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Cleanses the CRSP Index DataFrame by removing duplicates and enforcing numeric types.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Raw CRSP Index DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, int]]\n",
        "        - Cleansed DataFrame.\n",
        "        - Dictionary of cleansing statistics.\n",
        "    \"\"\"\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # 1. Deterministic Deduplication\n",
        "    # Sort by DATE to ensure deterministic 'keep=\"last\"'\n",
        "    df_sorted = df.sort_values(by=\"DATE\", kind=\"stable\")\n",
        "\n",
        "    # Drop exact duplicates\n",
        "    df_dedup = df_sorted.drop_duplicates()\n",
        "    exact_dupes = initial_count - len(df_dedup)\n",
        "\n",
        "    # Drop duplicate dates (keep last update)\n",
        "    df_dedup = df_dedup.drop_duplicates(subset=[\"DATE\"], keep=\"last\")\n",
        "    date_dupes = (initial_count - exact_dupes) - len(df_dedup)\n",
        "\n",
        "    # 2. Type Coercion\n",
        "    # Coerce vwretd to numeric\n",
        "    pre_nans = df_dedup[\"vwretd\"].isna().sum()\n",
        "    df_dedup[\"vwretd\"] = pd.to_numeric(df_dedup[\"vwretd\"], errors=\"coerce\")\n",
        "    post_nans = df_dedup[\"vwretd\"].isna().sum()\n",
        "    coercion_nans = int(post_nans - pre_nans)\n",
        "\n",
        "    # 3. Drop Missing vwretd\n",
        "    # Cannot compute market return or volatility without it\n",
        "    df_valid = df_dedup.dropna(subset=[\"vwretd\"])\n",
        "    dropped_missing = len(df_dedup) - len(df_valid)\n",
        "\n",
        "    stats = {\n",
        "        \"index_exact_duplicates_dropped\": int(exact_dupes),\n",
        "        \"index_date_duplicates_dropped\": int(date_dupes),\n",
        "        \"index_vwretd_coercion_nans\": coercion_nans,\n",
        "        \"index_rows_dropped_missing_vwretd\": int(dropped_missing)\n",
        "    }\n",
        "\n",
        "    return df_valid, stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Normalize DATE and create year_month column.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _normalize_index_dates(\n",
        "    df: pd.DataFrame,\n",
        "    month_index_type: str = \"period_M\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes index dates and creates the canonical month key, matching the logic\n",
        "    used for micro data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Cleansed Index DataFrame.\n",
        "    month_index_type : str\n",
        "        Configuration for month key type.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with 'year_month' column.\n",
        "    \"\"\"\n",
        "    # Normalize to midnight\n",
        "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"]).dt.normalize()\n",
        "\n",
        "    # Create Month Key\n",
        "    if month_index_type == \"period_M\":\n",
        "        df[\"year_month\"] = df[\"DATE\"].dt.to_period(\"M\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported month_index_type: {month_index_type}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Verify alignment and log coverage statistics.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _verify_calendar_alignment(\n",
        "    df_micro: pd.DataFrame,\n",
        "    df_index: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Verifies that every month present in the micro data has corresponding index data.\n",
        "    This is a fatal requirement for computing stress labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_micro : pd.DataFrame\n",
        "        Cleansed Micro DataFrame (from Task 4).\n",
        "    df_index : pd.DataFrame\n",
        "        Cleansed Index DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Alignment report.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If micro data contains months missing from the index data.\n",
        "    \"\"\"\n",
        "    # Get unique months\n",
        "    micro_months = set(df_micro[\"year_month\"].unique())\n",
        "    index_months = set(df_index[\"year_month\"].unique())\n",
        "\n",
        "    # Check subset condition\n",
        "    missing_months = sorted(list(micro_months - index_months))\n",
        "\n",
        "    if missing_months:\n",
        "        # Fatal error: We cannot label stress for these months\n",
        "        raise ValueError(\n",
        "            f\"CRITICAL ALIGNMENT FAILURE: {len(missing_months)} months exist in \"\n",
        "            f\"micro data but are missing from index data. Examples: {missing_months[:5]}\"\n",
        "        )\n",
        "\n",
        "    # Compute trading days per month\n",
        "    micro_counts = df_micro.groupby(\"year_month\")[\"DATE\"].nunique()\n",
        "    index_counts = df_index.groupby(\"year_month\")[\"DATE\"].nunique()\n",
        "\n",
        "    # Compare counts for overlapping months\n",
        "    # We expect them to be identical or very close (index might have fewer holidays?)\n",
        "    # Actually, index usually has *more* days if micro filters drop low-activity days,\n",
        "    # but here we count *potential* trading days.\n",
        "    # We just log the stats.\n",
        "    alignment_stats = {\n",
        "        \"n_micro_months\": len(micro_months),\n",
        "        \"n_index_months\": len(index_months),\n",
        "        \"months_aligned\": True\n",
        "    }\n",
        "\n",
        "    return alignment_stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def cleanse_crsp_index(\n",
        "    df_crsp_daily_clean: pd.DataFrame,\n",
        "    df_crsp_index: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing of the CRSP Index File and enforces alignment with\n",
        "    the already-cleansed micro data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_crsp_daily_clean : pd.DataFrame\n",
        "        Cleansed Micro DataFrame (from Task 4).\n",
        "    df_crsp_index : pd.DataFrame\n",
        "        Raw CRSP Index DataFrame.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        - 'df_index_clean': Cleansed Index DataFrame.\n",
        "        - 'audit_log': Cleansing and alignment statistics.\n",
        "    \"\"\"\n",
        "    month_type = study_config[\"feature_engineering\"].get(\"month_index_type\", \"period_M\")\n",
        "\n",
        "    # Step 1: Deduplicate and Coerce\n",
        "    df_step1, stats_step1 = _deduplicate_and_coerce_index(df_crsp_index)\n",
        "\n",
        "    # Step 2: Temporal Normalization\n",
        "    df_final = _normalize_index_dates(df_step1, month_index_type=month_type)\n",
        "\n",
        "    # Step 3: Verify Alignment\n",
        "    # This raises ValueError if alignment fails\n",
        "    alignment_stats = _verify_calendar_alignment(df_crsp_daily_clean, df_final)\n",
        "\n",
        "    audit_log = {\n",
        "        **stats_step1,\n",
        "        **alignment_stats,\n",
        "        \"month_index_type_used\": month_type\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"df_index_clean\": df_final,\n",
        "        \"audit_log\": audit_log\n",
        "    }\n"
      ],
      "metadata": {
        "id": "SxlcZGbAkxXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 — Construct the eligible daily universe (apply all paper filters)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Construct the eligible daily universe (apply all paper filters)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Apply share-code and exchange-code filters.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _apply_universe_filters(\n",
        "    df: pd.DataFrame,\n",
        "    universe_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Applies share code and exchange code filters to define the eligible universe.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Cleansed CRSP Daily DataFrame.\n",
        "    universe_config : Dict[str, Any]\n",
        "        Configuration containing valid share and exchange codes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, int]]\n",
        "        - Filtered DataFrame.\n",
        "        - Dictionary of drop statistics.\n",
        "    \"\"\"\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Extract valid codes\n",
        "    valid_shrcd = universe_config[\"valid_share_codes\"]\n",
        "    valid_exchcd = universe_config[\"valid_exch_codes\"]\n",
        "\n",
        "    # Apply Filters\n",
        "    # 1. Share Code Filter (Ordinary Common Shares: 10, 11)\n",
        "    mask_shrcd = df[\"SHRCD\"].isin(valid_shrcd)\n",
        "    df_shrcd = df[mask_shrcd]\n",
        "    dropped_shrcd = initial_count - len(df_shrcd)\n",
        "\n",
        "    # 2. Exchange Code Filter (NYSE/AMEX/NASDAQ: 1, 2, 3)\n",
        "    mask_exchcd = df_shrcd[\"EXCHCD\"].isin(valid_exchcd)\n",
        "    df_exchcd = df_shrcd[mask_exchcd]\n",
        "    dropped_exchcd = len(df_shrcd) - len(df_exchcd)\n",
        "\n",
        "    stats = {\n",
        "        \"rows_dropped_shrcd_filter\": int(dropped_shrcd),\n",
        "        \"rows_dropped_exchcd_filter\": int(dropped_exchcd)\n",
        "    }\n",
        "\n",
        "    return df_exchcd, stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Apply the penny-stock (microstructure) price filter.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _apply_price_filter(\n",
        "    df: pd.DataFrame,\n",
        "    universe_config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Applies the minimum price filter to remove penny stocks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame filtered by share/exchange codes.\n",
        "    universe_config : Dict[str, Any]\n",
        "        Configuration containing the minimum price threshold.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, int]]\n",
        "        - Filtered DataFrame.\n",
        "        - Dictionary of drop statistics.\n",
        "    \"\"\"\n",
        "    initial_count = len(df)\n",
        "    min_price = universe_config[\"min_abs_price_usd\"]\n",
        "\n",
        "    # Apply Price Filter (|PRC| >= 1.00)\n",
        "    # We use 'abs_prc' which was created in Task 4\n",
        "    mask_price = df[\"abs_prc\"] >= min_price\n",
        "    df_price = df[mask_price]\n",
        "    dropped_price = initial_count - len(df_price)\n",
        "\n",
        "    stats = {\n",
        "        \"rows_dropped_price_filter\": int(dropped_price)\n",
        "    }\n",
        "\n",
        "    return df_price, stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Define daily cross-section size Nd and produce the final eligible panel.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _finalize_universe_and_compute_Nd(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Computes the daily cross-section size (Nd) and prepares the final eligible panel.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Fully filtered DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.Series]\n",
        "        - Final eligible panel with selected columns.\n",
        "        - Series of Nd (number of eligible stocks) indexed by DATE.\n",
        "    \"\"\"\n",
        "    # Compute Nd: Count of unique PERMNOs per DATE\n",
        "    # Since we deduplicated in Task 4, simple count is sufficient, but nunique is safer\n",
        "    nd_series = df.groupby(\"DATE\")[\"PERMNO\"].nunique().rename(\"n_stocks\")\n",
        "\n",
        "    # Select only necessary columns for downstream tasks to optimize memory\n",
        "    # We need: DATE, year_month, PERMNO, RET, abs_prc, VOL, SHROUT\n",
        "    cols_to_keep = [\"DATE\", \"year_month\", \"PERMNO\", \"RET\", \"abs_prc\", \"VOL\", \"SHROUT\"]\n",
        "    df_final = df[cols_to_keep].copy()\n",
        "\n",
        "    # Ensure sorted by DATE, PERMNO for efficient grouping later\n",
        "    df_final = df_final.sort_values(by=[\"DATE\", \"PERMNO\"])\n",
        "\n",
        "    return df_final, nd_series\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def construct_eligible_universe(\n",
        "    df_crsp_daily_clean: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the eligible daily universe by applying\n",
        "    share code, exchange code, and price filters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_crsp_daily_clean : pd.DataFrame\n",
        "        Cleansed CRSP Daily DataFrame (from Task 4).\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        - 'df_eligible': Final eligible daily panel.\n",
        "        - 'nd_series': Daily cross-section size Series.\n",
        "        - 'audit_log': Filtering statistics.\n",
        "    \"\"\"\n",
        "    universe_config = study_config[\"universe_construction\"]\n",
        "\n",
        "    # Step 1: Share/Exchange Code Filters\n",
        "    df_step1, stats_step1 = _apply_universe_filters(df_crsp_daily_clean, universe_config)\n",
        "\n",
        "    # Step 2: Price Filter\n",
        "    df_step2, stats_step2 = _apply_price_filter(df_step1, universe_config)\n",
        "\n",
        "    # Step 3: Finalize and Compute Nd\n",
        "    df_final, nd_series = _finalize_universe_and_compute_Nd(df_step2)\n",
        "\n",
        "    audit_log = {\n",
        "        **stats_step1,\n",
        "        **stats_step2,\n",
        "        \"final_universe_row_count\": len(df_final),\n",
        "        \"final_universe_unique_dates\": len(nd_series)\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"df_eligible\": df_final,\n",
        "        \"nd_series\": nd_series,\n",
        "        \"audit_log\": audit_log\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Zfqmx7bJmiby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 — Compute daily cross-sectional return distribution moments\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Compute daily cross-sectional return distribution moments\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Compute daily cross-sectional mean and dispersion.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_mean_and_dispersion(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the daily cross-sectional mean return and population standard deviation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Eligible daily panel.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame indexed by DATE with columns 'mean_ret' and 'sigma_xs'.\n",
        "    \"\"\"\n",
        "    # Group by DATE\n",
        "    grouped = df.groupby(\"DATE\")[\"RET\"]\n",
        "\n",
        "    # Compute Mean\n",
        "    # Equation: \\bar{r}_d = (1/N_d) * sum(r_{i,d})\n",
        "    mean_ret = grouped.mean().rename(\"mean_ret\")\n",
        "\n",
        "    # Compute Population Standard Deviation (ddof=0)\n",
        "    # Equation: sigma^{xs}_d = sqrt((1/N_d) * sum((r_{i,d} - \\bar{r}_d)^2))\n",
        "    sigma_xs = grouped.std(ddof=0).rename(\"sigma_xs\")\n",
        "\n",
        "    # Combine into DataFrame\n",
        "    return pd.concat([mean_ret, sigma_xs], axis=1)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 2: Compute daily cross-sectional skewness.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_skewness(\n",
        "    df: pd.DataFrame,\n",
        "    daily_stats: pd.DataFrame,\n",
        "    epsilon: float = 1e-8\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the daily cross-sectional skewness using the population formula.\n",
        "    Handles cases with near-zero dispersion by setting skewness to NaN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Eligible daily panel.\n",
        "    daily_stats : pd.DataFrame\n",
        "        DataFrame containing 'mean_ret' and 'sigma_xs'.\n",
        "    epsilon : float\n",
        "        Threshold for zero dispersion.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Series of daily skewness indexed by DATE.\n",
        "    \"\"\"\n",
        "    # Merge daily mean back to panel to center returns\n",
        "    # We use the index (DATE) for merging\n",
        "    df_merged = df.join(daily_stats[\"mean_ret\"], on=\"DATE\")\n",
        "\n",
        "    # Compute centered returns\n",
        "    df_merged[\"centered_ret\"] = df_merged[\"RET\"] - df_merged[\"mean_ret\"]\n",
        "\n",
        "    # Compute 3rd central moment: mean((r - r_bar)^3)\n",
        "    # Note: groupby().mean() divides by N_d, which matches the population formula 1/N_d\n",
        "    m3 = df_merged.groupby(\"DATE\")[\"centered_ret\"].apply(lambda x: (x**3).mean())\n",
        "\n",
        "    # Get sigma_xs aligned\n",
        "    sigma = daily_stats[\"sigma_xs\"]\n",
        "\n",
        "    # Compute Skewness\n",
        "    # Equation: Skew = m3 / sigma^3\n",
        "    # Handle division by zero\n",
        "    valid_sigma = sigma > epsilon\n",
        "    skew_xs = pd.Series(np.nan, index=sigma.index, name=\"Skew_xs\")\n",
        "    skew_xs[valid_sigma] = m3[valid_sigma] / (sigma[valid_sigma] ** 3)\n",
        "\n",
        "    return skew_xs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Compute daily cross-sectional kurtosis.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_kurtosis(\n",
        "    df: pd.DataFrame,\n",
        "    daily_stats: pd.DataFrame,\n",
        "    epsilon: float = 1e-8\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the daily cross-sectional raw kurtosis using the population formula.\n",
        "    Handles cases with near-zero dispersion by setting kurtosis to NaN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Eligible daily panel.\n",
        "    daily_stats : pd.DataFrame\n",
        "        DataFrame containing 'mean_ret' and 'sigma_xs'.\n",
        "    epsilon : float\n",
        "        Threshold for zero dispersion.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Series of daily kurtosis indexed by DATE.\n",
        "    \"\"\"\n",
        "    # Merge daily mean back (or reuse if we passed the merged frame, but for modularity we re-merge or assume logic holds)\n",
        "    # Optimization: In orchestrator, we can pass the centered returns, but here we stick to the interface.\n",
        "    df_merged = df.join(daily_stats[\"mean_ret\"], on=\"DATE\")\n",
        "    df_merged[\"centered_ret\"] = df_merged[\"RET\"] - df_merged[\"mean_ret\"]\n",
        "\n",
        "    # Compute 4th central moment: mean((r - r_bar)^4)\n",
        "    m4 = df_merged.groupby(\"DATE\")[\"centered_ret\"].apply(lambda x: (x**4).mean())\n",
        "\n",
        "    # Get sigma_xs aligned\n",
        "    sigma = daily_stats[\"sigma_xs\"]\n",
        "\n",
        "    # Compute Kurtosis (Raw)\n",
        "    # Equation: Kurt = m4 / sigma^4\n",
        "    valid_sigma = sigma > epsilon\n",
        "    kurt_xs = pd.Series(np.nan, index=sigma.index, name=\"Kurt_xs\")\n",
        "    kurt_xs[valid_sigma] = m4[valid_sigma] / (sigma[valid_sigma] ** 4)\n",
        "\n",
        "    return kurt_xs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_daily_return_moments(\n",
        "    df_eligible: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of daily cross-sectional return moments:\n",
        "    Mean, Dispersion (Sigma), Skewness, and Kurtosis.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_eligible : pd.DataFrame\n",
        "        The eligible daily stock panel.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame indexed by DATE containing the computed moments.\n",
        "    \"\"\"\n",
        "    epsilon = study_config[\"feature_engineering\"].get(\"std_epsilon\", 1e-8)\n",
        "\n",
        "    # Step 1: Mean and Dispersion\n",
        "    daily_stats = _compute_mean_and_dispersion(df_eligible)\n",
        "\n",
        "    # Step 2: Skewness\n",
        "    skew_xs = _compute_skewness(df_eligible, daily_stats, epsilon)\n",
        "\n",
        "    # Step 3: Kurtosis\n",
        "    kurt_xs = _compute_kurtosis(df_eligible, daily_stats, epsilon)\n",
        "\n",
        "    # Combine all moments\n",
        "    final_stats = pd.concat([daily_stats, skew_xs, kurt_xs], axis=1)\n",
        "\n",
        "    return final_stats\n"
      ],
      "metadata": {
        "id": "qyTmoVVlnHFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 — Compute daily tail participation shares and mean absolute return\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Compute daily tail participation shares and mean absolute return\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Compute mean absolute return.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_mean_absolute_return(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the daily cross-sectional mean absolute return.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Eligible daily panel.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Series of mean absolute returns indexed by DATE.\n",
        "    \"\"\"\n",
        "    # Equation: \\overline{|r|}_d = (1/N_d) * sum(|r_{i,d}|)\n",
        "    return df[\"RET\"].abs().groupby(df[\"DATE\"]).mean().rename(\"mean_abs_ret\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 2: Compute fraction of extreme losers.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_frac_down(\n",
        "    df: pd.DataFrame,\n",
        "    tau: float\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the daily fraction of stocks with returns <= -tau.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Eligible daily panel.\n",
        "    tau : float\n",
        "        Tail threshold (e.g., 0.05).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Series of downside tail fractions indexed by DATE.\n",
        "    \"\"\"\n",
        "    # Equation: Frac^{dn}_d(tau) = (1/N_d) * sum(I{r_{i,d} <= -tau})\n",
        "    # Note: Inequality is inclusive per paper definition\n",
        "    is_down = df[\"RET\"] <= -tau\n",
        "    return is_down.groupby(df[\"DATE\"]).mean().rename(f\"Frac_dn_{int(tau*100)}pct\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Compute fraction of extreme winners.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_frac_up(\n",
        "    df: pd.DataFrame,\n",
        "    tau: float\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the daily fraction of stocks with returns >= tau.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Eligible daily panel.\n",
        "    tau : float\n",
        "        Tail threshold (e.g., 0.05).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Series of upside tail fractions indexed by DATE.\n",
        "    \"\"\"\n",
        "    # Equation: Frac^{up}_d(tau) = (1/N_d) * sum(I{r_{i,d} >= tau})\n",
        "    is_up = df[\"RET\"] >= tau\n",
        "    return is_up.groupby(df[\"DATE\"]).mean().rename(f\"Frac_up_{int(tau*100)}pct\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_daily_tail_measures(\n",
        "    df_eligible: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of daily tail risk measures and mean absolute return.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_eligible : pd.DataFrame\n",
        "        The eligible daily stock panel.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame indexed by DATE containing the computed tail measures.\n",
        "    \"\"\"\n",
        "    tau = study_config[\"feature_engineering\"][\"tail_threshold_tau\"]\n",
        "\n",
        "    # Step 1: Mean Absolute Return\n",
        "    mean_abs_ret = _compute_mean_absolute_return(df_eligible)\n",
        "\n",
        "    # Step 2: Downside Tail Fraction\n",
        "    frac_dn = _compute_frac_down(df_eligible, tau)\n",
        "\n",
        "    # Step 3: Upside Tail Fraction\n",
        "    frac_up = _compute_frac_up(df_eligible, tau)\n",
        "\n",
        "    # Combine\n",
        "    final_tails = pd.concat([mean_abs_ret, frac_dn, frac_up], axis=1)\n",
        "\n",
        "    return final_tails\n"
      ],
      "metadata": {
        "id": "6Wfb1ygmoFiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 — Compute daily trading-intensity proxies\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Compute daily trading-intensity proxies\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Compute cross-sectional average log volume.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_log_volume(\n",
        "    df: pd.DataFrame,\n",
        "    offset: float = 1.0\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Computes the daily cross-sectional average of log(1 + Volume).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Eligible daily panel.\n",
        "    offset : float\n",
        "        Offset for log transformation (default 1.0).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.Series, pd.Series]\n",
        "        - Series of mean log volume indexed by DATE.\n",
        "        - Series of valid volume counts (N_vol) indexed by DATE.\n",
        "    \"\"\"\n",
        "    # Filter valid volume\n",
        "    valid_vol = df[\"VOL\"].dropna()\n",
        "    valid_vol = valid_vol[valid_vol >= 0]\n",
        "\n",
        "    # Compute log transform\n",
        "    log_vol = np.log(offset + valid_vol)\n",
        "\n",
        "    # Group by DATE (using the index of the filtered series which aligns with df)\n",
        "    grouped = log_vol.groupby(df.loc[log_vol.index, \"DATE\"])\n",
        "\n",
        "    mean_log_vol = grouped.mean().rename(\"mean_log1p_vol\")\n",
        "    n_vol = grouped.count().rename(\"N_vol\")\n",
        "\n",
        "    return mean_log_vol, n_vol\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Compute cross-sectional average dollar volume.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_dollar_volume(\n",
        "    df: pd.DataFrame\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Computes the daily cross-sectional average dollar volume (|Price| * Volume).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Eligible daily panel.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.Series, pd.Series]\n",
        "        - Series of mean dollar volume indexed by DATE.\n",
        "        - Series of valid dollar volume counts (N_dol) indexed by DATE.\n",
        "    \"\"\"\n",
        "    # Compute dollar volume\n",
        "    # We use abs_prc which is guaranteed non-negative\n",
        "    # We must ensure both VOL and abs_prc are valid\n",
        "    valid_mask = df[\"VOL\"].notna() & (df[\"VOL\"] >= 0) & df[\"abs_prc\"].notna()\n",
        "    df_valid = df[valid_mask].copy()\n",
        "\n",
        "    df_valid[\"dollar_vol\"] = df_valid[\"abs_prc\"] * df_valid[\"VOL\"]\n",
        "\n",
        "    grouped = df_valid.groupby(\"DATE\")[\"dollar_vol\"]\n",
        "\n",
        "    mean_dol_vol = grouped.mean().rename(\"mean_dollar_vol\")\n",
        "    n_dol = grouped.count().rename(\"N_dol\")\n",
        "\n",
        "    return mean_dol_vol, n_dol\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Compute cross-sectional average turnover.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_turnover(\n",
        "    df: pd.DataFrame,\n",
        "    shares_scaling: float = 1000.0\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Computes the daily cross-sectional average turnover (Volume / SharesOutstanding).\n",
        "    Handles CRSP unit conventions (SHROUT in thousands).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Eligible daily panel.\n",
        "    shares_scaling : float\n",
        "        Scaling factor for SHROUT (default 1000.0).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.Series, pd.Series]\n",
        "        - Series of mean turnover indexed by DATE.\n",
        "        - Series of valid turnover counts (N_turn) indexed by DATE.\n",
        "    \"\"\"\n",
        "    # Filter valid inputs\n",
        "    # SHROUT must be positive for division\n",
        "    valid_mask = (\n",
        "        df[\"VOL\"].notna() & (df[\"VOL\"] >= 0) &\n",
        "        df[\"SHROUT\"].notna() & (df[\"SHROUT\"] > 0)\n",
        "    )\n",
        "    df_valid = df[valid_mask].copy()\n",
        "\n",
        "    # Compute Turnover\n",
        "    # Equation: Turn = Vol / (1000 * SHROUT)\n",
        "    df_valid[\"turnover\"] = df_valid[\"VOL\"] / (shares_scaling * df_valid[\"SHROUT\"])\n",
        "\n",
        "    # Filter infinite values if any (though SHROUT > 0 prevents div/0)\n",
        "    df_valid = df_valid[np.isfinite(df_valid[\"turnover\"])]\n",
        "\n",
        "    grouped = df_valid.groupby(\"DATE\")[\"turnover\"]\n",
        "\n",
        "    mean_turn = grouped.mean().rename(\"mean_turnover\")\n",
        "    n_turn = grouped.count().rename(\"N_turn\")\n",
        "\n",
        "    return mean_turn, n_turn\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_daily_trading_proxies(\n",
        "    df_eligible: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of daily trading intensity proxies:\n",
        "    Log Volume, Dollar Volume, and Turnover.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_eligible : pd.DataFrame\n",
        "        The eligible daily stock panel.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame indexed by DATE containing the computed proxies and coverage counts.\n",
        "    \"\"\"\n",
        "    # Extract Key Parameters\n",
        "    feat_config = study_config[\"feature_engineering\"]\n",
        "    log_offset = feat_config.get(\"log_volume_offset\", 1.0)\n",
        "    shares_scaling = feat_config[\"turnover\"].get(\"shares_out_scaling\", 1000.0)\n",
        "\n",
        "    # Step 1: Log Volume\n",
        "    mean_log_vol, n_vol = _compute_log_volume(df_eligible, log_offset)\n",
        "\n",
        "    # Step 2: Dollar Volume\n",
        "    mean_dol_vol, n_dol = _compute_dollar_volume(df_eligible)\n",
        "\n",
        "    # Step 3: Turnover\n",
        "    mean_turn, n_turn = _compute_turnover(df_eligible, shares_scaling)\n",
        "\n",
        "    # Combine\n",
        "    final_proxies = pd.concat([\n",
        "        mean_log_vol, n_vol,\n",
        "        mean_dol_vol, n_dol,\n",
        "        mean_turn, n_turn\n",
        "    ], axis=1)\n",
        "\n",
        "    return final_proxies\n"
      ],
      "metadata": {
        "id": "rP26gRCYokAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Aggregate daily cross-sectional statistics to monthly predictors X_t\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Aggregate daily cross-sectional statistics to monthly predictors X_t\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Apply the paper's within-month averaging operator to every daily statistic.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _aggregate_daily_to_monthly(\n",
        "    daily_stats: pd.DataFrame,\n",
        "    nd_series: pd.Series\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Aggregates daily statistics to monthly frequency by computing the within-month mean.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    daily_stats : pd.DataFrame\n",
        "        DataFrame of daily statistics indexed by DATE.\n",
        "    nd_series : pd.Series\n",
        "        Series of daily cross-section sizes (Nd) indexed by DATE.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.Series]\n",
        "        - Monthly features DataFrame indexed by year_month.\n",
        "        - Series of trading days per month (Dt).\n",
        "    \"\"\"\n",
        "    # Merge Nd into stats for aggregation\n",
        "    df_all = daily_stats.join(nd_series, how=\"inner\")\n",
        "\n",
        "    # Ensure year_month column exists (it was created in Task 4/6 but might be lost if index was DATE)\n",
        "    # We need to recover it from the DATE index\n",
        "    if \"year_month\" not in df_all.columns:\n",
        "        # Assuming DATE index is datetime\n",
        "        df_all[\"year_month\"] = df_all.index.to_period(\"M\")\n",
        "\n",
        "    # Group by year_month\n",
        "    grouped = df_all.groupby(\"year_month\")\n",
        "\n",
        "    # Compute monthly means (Z_t = 1/D_t * sum(z_d))\n",
        "    monthly_stats = grouped.mean()\n",
        "\n",
        "    # Compute D_t (number of trading days)\n",
        "    dt_series = grouped.size().rename(\"n_trading_days\")\n",
        "\n",
        "    return monthly_stats, dt_series\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Assemble the monthly feature vector X_t with canonical column names.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _format_monthly_features(\n",
        "    monthly_stats: pd.DataFrame,\n",
        "    feature_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Renames and reorders columns to match the canonical feature set X_t.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    monthly_stats : pd.DataFrame\n",
        "        Raw monthly aggregated statistics.\n",
        "    feature_config : Dict[str, Any]\n",
        "        Configuration containing the final feature names list.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Canonical X_t DataFrame.\n",
        "    \"\"\"\n",
        "    # Define mapping from daily stat names to monthly feature names\n",
        "    # This mapping must align with the column names produced in Tasks 6-9\n",
        "    # Daily Name -> Monthly Name\n",
        "    rename_map = {\n",
        "        \"n_stocks\": \"n_stocks\",\n",
        "        \"sigma_xs\": \"sigma_xs\",\n",
        "        \"Skew_xs\": \"Skew_xs\",\n",
        "        \"Kurt_xs\": \"Kurt_xs\",\n",
        "        \"mean_abs_ret\": \"mean_abs_ret\",\n",
        "        \"Frac_dn_5pct\": \"Frac_dn_5pct\",\n",
        "        \"Frac_up_5pct\": \"Frac_up_5pct\",\n",
        "        \"mean_log1p_vol\": \"mean_log1p_vol\",\n",
        "        \"mean_dollar_vol\": \"mean_dollar_vol\",\n",
        "        \"mean_turnover\": \"mean_turnover\"\n",
        "    }\n",
        "\n",
        "    # Rename\n",
        "    df_renamed = monthly_stats.rename(columns=rename_map)\n",
        "\n",
        "    # Select and Reorder\n",
        "    final_cols = feature_config[\"final_feature_names_monthly\"]\n",
        "\n",
        "    # Check for missing columns\n",
        "    missing = [c for c in final_cols if c not in df_renamed.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns for X_t: {missing}\")\n",
        "\n",
        "    return df_renamed[final_cols]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Drop months with insufficient daily coverage.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _filter_insufficient_months(\n",
        "    xt: pd.DataFrame,\n",
        "    dt_series: pd.Series,\n",
        "    min_days: int = 15\n",
        ") -> Tuple[pd.DataFrame, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Drops months with fewer than `min_days` trading days to ensure robust monthly estimates.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xt : pd.DataFrame\n",
        "        Monthly feature DataFrame.\n",
        "    dt_series : pd.Series\n",
        "        Series of trading days per month.\n",
        "    min_days : int\n",
        "        Minimum required trading days (default 15).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, int]]\n",
        "        - Filtered X_t DataFrame.\n",
        "        - Drop statistics.\n",
        "    \"\"\"\n",
        "    # Align indices\n",
        "    dt_aligned = dt_series.reindex(xt.index)\n",
        "\n",
        "    # Identify valid months\n",
        "    valid_mask = dt_aligned >= min_days\n",
        "\n",
        "    # Filter\n",
        "    xt_filtered = xt[valid_mask].copy()\n",
        "    dropped_count = len(xt) - len(xt_filtered)\n",
        "\n",
        "    stats = {\n",
        "        \"months_dropped_insufficient_days\": int(dropped_count),\n",
        "        \"min_trading_days_threshold\": min_days\n",
        "    }\n",
        "\n",
        "    return xt_filtered, stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def aggregate_monthly_features(\n",
        "    daily_stats_all: pd.DataFrame,\n",
        "    nd_series: pd.Series,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the aggregation of daily statistics into the monthly feature matrix X_t.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    daily_stats_all : pd.DataFrame\n",
        "        Consolidated DataFrame of all daily statistics (moments, tails, proxies).\n",
        "    nd_series : pd.Series\n",
        "        Series of daily universe sizes.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        - 'X_t': Monthly feature DataFrame.\n",
        "        - 'audit_log': Aggregation statistics.\n",
        "    \"\"\"\n",
        "    feat_config = study_config[\"feature_engineering\"]\n",
        "    # Default min days to 15 if not specified (safe for monthly stats)\n",
        "    min_days = feat_config.get(\"min_trading_days_per_month\", 15)\n",
        "\n",
        "    # Step 1: Aggregate\n",
        "    monthly_raw, dt_series = _aggregate_daily_to_monthly(daily_stats_all, nd_series)\n",
        "\n",
        "    # Step 2: Format\n",
        "    xt_formatted = _format_monthly_features(monthly_raw, feat_config)\n",
        "\n",
        "    # Step 3: Filter Coverage\n",
        "    xt_final, stats = _filter_insufficient_months(xt_formatted, dt_series, min_days)\n",
        "\n",
        "    audit_log = {\n",
        "        **stats,\n",
        "        \"final_month_count\": len(xt_final)\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"X_t\": xt_final,\n",
        "        \"audit_log\": audit_log\n",
        "    }\n"
      ],
      "metadata": {
        "id": "uygeBJN88RN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 — Construct monthly market aggregates \\(R^{mkt}_t\\) and \\(\\sigma^{mkt}_t\\)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Construct monthly market aggregates R^{mkt}_t and sigma^{mkt}_t\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Compute monthly market return by compounding daily `vwretd`.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_monthly_market_return(\n",
        "    df_index: pd.DataFrame,\n",
        "    min_days: int = 15\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the monthly market return by compounding daily value-weighted returns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_index : pd.DataFrame\n",
        "        Cleansed CRSP Index DataFrame.\n",
        "    min_days : int\n",
        "        Minimum trading days required to compute a valid monthly return.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Series of monthly market returns indexed by year_month.\n",
        "    \"\"\"\n",
        "    # Group by year_month\n",
        "    grouped = df_index.groupby(\"year_month\")[\"vwretd\"]\n",
        "\n",
        "    # Filter groups with sufficient days\n",
        "    # We do this by computing count first\n",
        "    counts = grouped.count()\n",
        "    valid_months = counts[counts >= min_days].index\n",
        "\n",
        "    # Filter original df to valid months for efficiency (or just apply and mask)\n",
        "    # Applying directly is cleaner\n",
        "    def compound_ret(x):\n",
        "        if len(x) < min_days:\n",
        "            return np.nan\n",
        "        return np.prod(1 + x) - 1\n",
        "\n",
        "    # Compute compounded return\n",
        "    # Equation: R_t = prod(1 + r_d) - 1\n",
        "    r_mkt = grouped.apply(compound_ret).rename(\"R_mkt\")\n",
        "\n",
        "    return r_mkt.dropna()\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Compute monthly realized volatility (annualized).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_monthly_realized_vol(\n",
        "    df_index: pd.DataFrame,\n",
        "    study_config: Dict[str, Any],\n",
        "    min_days: int = 15\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the annualized monthly realized volatility.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_index : pd.DataFrame\n",
        "        Cleansed CRSP Index DataFrame.\n",
        "    study_config : Dict[str, Any]\n",
        "        Configuration containing annualization factor and ddof.\n",
        "    min_days : int\n",
        "        Minimum trading days required.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Series of annualized realized volatility indexed by year_month.\n",
        "    \"\"\"\n",
        "    agg_config = study_config[\"market_aggregates\"]\n",
        "    ann_factor = agg_config.get(\"annualization_factor\", 252**0.5)\n",
        "    ddof = agg_config.get(\"realized_vol_ddof\", 1)\n",
        "\n",
        "    grouped = df_index.groupby(\"year_month\")[\"vwretd\"]\n",
        "\n",
        "    def realized_vol(x):\n",
        "        if len(x) < min_days:\n",
        "            return np.nan\n",
        "        return x.std(ddof=ddof) * ann_factor\n",
        "\n",
        "    # Equation: sigma_t = sqrt(252) * std(r_d)\n",
        "    sigma_mkt = grouped.apply(realized_vol).rename(\"sigma_mkt\")\n",
        "\n",
        "    return sigma_mkt.dropna()\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Align monthly market aggregates to the same year_month index as X_t.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _align_market_and_features(\n",
        "    xt: pd.DataFrame,\n",
        "    r_mkt: pd.Series,\n",
        "    sigma_mkt: pd.Series\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aligns market aggregates with the feature matrix X_t, ensuring a common sample.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xt : pd.DataFrame\n",
        "        Monthly feature matrix X_t.\n",
        "    r_mkt : pd.Series\n",
        "        Monthly market return.\n",
        "    sigma_mkt : pd.Series\n",
        "        Monthly realized volatility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Merged DataFrame containing X_t, R_mkt, and sigma_mkt.\n",
        "    \"\"\"\n",
        "    # Combine market vars\n",
        "    market_df = pd.concat([r_mkt, sigma_mkt], axis=1)\n",
        "\n",
        "    # Inner join with X_t to ensure common months\n",
        "    # This drops months where either features or market data are missing/insufficient\n",
        "    aligned_df = xt.join(market_df, how=\"inner\")\n",
        "\n",
        "    return aligned_df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def construct_market_aggregates(\n",
        "    df_crsp_index_clean: pd.DataFrame,\n",
        "    xt: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of monthly market aggregates and aligns them with features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_crsp_index_clean : pd.DataFrame\n",
        "        Cleansed CRSP Index DataFrame.\n",
        "    xt : pd.DataFrame\n",
        "        Monthly feature matrix X_t.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        - 'aligned_panel': DataFrame with features and market aggregates.\n",
        "        - 'audit_log': Construction statistics.\n",
        "    \"\"\"\n",
        "    min_days = study_config[\"feature_engineering\"].get(\"min_trading_days_per_month\", 15)\n",
        "\n",
        "    # Step 1: Market Return\n",
        "    r_mkt = _compute_monthly_market_return(df_crsp_index_clean, min_days)\n",
        "\n",
        "    # Step 2: Realized Volatility\n",
        "    sigma_mkt = _compute_monthly_realized_vol(df_crsp_index_clean, study_config, min_days)\n",
        "\n",
        "    # Step 3: Alignment\n",
        "    aligned_panel = _align_market_and_features(xt, r_mkt, sigma_mkt)\n",
        "\n",
        "    audit_log = {\n",
        "        \"market_return_months\": len(r_mkt),\n",
        "        \"realized_vol_months\": len(sigma_mkt),\n",
        "        \"aligned_months\": len(aligned_panel),\n",
        "        \"dropped_months_alignment\": len(xt) - len(aligned_panel)\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"aligned_panel\": aligned_panel,\n",
        "        \"audit_log\": audit_log\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Rg_90ad0_t0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 — Construct real-time stress labels \\(S_t\\) and supervised targets \\(Y_{t+1}\\)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Construct real-time stress labels S_t and supervised targets Y_{t+1}\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Compute the expanding real-time volatility quantile \\(q_{t-1}(\\alpha)\\).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_expanding_vol_quantile(\n",
        "    sigma_mkt: pd.Series,\n",
        "    alpha: float = 0.90,\n",
        "    min_history: int = 12,\n",
        "    interpolation: str = \"linear\"\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the expanding real-time volatility quantile q_{t-1}(alpha).\n",
        "    Strictly uses data up to t-1 to define the threshold for month t.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma_mkt : pd.Series\n",
        "        Monthly realized volatility indexed by year_month.\n",
        "    alpha : float\n",
        "        Quantile level (e.g., 0.90).\n",
        "    min_history : int\n",
        "        Minimum months of history required to compute a quantile.\n",
        "    interpolation : str\n",
        "        Interpolation method for quantile.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Series of quantiles indexed by year_month (aligned to the month t for which it serves as threshold).\n",
        "    \"\"\"\n",
        "    # We need q_{t-1}.\n",
        "    # Pandas expanding().quantile() at index t includes index t.\n",
        "    # So we compute expanding quantile on the series, then shift by 1.\n",
        "    # The shift(1) moves the quantile computed at t-1 to position t.\n",
        "    quantiles = sigma_mkt.expanding(min_periods=min_history).quantile(\n",
        "        quantile=alpha,\n",
        "        interpolation=interpolation\n",
        "    ).shift(1)\n",
        "\n",
        "    quantiles.name = f\"q_vol_{int(alpha*100)}\"\n",
        "    return quantiles\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 2: Apply the paper's stress definition.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _define_stress_label(\n",
        "    r_mkt: pd.Series,\n",
        "    sigma_mkt: pd.Series,\n",
        "    q_vol: pd.Series,\n",
        "    c_r: float = -0.05\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Constructs the binary stress label S_t based on market return and volatility.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    r_mkt : pd.Series\n",
        "        Monthly market return.\n",
        "    sigma_mkt : pd.Series\n",
        "        Monthly realized volatility.\n",
        "    q_vol : pd.Series\n",
        "        Expanding volatility quantile threshold (q_{t-1}).\n",
        "    c_r : float\n",
        "        Return cutoff (e.g., -0.05).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Binary Series S_t (1 if stress, 0 if not, NaN if threshold undefined).\n",
        "    \"\"\"\n",
        "    # Align all series\n",
        "    df = pd.concat([r_mkt, sigma_mkt, q_vol], axis=1)\n",
        "    df.columns = [\"R_mkt\", \"sigma_mkt\", \"q_vol\"]\n",
        "\n",
        "    # Initialize as NaN (unknown stress status if history insufficient)\n",
        "    s_t = pd.Series(np.nan, index=df.index, name=\"S_t\")\n",
        "\n",
        "    # Valid mask: where we have data and a valid threshold\n",
        "    valid_mask = df[\"q_vol\"].notna() & df[\"R_mkt\"].notna() & df[\"sigma_mkt\"].notna()\n",
        "\n",
        "    # Apply definition on valid rows\n",
        "    # S_t = 1 if (R_t <= c_R) OR (sigma_t >= q_{t-1})\n",
        "    # Note: inequalities are inclusive per paper\n",
        "    is_crash = df.loc[valid_mask, \"R_mkt\"] <= c_r\n",
        "    is_vol_spike = df.loc[valid_mask, \"sigma_mkt\"] >= df.loc[valid_mask, \"q_vol\"]\n",
        "\n",
        "    s_t[valid_mask] = (is_crash | is_vol_spike).astype(int)\n",
        "\n",
        "    return s_t\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Define the supervised target and enforce real-time feasibility assertion.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _create_supervised_target(\n",
        "    panel: pd.DataFrame,\n",
        "    s_t: pd.Series\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates the supervised learning target Y_{t+1} = S_{t+1} and aligns it with predictors X_t.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    panel : pd.DataFrame\n",
        "        Panel containing features and contemporaneous market vars (X_t, R_t, sigma_t).\n",
        "    s_t : pd.Series\n",
        "        Stress label S_t.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Panel with added 'S_t' and 'Y_next' columns.\n",
        "        Rows where Y_next is NaN (last month) are dropped or kept as NaN depending on policy.\n",
        "        Here we keep them but they can't be used for training.\n",
        "    \"\"\"\n",
        "    # Add S_t to panel\n",
        "    panel = panel.copy()\n",
        "    panel[\"S_t\"] = s_t\n",
        "\n",
        "    # Create Target Y_{t+1}\n",
        "    # Shift S_t backwards by 1: The label at t+1 becomes the target for row t\n",
        "    panel[\"Y_next\"] = s_t.shift(-1)\n",
        "\n",
        "    # Assert Real-Time Feasibility\n",
        "    # We verify that Y_next for row t corresponds to S_{t+1}\n",
        "    # (This is guaranteed by shift(-1) on a sorted index, but we assert index monotonicity)\n",
        "    if not panel.index.is_monotonic_increasing:\n",
        "        raise ValueError(\"Panel index must be sorted chronologically to create targets.\")\n",
        "\n",
        "    return panel\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def construct_stress_labels(\n",
        "    aligned_panel: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of stress labels and supervised targets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    aligned_panel : pd.DataFrame\n",
        "        Panel with X_t, R_mkt, sigma_mkt.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        - 'modeling_panel': Final DataFrame with features, labels, and targets.\n",
        "        - 'audit_log': Labeling statistics.\n",
        "    \"\"\"\n",
        "    # Extract parameters\n",
        "    target_config = study_config[\"target_definition\"]\n",
        "    alpha = target_config[\"vol_quantile_alpha\"]\n",
        "    c_r = target_config[\"return_cutoff_c_R\"]\n",
        "    min_hist = target_config.get(\"min_vol_history_months\", 12)\n",
        "    interp = target_config.get(\"vol_quantile_interpolation\", \"linear\")\n",
        "\n",
        "    # Step 1: Volatility Threshold\n",
        "    q_vol = _compute_expanding_vol_quantile(\n",
        "        aligned_panel[\"sigma_mkt\"], alpha, min_hist, interp\n",
        "    )\n",
        "\n",
        "    # Step 2: Stress Label S_t\n",
        "    s_t = _define_stress_label(\n",
        "        aligned_panel[\"R_mkt\"], aligned_panel[\"sigma_mkt\"], q_vol, c_r\n",
        "    )\n",
        "\n",
        "    # Step 3: Target Y_{t+1}\n",
        "    final_panel = _create_supervised_target(aligned_panel, s_t)\n",
        "\n",
        "    # Audit\n",
        "    valid_targets = final_panel[\"Y_next\"].notna().sum()\n",
        "    stress_rate = final_panel[\"Y_next\"].mean()\n",
        "\n",
        "    audit_log = {\n",
        "        \"months_with_valid_threshold\": q_vol.notna().sum(),\n",
        "        \"months_with_valid_target\": int(valid_targets),\n",
        "        \"unconditional_stress_rate\": float(stress_rate) if valid_targets > 0 else 0.0\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"modeling_panel\": final_panel,\n",
        "        \"audit_log\": audit_log\n",
        "    }\n"
      ],
      "metadata": {
        "id": "RUEzXcdHrP_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 — Define the expanding-window standardization callable\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Define the expanding-window standardization callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Implement training-window mean and standard deviation computation.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_training_moments(\n",
        "    x_train: pd.DataFrame,\n",
        "    std_epsilon: float = 1e-8\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Computes mean and population standard deviation on the training window.\n",
        "    Enforces complete-case analysis (drops rows with NaNs) to ensure robust moments.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x_train : pd.DataFrame\n",
        "        Feature matrix for the training window.\n",
        "    std_epsilon : float\n",
        "        Threshold for zero variance.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.Series, pd.Series]\n",
        "        - Means indexed by feature name.\n",
        "        - Standard deviations indexed by feature name.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If a feature is entirely NaN in the training window.\n",
        "    \"\"\"\n",
        "    # Enforce complete-case for moment estimation\n",
        "    # This avoids bias from inconsistent sample sizes across features\n",
        "    x_clean = x_train.dropna()\n",
        "\n",
        "    if len(x_clean) == 0:\n",
        "        # If dropping NaNs leaves no data, we can't standardize.\n",
        "        # However, if x_train was not empty but had NaNs, this is a data quality issue.\n",
        "        # We check if columns were all-NaN originally.\n",
        "        all_nan_cols = x_train.columns[x_train.isna().all()].tolist()\n",
        "        if all_nan_cols:\n",
        "            raise ValueError(f\"Features entirely NaN in training window: {all_nan_cols}\")\n",
        "        # If not all-NaN but rows disjointly missing, still an issue\n",
        "        raise ValueError(\"Training window has 0 complete cases after dropping NaNs.\")\n",
        "\n",
        "    # Compute Moments\n",
        "    mu = x_clean.mean()\n",
        "\n",
        "    # Population std (ddof=0) per paper\n",
        "    sigma = x_clean.std(ddof=0)\n",
        "\n",
        "    return mu, sigma\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Apply z-score transformation.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _apply_zscore(\n",
        "    x: Union[pd.DataFrame, pd.Series],\n",
        "    mu: pd.Series,\n",
        "    sigma: pd.Series,\n",
        "    std_epsilon: float = 1e-8\n",
        ") -> Union[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Applies z-score transformation using provided moments.\n",
        "    Handles zero-variance features by setting standardized values to 0.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : pd.DataFrame or pd.Series\n",
        "        Data to standardize.\n",
        "    mu : pd.Series\n",
        "        Training means.\n",
        "    sigma : pd.Series\n",
        "        Training standard deviations.\n",
        "    std_epsilon : float\n",
        "        Threshold for zero variance.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Standardized data.\n",
        "    \"\"\"\n",
        "    # Identify zero-variance features\n",
        "    zero_var_mask = sigma <= std_epsilon\n",
        "\n",
        "    # Avoid division by zero\n",
        "    # Replace 0 sigma with 1 (so division does nothing), then mask result to 0\n",
        "    safe_sigma = sigma.copy()\n",
        "    safe_sigma[zero_var_mask] = 1.0\n",
        "\n",
        "    # Standardize\n",
        "    # (x - mu) / sigma\n",
        "    # Pandas aligns on columns automatically\n",
        "    z = (x - mu) / safe_sigma\n",
        "\n",
        "    # Enforce zero-variance policy: set to 0\n",
        "    if isinstance(z, pd.DataFrame):\n",
        "        z.loc[:, zero_var_mask] = 0.0\n",
        "    else:\n",
        "        # Series case (single row)\n",
        "        z[zero_var_mask] = 0.0\n",
        "\n",
        "    return z\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def expanding_window_standardizer(\n",
        "    x_panel: pd.DataFrame,\n",
        "    train_end_idx: Any,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates expanding-window standardization.\n",
        "    Computes moments on X[...:train_end_idx] and transforms both training and current data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x_panel : pd.DataFrame\n",
        "        Full feature panel indexed by year_month.\n",
        "    train_end_idx : Any\n",
        "        The label of the last month in the training set (inclusive).\n",
        "        The 'current' month is assumed to be the one immediately following,\n",
        "        or we can just return the scaler for external use.\n",
        "        Here, we return the standardized training set and the scaler params.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        - 'x_train_std': Standardized training DataFrame.\n",
        "        - 'scaler_params': Dict with 'mu' and 'sigma'.\n",
        "    \"\"\"\n",
        "    epsilon = study_config[\"feature_engineering\"].get(\"std_epsilon\", 1e-8)\n",
        "\n",
        "    # Slice Training Data\n",
        "    # Get integer location of train_end_idx to slice safely if index is not unique (though it should be)\n",
        "    # Better: use boolean mask <= train_end_idx\n",
        "    train_mask = x_panel.index <= train_end_idx\n",
        "    x_train = x_panel.loc[train_mask]\n",
        "\n",
        "    # Step 1: Compute Moments\n",
        "    mu, sigma = _compute_training_moments(x_train, epsilon)\n",
        "\n",
        "    # Step 2: Transform Training Data\n",
        "    x_train_std = _apply_zscore(x_train, mu, sigma, epsilon)\n",
        "\n",
        "    return {\n",
        "        \"x_train_std\": x_train_std,\n",
        "        \"scaler_params\": {\"mu\": mu, \"sigma\": sigma}\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ukWhWNoClaBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 — Tune hyperparameters via forward-chaining time-series cross-validation in the initial window\n",
        "\n",
        "# ================================================================================\n",
        "# Task 14: Tune hyperparameters via forward-chaining time-series cross-validation\n",
        "# ================================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Define the initial training window and forward-chaining split structure.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _generate_cv_splits(\n",
        "    panel_index: pd.Index,\n",
        "    initial_window_months: int,\n",
        "    n_splits: int = 5,\n",
        "    val_size: int = 12,\n",
        "    step_size: int = 12\n",
        ") -> List[Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Generates forward-chaining time-series cross-validation splits within the initial window.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    panel_index : pd.Index\n",
        "        Index of the full modeling panel (year_month).\n",
        "    initial_window_months : int\n",
        "        Length of the initial training window (T0).\n",
        "    n_splits : int\n",
        "        Number of CV splits.\n",
        "    val_size : int\n",
        "        Size of validation set in months.\n",
        "    step_size : int\n",
        "        Step size between splits.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Tuple[np.ndarray, np.ndarray]]\n",
        "        List of (train_indices, val_indices) tuples.\n",
        "    \"\"\"\n",
        "    # Get unique sorted months in the initial window\n",
        "    # We assume panel_index is sorted\n",
        "    unique_months = panel_index.unique()\n",
        "    if len(unique_months) < initial_window_months:\n",
        "        raise ValueError(f\"Data length ({len(unique_months)}) < initial window ({initial_window_months})\")\n",
        "\n",
        "    initial_months = unique_months[:initial_window_months]\n",
        "    n_months = len(initial_months)\n",
        "\n",
        "    splits = []\n",
        "    # We work backwards from the end of the initial window to define splits\n",
        "    # Or forwards. Standard forward chaining:\n",
        "    # Split 1: Train [0...k], Val [k+1...k+v]\n",
        "    # Split 2: Train [0...k+s], Val [k+s+1...k+s+v]\n",
        "    # ...\n",
        "    # Last split validation should end at initial_window_months - 1\n",
        "\n",
        "    # Let's define the end of the last validation set as n_months - 1\n",
        "    # Start of last val set = n_months - val_size\n",
        "    # End of last train set = n_months - val_size - 1\n",
        "\n",
        "    # We iterate backwards to find split points\n",
        "    for i in range(n_splits):\n",
        "        val_end = n_months - 1 - (i * step_size)\n",
        "        val_start = val_end - val_size + 1\n",
        "        train_end = val_start - 1\n",
        "\n",
        "        if train_end < 0:\n",
        "            raise ValueError(\"Configuration results in empty training set for early splits.\")\n",
        "\n",
        "        # Get month labels\n",
        "        train_months = initial_months[:train_end+1]\n",
        "        val_months = initial_months[val_start:val_end+1]\n",
        "\n",
        "        # Map back to panel indices (boolean masks or integer positions)\n",
        "        # We use boolean masks for safety\n",
        "        train_mask = panel_index.isin(train_months)\n",
        "        val_mask = panel_index.isin(val_months)\n",
        "\n",
        "        # Convert to integer indices for sklearn compatibility if needed,\n",
        "        # but here we return masks or the subsetted data directly in the loop.\n",
        "        # Let's return the actual month labels for slicing.\n",
        "        splits.append((train_months, val_months))\n",
        "\n",
        "    # Reverse to be chronological (Split 1 is earliest)\n",
        "    return splits[::-1]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 2: Tune lambda for lasso-logit and ridge penalty for benchmark.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _tune_model_hyperparameters(\n",
        "    panel: pd.DataFrame,\n",
        "    splits: List[Tuple[pd.Index, pd.Index]],\n",
        "    features: List[str],\n",
        "    target: str,\n",
        "    penalty_type: str,\n",
        "    param_grid: List[float],\n",
        "    standardizer_func: callable,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Tunes regularization parameter using time-series CV.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    panel : pd.DataFrame\n",
        "        Modeling panel.\n",
        "    splits : List[Tuple[pd.Index, pd.Index]]\n",
        "        CV splits (train_months, val_months).\n",
        "    features : List[str]\n",
        "        List of feature column names.\n",
        "    target : str\n",
        "        Target column name.\n",
        "    penalty_type : str\n",
        "        'l1' or 'l2'.\n",
        "    param_grid : List[float]\n",
        "        Grid of C values (inverse regularization).\n",
        "    standardizer_func : callable\n",
        "        Function to standardize data (Task 13).\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Best hyperparameters and CV results.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for C in param_grid:\n",
        "        fold_scores = []\n",
        "\n",
        "        for train_months, val_months in splits:\n",
        "            # Slice Data\n",
        "            train_data = panel[panel.index.isin(train_months)]\n",
        "            val_data = panel[panel.index.isin(val_months)]\n",
        "\n",
        "            # Standardize (Fit on Train, Transform Train & Val)\n",
        "            # We use the standardizer from Task 13\n",
        "            # Note: Task 13 standardizer takes full panel and train_end_idx.\n",
        "            # Here we can just pass the train slice directly if we adapt or reuse logic.\n",
        "            # Let's reuse logic: compute moments on train, apply to both.\n",
        "\n",
        "            # Extract X and Y\n",
        "            X_train_raw = train_data[features]\n",
        "            y_train = train_data[target]\n",
        "            X_val_raw = val_data[features]\n",
        "            y_val = val_data[target]\n",
        "\n",
        "            # Standardize\n",
        "            # We manually call the internal logic of Task 13 for efficiency/clarity here\n",
        "            # or assume standardizer_func returns scaler params\n",
        "            std_result = standardizer_func(panel[features], train_months[-1], study_config)\n",
        "            scaler = std_result[\"scaler_params\"]\n",
        "\n",
        "            # Apply z-score (Task 13 logic)\n",
        "            # We need to import _apply_zscore or reimplement simple (x-mu)/sigma\n",
        "            # Reimplementing for self-containment using scaler params\n",
        "            epsilon = study_config[\"feature_engineering\"].get(\"std_epsilon\", 1e-8)\n",
        "\n",
        "            def apply_z(x, mu, sigma):\n",
        "                z = (x - mu) / sigma.replace(0, 1) # Handle 0 sigma\n",
        "                z.loc[:, sigma <= epsilon] = 0.0\n",
        "                return z\n",
        "\n",
        "            X_train = apply_z(X_train_raw, scaler[\"mu\"], scaler[\"sigma\"])\n",
        "            X_val = apply_z(X_val_raw, scaler[\"mu\"], scaler[\"sigma\"])\n",
        "\n",
        "            # Fit Model\n",
        "            # Solver: liblinear for small datasets supports l1/l2\n",
        "            model = LogisticRegression(\n",
        "                penalty=penalty_type,\n",
        "                C=C,\n",
        "                solver='liblinear', # Deterministic, supports l1/l2\n",
        "                random_state=study_config[\"reproducibility\"][\"random_seed\"],\n",
        "                fit_intercept=True\n",
        "            )\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Predict\n",
        "            probs = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "            # Score (Log Loss)\n",
        "            eps = study_config[\"evaluation_inference\"].get(\"proba_clip_epsilon\", 1e-6)\n",
        "            probs_clipped = np.clip(probs, eps, 1 - eps)\n",
        "            score = log_loss(y_val, probs_clipped)\n",
        "            fold_scores.append(score)\n",
        "\n",
        "        avg_score = np.mean(fold_scores)\n",
        "        results.append({\"C\": C, \"score\": avg_score})\n",
        "\n",
        "    # Select Best\n",
        "    best_result = min(results, key=lambda x: x[\"score\"])\n",
        "\n",
        "    return {\n",
        "        \"best_C\": best_result[\"C\"],\n",
        "        \"best_score\": best_result[\"score\"],\n",
        "        \"grid_results\": results\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def tune_hyperparameters(\n",
        "    modeling_panel: pd.DataFrame,\n",
        "    standardizer_func: callable,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates hyperparameter tuning for MSPI (Lasso) and Benchmark (Ridge).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    modeling_panel : pd.DataFrame\n",
        "        Panel with features and targets.\n",
        "    standardizer_func : callable\n",
        "        Task 13 standardizer.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Frozen hyperparameters.\n",
        "    \"\"\"\n",
        "    # Extract Parameters\n",
        "    proto = study_config[\"learning_protocol\"]\n",
        "    tune_config = proto[\"hyperparameter_tuning\"]\n",
        "\n",
        "    # 1. Generate Splits\n",
        "    splits = _generate_cv_splits(\n",
        "        modeling_panel.index,\n",
        "        proto[\"initial_training_window_months\"],\n",
        "        tune_config.get(\"cv_n_splits\", 5),\n",
        "        tune_config.get(\"cv_val_size_months\", 12),\n",
        "        tune_config.get(\"cv_step_size_months\", 12)\n",
        "    )\n",
        "\n",
        "    # 2. Tune MSPI (Lasso)\n",
        "    # Define Grid (C = 1/lambda)\n",
        "    # Paper uses lambda, sklearn uses C. We tune C.\n",
        "    # Grid: 10 values log-spaced\n",
        "    c_grid = np.logspace(-4, 4, 10)\n",
        "\n",
        "    mspi_feats = study_config[\"feature_engineering\"][\"final_feature_names_monthly\"]\n",
        "    mspi_res = _tune_model_hyperparameters(\n",
        "        modeling_panel, splits, mspi_feats, \"Y_next\", \"l1\", c_grid, standardizer_func, study_config\n",
        "    )\n",
        "\n",
        "    # 3. Tune Benchmark (Ridge)\n",
        "    bench_feats = [\"R_mkt\", \"sigma_mkt\"]\n",
        "    bench_res = _tune_model_hyperparameters(\n",
        "        modeling_panel, splits, bench_feats, \"Y_next\", \"l2\", c_grid, standardizer_func, study_config\n",
        "    )\n",
        "\n",
        "    frozen_params = {\n",
        "        \"mspi_lasso_C\": mspi_res[\"best_C\"],\n",
        "        \"benchmark_ridge_C\": bench_res[\"best_C\"],\n",
        "        \"tuning_audit\": {\n",
        "            \"mspi\": mspi_res,\n",
        "            \"benchmark\": bench_res\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return frozen_params\n"
      ],
      "metadata": {
        "id": "EYak_gDBmgVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 — Fit and forecast MSPI under the expanding-window lasso-logit protocol\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Fit and forecast MSPI under the expanding-window lasso-logit protocol\n",
        "# ==============================================================================\n",
        "\n",
        "def _fit_predict_lasso_logit(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    X_test: pd.DataFrame,\n",
        "    C: float,\n",
        "    random_seed: int\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Fits an L1-regularized logistic regression on training data and predicts\n",
        "    probability for the test instance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : pd.DataFrame\n",
        "        Standardized training features.\n",
        "    y_train : pd.Series\n",
        "        Training targets.\n",
        "    X_test : pd.DataFrame\n",
        "        Standardized test features (single row).\n",
        "    C : float\n",
        "        Inverse regularization strength.\n",
        "    random_seed : int\n",
        "        Seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[float, float]\n",
        "        - Predicted probability (MSPI_t).\n",
        "        - Raw score (linear predictor).\n",
        "    \"\"\"\n",
        "    # Configure Lasso-Logit\n",
        "    # Solver 'liblinear' is good for small datasets and supports L1\n",
        "    model = LogisticRegression(\n",
        "        penalty='l1',\n",
        "        C=C,\n",
        "        solver='liblinear',\n",
        "        random_state=random_seed,\n",
        "        fit_intercept=True\n",
        "    )\n",
        "\n",
        "    # Fit\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict Probability\n",
        "    # predict_proba returns [prob_0, prob_1]\n",
        "    prob = model.predict_proba(X_test)[0, 1]\n",
        "\n",
        "    # Compute Raw Score (decision function)\n",
        "    # decision_function returns raw score (z)\n",
        "    score = model.decision_function(X_test)[0]\n",
        "\n",
        "    return prob, score\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Execute the expanding-window forecasting loop.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _expanding_window_forecast_loop(\n",
        "    panel: pd.DataFrame,\n",
        "    features: List[str],\n",
        "    target: str,\n",
        "    frozen_C: float,\n",
        "    standardizer_func: callable,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes the real-time expanding window forecasting loop.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    panel : pd.DataFrame\n",
        "        Full modeling panel indexed by year_month.\n",
        "    features : List[str]\n",
        "        List of feature names.\n",
        "    target : str\n",
        "        Target column name (Y_next).\n",
        "    frozen_C : float\n",
        "        Frozen hyperparameter C.\n",
        "    standardizer_func : callable\n",
        "        Task 13 standardizer.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Forecasts indexed by forecast month t.\n",
        "    \"\"\"\n",
        "    # Define OOS Period\n",
        "    # We start forecasting after the initial training window\n",
        "    initial_window = study_config[\"learning_protocol\"][\"initial_training_window_months\"]\n",
        "    oos_start_idx = initial_window\n",
        "\n",
        "    # Get all unique months\n",
        "    all_months = panel.index.unique().sort_values()\n",
        "\n",
        "    # We need at least initial_window months of history to start\n",
        "    if len(all_months) <= initial_window:\n",
        "        raise ValueError(\"Panel length insufficient for initial training window.\")\n",
        "\n",
        "    forecast_months = all_months[oos_start_idx:]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for t in forecast_months:\n",
        "        # Define Training Set\n",
        "        # At month t, we observe X_t. We want to predict Y_{t+1}.\n",
        "        # The latest available label is S_t (which is Y_t).\n",
        "        # So we can train on all pairs (X_tau, Y_{tau+1}) where tau+1 <= t.\n",
        "        # This means the label month must be <= t.\n",
        "        # In our panel, the target column 'Y_next' at row tau holds Y_{tau+1}.\n",
        "        # So we filter rows where the index (tau) satisfies tau < t?\n",
        "        # No, let's be precise.\n",
        "        # Row tau has X_tau and Y_{tau+1}.\n",
        "        # We can use row tau if Y_{tau+1} is known at time t.\n",
        "        # Y_{tau+1} corresponds to S_{tau+1}.\n",
        "        # S_{tau+1} is known at end of month tau+1.\n",
        "        # So we need tau+1 <= t.\n",
        "        # Thus tau <= t-1.\n",
        "\n",
        "        # Training rows: index < t\n",
        "        train_mask = panel.index < t\n",
        "        # We also need to ensure we have valid targets\n",
        "        train_data = panel.loc[train_mask].dropna(subset=[target])\n",
        "\n",
        "        # Current row: index == t\n",
        "        current_data = panel.loc[[t]]\n",
        "\n",
        "        if len(train_data) < initial_window:\n",
        "            # Should not happen given loop start, but safety check\n",
        "            continue\n",
        "\n",
        "        # Standardize\n",
        "        # Fit on training features only\n",
        "        # We use the standardizer_func which expects full panel and train_end_idx\n",
        "        # But here we have sliced data. Let's adapt.\n",
        "        # We can pass the train_data directly if we modify standardizer or use internal logic.\n",
        "        # Let's use the internal logic we implemented in Task 14 helper for clarity.\n",
        "        # Or better, call standardizer_func with the full panel and the label of the last training month.\n",
        "        last_train_month = train_data.index[-1]\n",
        "\n",
        "        # Note: standardizer_func (Task 13) computes moments on X[...:train_end_idx]\n",
        "        # Here train_end_idx is last_train_month.\n",
        "        # This matches our training set definition.\n",
        "        std_result = standardizer_func(panel[features], last_train_month, study_config)\n",
        "        scaler = std_result[\"scaler_params\"]\n",
        "\n",
        "        # Apply Standardization\n",
        "        # Reimplement apply_z logic locally or import\n",
        "        epsilon = study_config[\"feature_engineering\"].get(\"std_epsilon\", 1e-8)\n",
        "        def apply_z(x, mu, sigma):\n",
        "            z = (x - mu) / sigma.replace(0, 1)\n",
        "            z.loc[:, sigma <= epsilon] = 0.0\n",
        "            return z\n",
        "\n",
        "        # Z-score Feature Sets and Extract Target\n",
        "        X_train = apply_z(train_data[features], scaler[\"mu\"], scaler[\"sigma\"])\n",
        "        X_test = apply_z(current_data[features], scaler[\"mu\"], scaler[\"sigma\"])\n",
        "        y_train = train_data[target]\n",
        "\n",
        "        # Fit and Predict\n",
        "        prob, score = _fit_predict_lasso_logit(\n",
        "            X_train, y_train, X_test, frozen_C,\n",
        "            study_config[\"reproducibility\"][\"random_seed\"]\n",
        "        )\n",
        "\n",
        "        # Store Result\n",
        "        # We store the realized target Y_{t+1} (which is in current_data['Y_next']) for evaluation\n",
        "        realized_target = current_data[target].iloc[0]\n",
        "\n",
        "        results.append({\n",
        "            \"forecast_month\": t,\n",
        "            \"MSPI\": prob,\n",
        "            \"raw_score\": score,\n",
        "            \"Y_next\": realized_target\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).set_index(\"forecast_month\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def forecast_mspi(\n",
        "    modeling_panel: pd.DataFrame,\n",
        "    frozen_hyperparams: Dict[str, Any],\n",
        "    standardizer_func: callable,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of MSPI forecasts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    modeling_panel : pd.DataFrame\n",
        "        Panel with features and targets.\n",
        "    frozen_hyperparams : Dict[str, Any]\n",
        "        Dictionary containing 'mspi_lasso_C'.\n",
        "    standardizer_func : callable\n",
        "        Task 13 standardizer.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Forecasts indexed by forecast month.\n",
        "    \"\"\"\n",
        "    # Extract parameters\n",
        "    features = study_config[\"feature_engineering\"][\"final_feature_names_monthly\"]\n",
        "    target = \"Y_next\"\n",
        "    C = frozen_hyperparams[\"mspi_lasso_C\"]\n",
        "\n",
        "    # Make forecasts\n",
        "    forecasts = _expanding_window_forecast_loop(\n",
        "        modeling_panel, features, target, C, standardizer_func, study_config\n",
        "    )\n",
        "\n",
        "    return forecasts\n"
      ],
      "metadata": {
        "id": "QN81vaUgQOWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 — Fit and forecast the benchmark ridge-logit under the same protocol\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Fit and forecast the benchmark ridge-logit under the same protocol\n",
        "# ==============================================================================\n",
        "\n",
        "def _fit_predict_ridge_logit(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    X_test: pd.DataFrame,\n",
        "    C: float,\n",
        "    random_seed: int\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Fits an L2-regularized logistic regression (Ridge) and predicts probability.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : pd.DataFrame\n",
        "        Standardized training features.\n",
        "    y_train : pd.Series\n",
        "        Training targets.\n",
        "    X_test : pd.DataFrame\n",
        "        Standardized test features.\n",
        "    C : float\n",
        "        Inverse regularization strength.\n",
        "    random_seed : int\n",
        "        Seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[float, float]\n",
        "        - Predicted probability.\n",
        "        - Raw score.\n",
        "    \"\"\"\n",
        "    # Initialize Model\n",
        "    model = LogisticRegression(\n",
        "        penalty='l2',\n",
        "        C=C,\n",
        "        solver='liblinear', # Supports l2\n",
        "        random_state=random_seed,\n",
        "        fit_intercept=True\n",
        "    )\n",
        "\n",
        "    # Fit Model\n",
        "    model.fit(X_train, y_train)\n",
        "    prob = model.predict_proba(X_test)[0, 1]\n",
        "    score = model.decision_function(X_test)[0]\n",
        "\n",
        "    return prob, score\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 2: Apply expanding-window standardization and ridge-logit estimation.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _expanding_window_benchmark_loop(\n",
        "    panel: pd.DataFrame,\n",
        "    target: str,\n",
        "    frozen_C: float,\n",
        "    standardizer_func: callable,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes the real-time expanding window forecasting loop for the benchmark.\n",
        "    Features are fixed to ['R_mkt', 'sigma_mkt'].\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    panel : pd.DataFrame\n",
        "        Full modeling panel.\n",
        "    target : str\n",
        "        Target column name.\n",
        "    frozen_C : float\n",
        "        Frozen hyperparameter C.\n",
        "    standardizer_func : callable\n",
        "        Task 13 standardizer.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Forecasts indexed by forecast month t.\n",
        "    \"\"\"\n",
        "    # Extract Parameters\n",
        "    features = [\"R_mkt\", \"sigma_mkt\"]\n",
        "    initial_window = study_config[\"learning_protocol\"][\"initial_training_window_months\"]\n",
        "    oos_start_idx = initial_window\n",
        "\n",
        "    # Sort and Slice Data Structures\n",
        "    all_months = panel.index.unique().sort_values()\n",
        "    forecast_months = all_months[oos_start_idx:]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for t in forecast_months:\n",
        "        # Training rows: index < t (label available at t)\n",
        "        train_mask = panel.index < t\n",
        "        train_data = panel.loc[train_mask].dropna(subset=[target])\n",
        "        current_data = panel.loc[[t]]\n",
        "\n",
        "        if len(train_data) < initial_window:\n",
        "            continue\n",
        "\n",
        "        # Standardize\n",
        "        last_train_month = train_data.index[-1]\n",
        "        std_result = standardizer_func(panel[features], last_train_month, study_config)\n",
        "        scaler = std_result[\"scaler_params\"]\n",
        "\n",
        "        epsilon = study_config[\"feature_engineering\"].get(\"std_epsilon\", 1e-8)\n",
        "        def apply_z(x, mu, sigma):\n",
        "            z = (x - mu) / sigma.replace(0, 1)\n",
        "            z.loc[:, sigma <= epsilon] = 0.0\n",
        "            return z\n",
        "\n",
        "        X_train = apply_z(train_data[features], scaler[\"mu\"], scaler[\"sigma\"])\n",
        "        X_test = apply_z(current_data[features], scaler[\"mu\"], scaler[\"sigma\"])\n",
        "        y_train = train_data[target]\n",
        "\n",
        "        # Fit and Predict\n",
        "        prob, score = _fit_predict_ridge_logit(\n",
        "            X_train, y_train, X_test, frozen_C,\n",
        "            study_config[\"reproducibility\"][\"random_seed\"]\n",
        "        )\n",
        "\n",
        "        realized_target = current_data[target].iloc[0]\n",
        "\n",
        "        results.append({\n",
        "            \"forecast_month\": t,\n",
        "            \"p_benchmark\": prob,\n",
        "            \"raw_score_benchmark\": score,\n",
        "            \"Y_next\": realized_target\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).set_index(\"forecast_month\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def forecast_benchmark(\n",
        "    modeling_panel: pd.DataFrame,\n",
        "    frozen_hyperparams: Dict[str, Any],\n",
        "    standardizer_func: callable,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of Benchmark forecasts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    modeling_panel : pd.DataFrame\n",
        "        Panel with features and targets.\n",
        "    frozen_hyperparams : Dict[str, Any]\n",
        "        Dictionary containing 'benchmark_ridge_C'.\n",
        "    standardizer_func : callable\n",
        "        Task 13 standardizer.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Forecasts indexed by forecast month.\n",
        "    \"\"\"\n",
        "    # Extract Parameters\n",
        "    target = \"Y_next\"\n",
        "    C = frozen_hyperparams[\"benchmark_ridge_C\"]\n",
        "\n",
        "    # Make Forecasts\n",
        "    forecasts = _expanding_window_benchmark_loop(\n",
        "        modeling_panel, target, C, standardizer_func, study_config\n",
        "    )\n",
        "\n",
        "    return forecasts\n"
      ],
      "metadata": {
        "id": "hojEPFh9RyEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 — Create end-to-end orchestrator callable\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Create end-to-end orchestrator callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 1/3: Define orchestrator-level anti–look-ahead unit tests.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _run_anti_look_ahead_checks(\n",
        "    mspi_forecasts: pd.DataFrame,\n",
        "    labels: pd.DataFrame,\n",
        "    audit_log: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs rigorous post-execution safety checks to ensure no look-ahead bias occurred\n",
        "    during the forecasting process. Verifies alignment between forecasts and realized labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mspi_forecasts : pd.DataFrame\n",
        "        DataFrame of MSPI forecasts indexed by forecast month t.\n",
        "        Must contain 'Y_next' (the realized target for that forecast).\n",
        "    labels : pd.DataFrame\n",
        "        DataFrame of ground-truth labels indexed by month t.\n",
        "        Must contain 'Y_next' (S_{t+1}).\n",
        "    audit_log : Dict[str, Any]\n",
        "        Accumulated audit logs from the pipeline.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If target alignment checks fail, indicating potential data leakage or shifting errors.\n",
        "    \"\"\"\n",
        "    # Check 1: Target Alignment\n",
        "    # The 'Y_next' column in the forecast dataframe represents the realized target\n",
        "    # that the model was trying to predict at time t. This must match the\n",
        "    # 'Y_next' column in the master label dataframe for the same month t.\n",
        "\n",
        "    # Find common months (intersection of indices)\n",
        "    common_months = mspi_forecasts.index.intersection(labels.index)\n",
        "\n",
        "    if len(common_months) == 0:\n",
        "        # This might happen if OOS period is disjoint from label period (should not happen)\n",
        "        raise ValueError(\"CRITICAL: No overlapping months between forecasts and labels for validation.\")\n",
        "\n",
        "    forecast_targets = mspi_forecasts.loc[common_months, \"Y_next\"]\n",
        "    label_targets = labels.loc[common_months, \"Y_next\"]\n",
        "\n",
        "    # Check for equality (handling potential floating point types for binary labels)\n",
        "    # We assume labels are 0/1.\n",
        "    if not forecast_targets.equals(label_targets):\n",
        "        # Detailed check for mismatches\n",
        "        mismatches = (forecast_targets != label_targets)\n",
        "        if mismatches.any():\n",
        "            n_mismatch = mismatches.sum()\n",
        "            example_mismatch = common_months[mismatches][0]\n",
        "            raise ValueError(\n",
        "                f\"CRITICAL: Target misalignment detected. {n_mismatch} months have mismatched targets. \"\n",
        "                f\"Example at {example_mismatch}: Forecast Y_next={forecast_targets[example_mismatch]}, \"\n",
        "                f\"Label Y_next={label_targets[example_mismatch]}.\"\n",
        "            )\n",
        "\n",
        "    # Check 2: Forecast Availability\n",
        "    # Ensure we didn't produce forecasts for months where inputs were missing\n",
        "    # (This is implicitly handled by the loop, but good to verify coverage)\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def run_mspi_pipeline(\n",
        "    df_crsp_daily: pd.DataFrame,\n",
        "    df_crsp_index: pd.DataFrame,\n",
        "    raw_study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete Market Stress Probability Index (MSPI) research pipeline.\n",
        "\n",
        "    This orchestrator enforces the strict sequential dependency graph required to\n",
        "    reproduce the results of 'Algorithmic Monitoring' (Schmitt, 2026). It manages\n",
        "    data flow from raw CRSP inputs through feature engineering, labeling,\n",
        "    hyperparameter tuning, and out-of-sample forecasting.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_crsp_daily : pd.DataFrame\n",
        "        Raw CRSP Daily Stock File (micro-data).\n",
        "    df_crsp_index : pd.DataFrame\n",
        "        Raw CRSP Daily Index File (market aggregate data).\n",
        "    raw_study_config : Dict[str, Any]\n",
        "        Configuration dictionary specifying all study parameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing the following keys:\n",
        "        - 'monthly_features': pd.DataFrame of X_t features.\n",
        "        - 'market_aggregates': pd.DataFrame of R_mkt and sigma_mkt.\n",
        "        - 'labels': pd.DataFrame of S_t and Y_next.\n",
        "        - 'mspi_forecasts': pd.DataFrame of MSPI out-of-sample forecasts.\n",
        "        - 'benchmark_forecasts': pd.DataFrame of Benchmark out-of-sample forecasts.\n",
        "        - 'frozen_hyperparams': Dict of tuned hyperparameters.\n",
        "        - 'audit_log': Dict of detailed execution logs and statistics.\n",
        "    \"\"\"\n",
        "    # Initialize Audit Log\n",
        "    audit_log = {}\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 1: Configuration and Validation\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 1: Configuration and Validation\")\n",
        "\n",
        "    # Task 1: Validate Config\n",
        "    config_res = validate_study_config(raw_study_config)\n",
        "    study_config = config_res[\"validated_config\"]\n",
        "    audit_log[\"config_validation\"] = config_res[\"validation_report\"]\n",
        "\n",
        "    # Task 2: Validate Micro Data\n",
        "    val_daily = validate_crsp_daily(df_crsp_daily, study_config)\n",
        "    audit_log[\"validation_daily\"] = val_daily\n",
        "\n",
        "    # Task 3: Validate Index Data\n",
        "    val_index = validate_crsp_index(df_crsp_daily, df_crsp_index, study_config)\n",
        "    audit_log[\"validation_index\"] = val_index\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 2: Data Cleansing and Universe Construction\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 2: Data Cleansing and Universe Construction\")\n",
        "\n",
        "    # Task 4: Cleanse Micro Data\n",
        "    clean_daily_res = cleanse_crsp_daily(df_crsp_daily, study_config)\n",
        "    df_daily_clean = clean_daily_res[\"df_clean\"]\n",
        "    audit_log[\"cleansing_daily\"] = clean_daily_res[\"audit_log\"]\n",
        "\n",
        "    # Task 5: Cleanse Index Data\n",
        "    clean_index_res = cleanse_crsp_index(df_daily_clean, df_crsp_index, study_config)\n",
        "    df_index_clean = clean_index_res[\"df_index_clean\"]\n",
        "    audit_log[\"cleansing_index\"] = clean_index_res[\"audit_log\"]\n",
        "\n",
        "    # Task 6: Construct Eligible Universe\n",
        "    universe_res = construct_eligible_universe(df_daily_clean, study_config)\n",
        "    df_eligible = universe_res[\"df_eligible\"]\n",
        "    nd_series = universe_res[\"nd_series\"]\n",
        "    audit_log[\"universe_construction\"] = universe_res[\"audit_log\"]\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 3: Feature Engineering (Daily Signals)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 3: Feature Engineering (Daily Signals)\")\n",
        "\n",
        "    # Task 7: Return Moments\n",
        "    moments = compute_daily_return_moments(df_eligible, study_config)\n",
        "\n",
        "    # Task 8: Tail Measures\n",
        "    tails = compute_daily_tail_measures(df_eligible, study_config)\n",
        "\n",
        "    # Task 9: Trading Proxies\n",
        "    proxies = compute_daily_trading_proxies(df_eligible, study_config)\n",
        "\n",
        "    # Merge Daily Statistics\n",
        "    # We align on DATE index. All should have same index from df_eligible.\n",
        "    daily_stats_all = pd.concat([moments, tails, proxies], axis=1)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 4: Monthly Aggregation and Labeling\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 4: Monthly Aggregation and Labeling\")\n",
        "\n",
        "    # Task 10: Aggregate Monthly Features\n",
        "    agg_res = aggregate_monthly_features(daily_stats_all, nd_series, study_config)\n",
        "    X_t = agg_res[\"X_t\"]\n",
        "    audit_log[\"monthly_aggregation\"] = agg_res[\"audit_log\"]\n",
        "\n",
        "    # Task 11: Construct Market Aggregates\n",
        "    market_res = construct_market_aggregates(df_index_clean, X_t, study_config)\n",
        "    aligned_panel = market_res[\"aligned_panel\"]\n",
        "    audit_log[\"market_aggregates\"] = market_res[\"audit_log\"]\n",
        "\n",
        "    # Task 12: Construct Stress Labels\n",
        "    label_res = construct_stress_labels(aligned_panel, study_config)\n",
        "    modeling_panel = label_res[\"modeling_panel\"]\n",
        "    audit_log[\"label_construction\"] = label_res[\"audit_log\"]\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 5: Model Training and Forecasting\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 5: Model Training and Forecasting\")\n",
        "\n",
        "    # Task 14: Tune Hyperparameters\n",
        "    # We pass the expanding_window_standardizer callable explicitly\n",
        "    tune_res = tune_hyperparameters(\n",
        "        modeling_panel,\n",
        "        expanding_window_standardizer,\n",
        "        study_config\n",
        "    )\n",
        "    frozen_params = tune_res\n",
        "    audit_log[\"hyperparameter_tuning\"] = tune_res[\"tuning_audit\"]\n",
        "\n",
        "    # Task 15: Forecast MSPI\n",
        "    mspi_forecasts = forecast_mspi(\n",
        "        modeling_panel,\n",
        "        frozen_params,\n",
        "        expanding_window_standardizer,\n",
        "        study_config\n",
        "    )\n",
        "\n",
        "    # Task 16: Forecast Benchmark\n",
        "    bench_forecasts = forecast_benchmark(\n",
        "        modeling_panel,\n",
        "        frozen_params,\n",
        "        expanding_window_standardizer,\n",
        "        study_config\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 6: Finalization\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 6: Finalization and Safety Checks\")\n",
        "\n",
        "    # Run Anti-Look-Ahead Checks\n",
        "    _run_anti_look_ahead_checks(mspi_forecasts, modeling_panel, audit_log)\n",
        "\n",
        "    # Compile Final Results\n",
        "    results = {\n",
        "        \"monthly_features\": X_t,\n",
        "        \"market_aggregates\": aligned_panel[[\"R_mkt\", \"sigma_mkt\"]],\n",
        "        \"labels\": modeling_panel[[\"S_t\", \"Y_next\"]],\n",
        "        \"mspi_forecasts\": mspi_forecasts,\n",
        "        \"benchmark_forecasts\": bench_forecasts,\n",
        "        \"frozen_hyperparams\": frozen_params,\n",
        "        \"audit_log\": audit_log\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "-4Wi7ZNYsfHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 — Fit RF and GB with real-time Platt calibration (robustness horse race)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Fit RF and GB with real-time Platt calibration (robustness horse race)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 1: Random forest: fit, score, and calibrate in real time.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _fit_predict_calibrate_model(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    X_test: pd.DataFrame,\n",
        "    model_type: str,\n",
        "    hyperparams: Dict[str, Any],\n",
        "    random_seed: int\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Fits a nonlinear model (RF or GB), computes raw scores, and applies real-time\n",
        "    Platt calibration using the training set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : pd.DataFrame\n",
        "        Standardized training features.\n",
        "    y_train : pd.Series\n",
        "        Training targets.\n",
        "    X_test : pd.DataFrame\n",
        "        Standardized test features.\n",
        "    model_type : str\n",
        "        'rf' or 'gb'.\n",
        "    hyperparams : Dict[str, Any]\n",
        "        Model hyperparameters.\n",
        "    random_seed : int\n",
        "        Seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[float, float]\n",
        "        - Calibrated probability.\n",
        "        - Raw score (uncalibrated).\n",
        "    \"\"\"\n",
        "    # 1. Initialize Model\n",
        "    # Initialize Random Forest Model\n",
        "    if model_type == 'rf':\n",
        "        model = RandomForestClassifier(\n",
        "            random_state=random_seed,\n",
        "            n_jobs=1, # Determinism\n",
        "            **hyperparams\n",
        "        )\n",
        "\n",
        "    # Initialize Gradient Boosting Classifier\n",
        "    elif model_type == 'gb':\n",
        "        model = GradientBoostingClassifier(\n",
        "            random_state=random_seed,\n",
        "            **hyperparams\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # 2. Fit Model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 3. Get Raw Scores\n",
        "    # For RF, raw score is mean predicted probability (fraction of trees)\n",
        "    # For GB, raw score is decision function (log-odds)\n",
        "    if model_type == 'rf':\n",
        "        # RF \"raw score\" is the uncalibrated probability\n",
        "        # We need scores for training set to fit calibration\n",
        "        train_scores = model.predict_proba(X_train)[:, 1]\n",
        "        test_score = model.predict_proba(X_test)[0, 1]\n",
        "\n",
        "        # Platt scaling expects log-odds or similar, but can work on probs.\n",
        "        # However, standard Platt is logistic regression on scores.\n",
        "        # If scores are probs [0,1], we might need to logit transform them first\n",
        "        # or just fit logistic regression on them directly (which learns a sigmoid on top of sigmoid-like).\n",
        "        # The task says \"random forest score is the ensemble average... apply calibration map\".\n",
        "        # We use the raw ensemble average as the feature for calibration.\n",
        "\n",
        "    elif model_type == 'gb':\n",
        "        # GB raw score is decision function\n",
        "        train_scores = model.decision_function(X_train)\n",
        "        test_score = model.decision_function(X_test)[0]\n",
        "\n",
        "    # 4. Fit Calibration (Platt Scaling)\n",
        "    # Logistic Regression of y_train on train_scores\n",
        "    # We reshape scores to (n_samples, 1)\n",
        "    calibrator = LogisticRegression(solver='liblinear', penalty='none')\n",
        "\n",
        "    # Note: penalty='none' might fail if separation exists, use l2 with small C if needed.\n",
        "    # Let's use default l2 with C=1.0 for stability as per standard Platt implementations.\n",
        "    calibrator = LogisticRegression(solver='liblinear', C=1.0)\n",
        "\n",
        "    calibrator.fit(train_scores.reshape(-1, 1), y_train)\n",
        "\n",
        "    # 5. Apply Calibration\n",
        "    calibrated_prob = calibrator.predict_proba(np.array([[test_score]]))[0, 1]\n",
        "\n",
        "    return calibrated_prob, test_score\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 3: Store all raw scores and calibrated probabilities for evaluation.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _expanding_window_nonlinear_loop(\n",
        "    panel: pd.DataFrame,\n",
        "    features: List[str],\n",
        "    target: str,\n",
        "    model_type: str,\n",
        "    hyperparams: Dict[str, Any],\n",
        "    standardizer_func: callable,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Executes the real-time expanding window loop for nonlinear models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    panel : pd.DataFrame\n",
        "        Full modeling panel.\n",
        "    features : List[str]\n",
        "        Feature names.\n",
        "    target : str\n",
        "        Target column name.\n",
        "    model_type : str\n",
        "        'rf' or 'gb'.\n",
        "    hyperparams : Dict[str, Any]\n",
        "        Model hyperparameters.\n",
        "    standardizer_func : callable\n",
        "        Task 13 standardizer.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Forecasts indexed by forecast month t.\n",
        "    \"\"\"\n",
        "    initial_window = study_config[\"learning_protocol\"][\"initial_training_window_months\"]\n",
        "    oos_start_idx = initial_window\n",
        "\n",
        "    all_months = panel.index.unique().sort_values()\n",
        "    forecast_months = all_months[oos_start_idx:]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for t in forecast_months:\n",
        "        # Training rows: index < t\n",
        "        train_mask = panel.index < t\n",
        "        train_data = panel.loc[train_mask].dropna(subset=[target])\n",
        "        current_data = panel.loc[[t]]\n",
        "\n",
        "        if len(train_data) < initial_window:\n",
        "            continue\n",
        "\n",
        "        # Standardize (Optional for trees, but mandated by task text \"using standardized X\")\n",
        "        last_train_month = train_data.index[-1]\n",
        "        std_result = standardizer_func(panel[features], last_train_month, study_config)\n",
        "        scaler = std_result[\"scaler_params\"]\n",
        "\n",
        "        epsilon = study_config[\"feature_engineering\"].get(\"std_epsilon\", 1e-8)\n",
        "        def apply_z(x, mu, sigma):\n",
        "            z = (x - mu) / sigma.replace(0, 1)\n",
        "            z.loc[:, sigma <= epsilon] = 0.0\n",
        "            return z\n",
        "\n",
        "        X_train = apply_z(train_data[features], scaler[\"mu\"], scaler[\"sigma\"])\n",
        "        X_test = apply_z(current_data[features], scaler[\"mu\"], scaler[\"sigma\"])\n",
        "        y_train = train_data[target]\n",
        "\n",
        "        # Fit, Predict, Calibrate\n",
        "        prob, score = _fit_predict_calibrate_model(\n",
        "            X_train, y_train, X_test, model_type, hyperparams,\n",
        "            study_config[\"reproducibility\"][\"random_seed\"]\n",
        "        )\n",
        "\n",
        "        realized_target = current_data[target].iloc[0]\n",
        "\n",
        "        results.append({\n",
        "            \"forecast_month\": t,\n",
        "            f\"p_{model_type}\": prob,\n",
        "            f\"raw_score_{model_type}\": score,\n",
        "            \"Y_next\": realized_target\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).set_index(\"forecast_month\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def run_robustness_horse_race(\n",
        "    modeling_panel: pd.DataFrame,\n",
        "    mspi_forecasts: pd.DataFrame,\n",
        "    benchmark_forecasts: pd.DataFrame,\n",
        "    frozen_hyperparams: Dict[str, Any],\n",
        "    standardizer_func: callable,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the nonlinear model horse race (RF and GB) and unifies results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    modeling_panel : pd.DataFrame\n",
        "        Panel with features and targets.\n",
        "    mspi_forecasts : pd.DataFrame\n",
        "        MSPI forecasts.\n",
        "    benchmark_forecasts : pd.DataFrame\n",
        "        Benchmark forecasts.\n",
        "    frozen_hyperparams : Dict[str, Any]\n",
        "        Dictionary containing RF/GB params (if tuned, else defaults).\n",
        "    standardizer_func : callable\n",
        "        Task 13 standardizer.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Unified evaluation table with all models.\n",
        "    \"\"\"\n",
        "    features = study_config[\"feature_engineering\"][\"final_feature_names_monthly\"]\n",
        "    target = \"Y_next\"\n",
        "\n",
        "    # Get Hyperparams (or defaults if not tuned in Task 14)\n",
        "    # In a full implementation, these should be tuned. Here we use defaults or config.\n",
        "    rf_params = frozen_hyperparams.get(\"rf_params\", {\"n_estimators\": 100, \"max_depth\": 5})\n",
        "    gb_params = frozen_hyperparams.get(\"gb_params\", {\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 3})\n",
        "\n",
        "    # Forecast RF\n",
        "    rf_forecasts = _expanding_window_nonlinear_loop(\n",
        "        modeling_panel, features, target, \"rf\", rf_params, standardizer_func, study_config\n",
        "    )\n",
        "\n",
        "    # Forecast GB\n",
        "    gb_forecasts = _expanding_window_nonlinear_loop(\n",
        "        modeling_panel, features, target, \"gb\", gb_params, standardizer_func, study_config\n",
        "    )\n",
        "\n",
        "    # Unify Results\n",
        "    # Join on forecast_month\n",
        "    # We start with MSPI as base\n",
        "    unified = mspi_forecasts.rename(columns={\"MSPI\": \"p_mspi\", \"raw_score\": \"raw_score_mspi\"})\n",
        "\n",
        "    # Join Benchmark\n",
        "    unified = unified.join(\n",
        "        benchmark_forecasts[[\"p_benchmark\", \"raw_score_benchmark\"]],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    # Join RF\n",
        "    unified = unified.join(\n",
        "        rf_forecasts[[\"p_rf\", \"raw_score_rf\"]],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    # Join GB\n",
        "    unified = unified.join(\n",
        "        gb_forecasts[[\"p_gb\", \"raw_score_gb\"]],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    return unified\n"
      ],
      "metadata": {
        "id": "xpIOKJBq_cbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 — Compute out-of-sample discrimination metrics (raw scores)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Compute out-of-sample discrimination metrics (raw scores)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 1 and 2: Compute ROC-AUC for each model / Compute PR-AUC for each model.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_auc_metrics(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    score_cols: Dict[str, str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes ROC-AUC and PR-AUC for multiple models on a common evaluation set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Evaluation DataFrame containing targets and raw scores.\n",
        "    target_col : str\n",
        "        Name of the target column (e.g., 'Y_next').\n",
        "    score_cols : Dict[str, str]\n",
        "        Dictionary mapping Model Name -> Raw Score Column Name.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame indexed by Model Name with columns 'AUC', 'PR-AUC'.\n",
        "    \"\"\"\n",
        "    # Ensure no missing values in target or scores for fair comparison\n",
        "    # We use the intersection of valid data\n",
        "    cols_to_check = [target_col] + list(score_cols.values())\n",
        "    df_clean = df.dropna(subset=cols_to_check)\n",
        "\n",
        "    y_true = df_clean[target_col]\n",
        "\n",
        "    # Check for degenerate labels\n",
        "    if y_true.nunique() < 2:\n",
        "        # Cannot compute AUC if only one class is present\n",
        "        # Return NaNs or raise warning\n",
        "        # We return NaNs to allow pipeline to proceed (e.g. if OOS is very short)\n",
        "        results = []\n",
        "        for model_name in score_cols.keys():\n",
        "            results.append({\n",
        "                \"Model\": model_name,\n",
        "                \"AUC\": np.nan,\n",
        "                \"PR-AUC\": np.nan\n",
        "            })\n",
        "        return pd.DataFrame(results).set_index(\"Model\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Iterate through models and scores\n",
        "    for model_name, score_col in score_cols.items():\n",
        "        y_score = df_clean[score_col]\n",
        "\n",
        "        # Compute ROC AUC and Precision Scores\n",
        "        auc = roc_auc_score(y_true, y_score)\n",
        "        pr_auc = average_precision_score(y_true, y_score)\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"AUC\": auc,\n",
        "            \"PR-AUC\": pr_auc\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).set_index(\"Model\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 3: Store discrimination results in a comparison table.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _add_metadata_to_metrics(\n",
        "    metrics_df: pd.DataFrame,\n",
        "    df_eval: pd.DataFrame,\n",
        "    target_col: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds sample size and event rate metadata to the metrics table.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    metrics_df : pd.DataFrame\n",
        "        DataFrame with AUC/PR-AUC.\n",
        "    df_eval : pd.DataFrame\n",
        "        Evaluation DataFrame.\n",
        "    target_col : str\n",
        "        Target column name.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Metrics DataFrame with added metadata columns.\n",
        "    \"\"\"\n",
        "    # Compute metadata on the same cleaned set used for metrics\n",
        "    # (Implicitly assumed df_eval is the one passed to _compute_auc_metrics or we re-clean)\n",
        "    # For robustness, we re-clean based on target (scores assumed valid if passed)\n",
        "    df_clean = df_eval.dropna(subset=[target_col])\n",
        "\n",
        "    # Compute Samples and Event Rate\n",
        "    n_samples = len(df_clean)\n",
        "    event_rate = df_clean[target_col].mean()\n",
        "\n",
        "    # Store Variables\n",
        "    metrics_df[\"N\"] = n_samples\n",
        "    metrics_df[\"Event_Rate\"] = event_rate\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_discrimination_metrics(\n",
        "    unified_evaluation_panel: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of discrimination metrics (AUC, PR-AUC).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unified_evaluation_panel : pd.DataFrame\n",
        "        DataFrame containing targets and raw scores for all models.\n",
        "        Expected columns: 'Y_next', 'raw_score_mspi', 'raw_score_benchmark',\n",
        "        'raw_score_rf', 'raw_score_gb'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Comparison table of discrimination metrics.\n",
        "    \"\"\"\n",
        "    # Initialize name of target\n",
        "    target_col = \"Y_next\"\n",
        "\n",
        "    # Define mapping of Model Name -> Raw Score Column\n",
        "    # We check which columns exist to support partial runs (e.g. if RF/GB skipped)\n",
        "    score_map = {\n",
        "        \"MSPI\": \"raw_score_mspi\",\n",
        "        \"Benchmark\": \"raw_score_benchmark\"\n",
        "    }\n",
        "\n",
        "    if \"raw_score_rf\" in unified_evaluation_panel.columns:\n",
        "        score_map[\"Random Forest\"] = \"raw_score_rf\"\n",
        "    if \"raw_score_gb\" in unified_evaluation_panel.columns:\n",
        "        score_map[\"Gradient Boosting\"] = \"raw_score_gb\"\n",
        "\n",
        "    # Compute Metrics\n",
        "    metrics_df = _compute_auc_metrics(unified_evaluation_panel, target_col, score_map)\n",
        "\n",
        "    # Add Metadata\n",
        "    final_df = _add_metadata_to_metrics(metrics_df, unified_evaluation_panel, target_col)\n",
        "\n",
        "    return final_df\n"
      ],
      "metadata": {
        "id": "nW2_zsXcOfGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 — Compute probability accuracy metrics and calibration diagnostics\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Compute probability accuracy metrics and calibration diagnostics\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 1: Compute Brier score and log loss for each model on probability forecasts.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_prob_scores(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    prob_cols: Dict[str, str],\n",
        "    epsilon: float = 1e-6\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes Brier Score, Log Loss, and Mean Predicted Probability.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Evaluation DataFrame.\n",
        "    target_col : str\n",
        "        Target column name.\n",
        "    prob_cols : Dict[str, str]\n",
        "        Mapping Model Name -> Probability Column.\n",
        "    epsilon : float\n",
        "        Clipping epsilon for log loss.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Metrics DataFrame indexed by Model Name.\n",
        "    \"\"\"\n",
        "    cols_to_check = [target_col] + list(prob_cols.values())\n",
        "    df_clean = df.dropna(subset=cols_to_check)\n",
        "    y_true = df_clean[target_col]\n",
        "\n",
        "    results = []\n",
        "    for model_name, prob_col in prob_cols.items():\n",
        "        y_prob = df_clean[prob_col]\n",
        "\n",
        "        # Brier Score\n",
        "        brier = brier_score_loss(y_true, y_prob)\n",
        "\n",
        "        # Log Loss (with clipping)\n",
        "        y_prob_clipped = np.clip(y_prob, epsilon, 1 - epsilon)\n",
        "        ll = log_loss(y_true, y_prob_clipped)\n",
        "\n",
        "        # Mean Prob\n",
        "        mean_prob = y_prob.mean()\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Brier\": brier,\n",
        "            \"LogLoss\": ll,\n",
        "            \"Mean_Predicted_Prob\": mean_prob\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).set_index(\"Model\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 2: Compute calibration curve and Expected Calibration Error (ECE).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_ece(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    prob_cols: Dict[str, str],\n",
        "    n_bins: int = 10\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes Expected Calibration Error (ECE) using equal-mass quantile binning.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Evaluation DataFrame.\n",
        "    target_col : str\n",
        "        Target column name.\n",
        "    prob_cols : Dict[str, str]\n",
        "        Mapping Model Name -> Probability Column.\n",
        "    n_bins : int\n",
        "        Number of bins.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with ECE per model.\n",
        "    \"\"\"\n",
        "    cols_to_check = [target_col] + list(prob_cols.values())\n",
        "    df_clean = df.dropna(subset=cols_to_check)\n",
        "    y_true = df_clean[target_col]\n",
        "\n",
        "    results = []\n",
        "    for model_name, prob_col in prob_cols.items():\n",
        "        y_prob = df_clean[prob_col]\n",
        "\n",
        "        # Quantile Binning\n",
        "        # duplicates='drop' handles ties by reducing number of bins\n",
        "        try:\n",
        "            bins = pd.qcut(y_prob, q=n_bins, duplicates='drop')\n",
        "        except ValueError:\n",
        "            # Fallback if too few unique values\n",
        "            bins = pd.cut(y_prob, bins=n_bins)\n",
        "\n",
        "        # Compute bin stats\n",
        "        bin_df = pd.DataFrame({\"y_true\": y_true, \"y_prob\": y_prob, \"bin\": bins})\n",
        "        bin_stats = bin_df.groupby(\"bin\", observed=True).agg(\n",
        "            mean_pred=(\"y_prob\", \"mean\"),\n",
        "            mean_obs=(\"y_true\", \"mean\"),\n",
        "            count=(\"y_true\", \"count\")\n",
        "        )\n",
        "\n",
        "        # ECE = sum(weight * |mean_pred - mean_obs|)\n",
        "        total_count = len(df_clean)\n",
        "        bin_stats[\"weight\"] = bin_stats[\"count\"] / total_count\n",
        "        bin_stats[\"abs_err\"] = (bin_stats[\"mean_pred\"] - bin_stats[\"mean_obs\"]).abs()\n",
        "        ece = (bin_stats[\"weight\"] * bin_stats[\"abs_err\"]).sum()\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"ECE\": ece\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).set_index(\"Model\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_probability_metrics(\n",
        "    unified_evaluation_panel: pd.DataFrame,\n",
        "    discrimination_metrics: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of probability accuracy metrics and combines them\n",
        "    with discrimination metrics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unified_evaluation_panel : pd.DataFrame\n",
        "        DataFrame with targets and probabilities.\n",
        "    discrimination_metrics : pd.DataFrame\n",
        "        DataFrame from Task 19.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Complete evaluation table.\n",
        "    \"\"\"\n",
        "    target_col = \"Y_next\"\n",
        "    epsilon = study_config[\"evaluation_inference\"].get(\"proba_clip_epsilon\", 1e-6)\n",
        "    n_bins = study_config[\"evaluation_inference\"][\"calibration\"].get(\"ece_bins\", 10)\n",
        "\n",
        "    # Define mapping\n",
        "    prob_map = {\n",
        "        \"MSPI\": \"p_mspi\",\n",
        "        \"Benchmark\": \"p_benchmark\"\n",
        "    }\n",
        "    if \"p_rf\" in unified_evaluation_panel.columns:\n",
        "        prob_map[\"Random Forest\"] = \"p_rf\"\n",
        "    if \"p_gb\" in unified_evaluation_panel.columns:\n",
        "        prob_map[\"Gradient Boosting\"] = \"p_gb\"\n",
        "\n",
        "    # Compute Prob Scores\n",
        "    prob_scores = _compute_prob_scores(unified_evaluation_panel, target_col, prob_map, epsilon)\n",
        "\n",
        "    # Compute ECE\n",
        "    ece_scores = _compute_ece(unified_evaluation_panel, target_col, prob_map, n_bins)\n",
        "\n",
        "    # Merge All\n",
        "    # discrimination_metrics is base\n",
        "    final_table = discrimination_metrics.join(prob_scores, how=\"outer\")\n",
        "    final_table = final_table.join(ece_scores, how=\"outer\")\n",
        "\n",
        "    return final_table\n"
      ],
      "metadata": {
        "id": "4vK5xA2KQLyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 — Block-bootstrap inference for out-of-sample performance differences\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Block-bootstrap inference for out-of-sample performance differences\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Configure the bootstrap per the paper's specification.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _generate_block_bootstrap_indices(\n",
        "    n_samples: int,\n",
        "    block_length: int,\n",
        "    n_replications: int,\n",
        "    random_seed: int\n",
        ") -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates indices for moving block bootstrap.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples : int\n",
        "        Total number of time steps in OOS.\n",
        "    block_length : int\n",
        "        Length of each block (L).\n",
        "    n_replications : int\n",
        "        Number of bootstrap samples (B).\n",
        "    random_seed : int\n",
        "        Seed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[np.ndarray]\n",
        "        List of index arrays, each of length n_samples.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_seed)\n",
        "    indices = []\n",
        "\n",
        "    # Number of possible blocks\n",
        "    n_blocks = n_samples - block_length + 1\n",
        "\n",
        "    if n_blocks <= 0:\n",
        "        raise ValueError(f\"Block length {block_length} > sample size {n_samples}\")\n",
        "\n",
        "    # Iterate through replications\n",
        "    for _ in range(n_replications):\n",
        "        sample_indices = []\n",
        "        while len(sample_indices) < n_samples:\n",
        "            # Pick random start index\n",
        "            start_idx = rng.integers(0, n_blocks)\n",
        "            # Add block\n",
        "            block = np.arange(start_idx, start_idx + block_length)\n",
        "            sample_indices.extend(block)\n",
        "\n",
        "        # Truncate to exact length\n",
        "        indices.append(np.array(sample_indices[:n_samples]))\n",
        "\n",
        "    return indices\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 2: Compute performance differences within each bootstrap resample.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _compute_bootstrap_deltas(\n",
        "    df: pd.DataFrame,\n",
        "    indices_list: List[np.ndarray],\n",
        "    target_col: str,\n",
        "    score_cols: Dict[str, str],\n",
        "    prob_cols: Dict[str, str],\n",
        "    epsilon: float\n",
        ") -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    Computes the difference in metrics (MSPI - Benchmark) for each bootstrap sample.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Evaluation DataFrame.\n",
        "    indices_list : List[np.ndarray]\n",
        "        Bootstrap indices.\n",
        "    target_col : str\n",
        "        Target column.\n",
        "    score_cols : Dict[str, str]\n",
        "        Raw score columns for AUC/PR-AUC.\n",
        "    prob_cols : Dict[str, str]\n",
        "        Prob columns for Brier/LogLoss.\n",
        "    epsilon : float\n",
        "        Log loss clipping.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, List[float]]\n",
        "        Dictionary of delta lists per metric.\n",
        "    \"\"\"\n",
        "    deltas = {\n",
        "        \"AUC\": [], \"PR-AUC\": [], \"Brier\": [], \"LogLoss\": []\n",
        "    }\n",
        "\n",
        "    # Extract arrays for speed\n",
        "    y_true = df[target_col].values\n",
        "\n",
        "    # MSPI\n",
        "    s_mspi = df[score_cols[\"MSPI\"]].values\n",
        "    p_mspi = df[prob_cols[\"MSPI\"]].values\n",
        "\n",
        "    # Benchmark\n",
        "    s_bench = df[score_cols[\"Benchmark\"]].values\n",
        "    p_bench = df[prob_cols[\"Benchmark\"]].values\n",
        "\n",
        "    for idx in indices_list:\n",
        "        y_boot = y_true[idx]\n",
        "\n",
        "        # Check for degeneracy\n",
        "        if len(np.unique(y_boot)) < 2:\n",
        "            continue # Skip degenerate samples\n",
        "\n",
        "        # MSPI Metrics\n",
        "        auc_m = roc_auc_score(y_boot, s_mspi[idx])\n",
        "        pr_m = average_precision_score(y_boot, s_mspi[idx])\n",
        "        br_m = brier_score_loss(y_boot, p_mspi[idx])\n",
        "        ll_m = log_loss(y_boot, np.clip(p_mspi[idx], epsilon, 1-epsilon))\n",
        "\n",
        "        # Bench Metrics\n",
        "        auc_b = roc_auc_score(y_boot, s_bench[idx])\n",
        "        pr_b = average_precision_score(y_boot, s_bench[idx])\n",
        "        br_b = brier_score_loss(y_boot, p_bench[idx])\n",
        "        ll_b = log_loss(y_boot, np.clip(p_bench[idx], epsilon, 1-epsilon))\n",
        "\n",
        "        # Deltas (MSPI - Bench)\n",
        "        deltas[\"AUC\"].append(auc_m - auc_b)\n",
        "        deltas[\"PR-AUC\"].append(pr_m - pr_b)\n",
        "        deltas[\"Brier\"].append(br_m - br_b)\n",
        "        deltas[\"LogLoss\"].append(ll_m - ll_b)\n",
        "\n",
        "    return deltas\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 3: Report bootstrap summaries (mean delta and confidence intervals).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _summarize_bootstrap(\n",
        "    deltas: Dict[str, List[float]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes mean and 95% CI for bootstrap deltas.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    deltas : Dict[str, List[float]]\n",
        "        Bootstrap deltas.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Inference table.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Iterate through metrics\n",
        "    for metric, values in deltas.items():\n",
        "        arr = np.array(values)\n",
        "        n_valid = len(arr)\n",
        "\n",
        "        if n_valid == 0:\n",
        "            results.append({\n",
        "                \"Metric\": metric,\n",
        "                \"Delta_Mean\": np.nan,\n",
        "                \"CI_2.5\": np.nan,\n",
        "                \"CI_97.5\": np.nan,\n",
        "                \"Valid_Replications\": 0\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Compute mean, low, high\n",
        "        mean_delta = np.mean(arr)\n",
        "        ci_low = np.percentile(arr, 2.5)\n",
        "        ci_high = np.percentile(arr, 97.5)\n",
        "\n",
        "        # Interpretation\n",
        "        # AUC/PR-AUC: Positive is good\n",
        "        # Brier/LogLoss: Negative is good\n",
        "        if metric in [\"AUC\", \"PR-AUC\"]:\n",
        "            sig = (ci_low > 0)\n",
        "        else:\n",
        "            sig = (ci_high < 0)\n",
        "\n",
        "        results.append({\n",
        "            \"Metric\": metric,\n",
        "            \"Delta_Mean\": mean_delta,\n",
        "            \"CI_2.5\": ci_low,\n",
        "            \"CI_97.5\": ci_high,\n",
        "            \"Significant_95\": sig,\n",
        "            \"Valid_Replications\": n_valid\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results).set_index(\"Metric\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def perform_bootstrap_inference(\n",
        "    unified_evaluation_panel: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates block-bootstrap inference for MSPI vs Benchmark.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unified_evaluation_panel : pd.DataFrame\n",
        "        Evaluation data.\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Inference table.\n",
        "    \"\"\"\n",
        "    # Get Bootstrap Configuration\n",
        "    boot_config = study_config[\"evaluation_inference\"][\"block_bootstrap\"]\n",
        "    if not boot_config.get(\"enabled\", True):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Extract key parameters\n",
        "    L = boot_config.get(\"block_length_months\", 12)\n",
        "    B = boot_config.get(\"replications\", 2000)\n",
        "    seed = study_config[\"reproducibility\"][\"random_seed\"]\n",
        "    epsilon = study_config[\"evaluation_inference\"].get(\"proba_clip_epsilon\", 1e-6)\n",
        "\n",
        "    # Clean data (intersection)\n",
        "    target_col = \"Y_next\"\n",
        "    score_cols = {\"MSPI\": \"raw_score_mspi\", \"Benchmark\": \"raw_score_benchmark\"}\n",
        "    prob_cols = {\"MSPI\": \"p_mspi\", \"Benchmark\": \"p_benchmark\"}\n",
        "\n",
        "    cols = [target_col] + list(score_cols.values()) + list(prob_cols.values())\n",
        "    df_clean = unified_evaluation_panel.dropna(subset=cols)\n",
        "\n",
        "    # 1. Generate Indices\n",
        "    indices = _generate_block_bootstrap_indices(len(df_clean), L, B, seed)\n",
        "\n",
        "    # 2. Compute Deltas\n",
        "    deltas = _compute_bootstrap_deltas(\n",
        "        df_clean, indices, target_col, score_cols, prob_cols, epsilon\n",
        "    )\n",
        "\n",
        "    # 3. Summarize\n",
        "    inference_table = _summarize_bootstrap(deltas)\n",
        "\n",
        "    return inference_table\n"
      ],
      "metadata": {
        "id": "qVDPxjEiRsc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22 — Economic analysis: innovations and local projections\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Economic analysis: innovations and local projections\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 1: Estimate predictive regression for next-month realized volatility (paper's Eq. (4)).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _estimate_volatility_prediction(\n",
        "    mspi_series: pd.Series,\n",
        "    market_df: pd.DataFrame,\n",
        "    target_vol: pd.Series\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates the predictive regression of next-month realized volatility on MSPI.\n",
        "    Equation: sigma_{t+1} = alpha + gamma * MSPI_t + phi' * Z_t + eta_{t+1}\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mspi_series : pd.Series\n",
        "        MSPI forecasts indexed by t.\n",
        "    market_df : pd.DataFrame\n",
        "        Controls Z_t (R_mkt, sigma_mkt) indexed by t.\n",
        "    target_vol : pd.Series\n",
        "        Realized volatility sigma_mkt indexed by t (will be shifted to t+1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Regression statistics (gamma, t-stat, R2).\n",
        "    \"\"\"\n",
        "    # Align Data\n",
        "    # LHS: sigma_{t+1} (shift target_vol backwards by 1 to align t+1 with t)\n",
        "    # Note: target_vol index is time of realization.\n",
        "    # If we shift(-1), the value at index t becomes the value from t+1.\n",
        "    lhs = target_vol.shift(-1).rename(\"sigma_next\")\n",
        "\n",
        "    # RHS: MSPI_t, R_mkt_t, sigma_mkt_t\n",
        "    rhs = pd.concat([mspi_series, market_df], axis=1)\n",
        "    rhs = sm.add_constant(rhs)\n",
        "\n",
        "    # Combine and Drop NaNs\n",
        "    data = pd.concat([lhs, rhs], axis=1).dropna()\n",
        "\n",
        "    # Fit OLS with HAC (Newey-West, lag=1)\n",
        "    model = sm.OLS(data[\"sigma_next\"], data.drop(columns=[\"sigma_next\"]))\n",
        "    results = model.fit(cov_type='HAC', cov_kwds={'maxlags': 1})\n",
        "\n",
        "    # Extract MSPI coefficient (gamma)\n",
        "    mspi_col = mspi_series.name\n",
        "    gamma = results.params[mspi_col]\n",
        "    t_stat = results.tvalues[mspi_col]\n",
        "    p_val = results.pvalues[mspi_col]\n",
        "\n",
        "    return {\n",
        "        \"gamma\": gamma,\n",
        "        \"t_stat\": t_stat,\n",
        "        \"p_value\": p_val,\n",
        "        \"R2\": results.rsquared,\n",
        "        \"N\": int(results.nobs)\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 2: Construct stress-risk innovations.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _construct_innovations(\n",
        "    mspi_series: pd.Series,\n",
        "    market_df: pd.DataFrame\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Constructs stress-risk innovations u_t by orthogonalizing MSPI wrt lagged information.\n",
        "    Equation: MSPI_t = delta_0 + delta_1 * MSPI_{t-1} + Delta' * Z_{t-1} + u_t\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mspi_series : pd.Series\n",
        "        MSPI forecasts indexed by t.\n",
        "    market_df : pd.DataFrame\n",
        "        Market variables indexed by t.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Innovation series u_t indexed by t.\n",
        "    \"\"\"\n",
        "    # Lagged Regressors\n",
        "    # Shift forward by 1 to get t-1 values at index t\n",
        "    lagged_mspi = mspi_series.shift(1).rename(\"MSPI_lag\")\n",
        "    lagged_market = market_df.shift(1).add_suffix(\"_lag\")\n",
        "\n",
        "    rhs = pd.concat([lagged_mspi, lagged_market], axis=1)\n",
        "    rhs = sm.add_constant(rhs)\n",
        "\n",
        "    # LHS: MSPI_t\n",
        "    lhs = mspi_series\n",
        "\n",
        "    # Align\n",
        "    data = pd.concat([lhs, rhs], axis=1).dropna()\n",
        "\n",
        "    # Fit OLS\n",
        "    model = sm.OLS(data[mspi_series.name], data.drop(columns=[mspi_series.name]))\n",
        "    results = model.fit()\n",
        "\n",
        "    # Residuals are innovations\n",
        "    u_t = results.resid.rename(\"stress_innovation\")\n",
        "\n",
        "    return u_t\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 3: Estimate local projections.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def _estimate_local_projections(\n",
        "    innovations: pd.Series,\n",
        "    outcome_series: pd.Series,\n",
        "    controls: pd.DataFrame,\n",
        "    horizons: int = 12\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Estimates local projections of outcome on stress innovations.\n",
        "    Equation: y_{t+h} = a_h + b_h * u_t + Gamma_h' * W_{t-1} + eps_{t+h}\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    innovations : pd.Series\n",
        "        Stress innovations u_t.\n",
        "    outcome_series : pd.Series\n",
        "        Outcome variable y_t (e.g., sigma_mkt).\n",
        "    controls : pd.DataFrame\n",
        "        Control variables W_{t-1} (already lagged? No, usually contemporaneous to innovation generation time t-1 info).\n",
        "        The task says W_{t-1}. We assume controls passed are Z_{t-1} aligned at t.\n",
        "    horizons : int\n",
        "        Max horizon H.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        IRF table with coefficients and CI.\n",
        "    \"\"\"\n",
        "    irf_results = []\n",
        "\n",
        "    # Align base data\n",
        "    # We need u_t and W_{t-1} aligned at index t\n",
        "    # If controls are Z_t, we shift them.\n",
        "    # Let's assume controls passed are Z_t, so we shift(1).\n",
        "    w_lag = controls.shift(1).add_suffix(\"_lag\")\n",
        "\n",
        "    rhs_base = pd.concat([innovations, w_lag], axis=1)\n",
        "    rhs_base = sm.add_constant(rhs_base)\n",
        "\n",
        "    for h in range(horizons + 1):\n",
        "        # LHS: y_{t+h}\n",
        "        # Shift outcome backwards by h to align t+h with t\n",
        "        lhs = outcome_series.shift(-h).rename(\"outcome_h\")\n",
        "\n",
        "        # Align\n",
        "        data = pd.concat([lhs, rhs_base], axis=1).dropna()\n",
        "\n",
        "        if len(data) < 20: # Safety check\n",
        "            continue\n",
        "\n",
        "        # Fit OLS with HAC\n",
        "        # Lag for HAC should be at least h\n",
        "        nw_lag = h + 1\n",
        "        model = sm.OLS(data[\"outcome_h\"], data.drop(columns=[\"outcome_h\"]))\n",
        "        results = model.fit(cov_type='HAC', cov_kwds={'maxlags': nw_lag})\n",
        "\n",
        "        # Extract coefficient on innovation\n",
        "        u_name = innovations.name\n",
        "        b_h = results.params[u_name]\n",
        "        se_h = results.bse[u_name]\n",
        "        ci = results.conf_int().loc[u_name]\n",
        "\n",
        "        irf_results.append({\n",
        "            \"Horizon\": h,\n",
        "            \"Response\": b_h,\n",
        "            \"SE\": se_h,\n",
        "            \"CI_Lower\": ci[0],\n",
        "            \"CI_Upper\": ci[1]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(irf_results).set_index(\"Horizon\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def perform_economic_analysis(\n",
        "    mspi_forecasts: pd.DataFrame,\n",
        "    market_aggregates: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the economic analysis: predictive regressions and local projections.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mspi_forecasts : pd.DataFrame\n",
        "        MSPI forecasts (p_mspi).\n",
        "    market_aggregates : pd.DataFrame\n",
        "        Market variables (R_mkt, sigma_mkt).\n",
        "    study_config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Analysis results.\n",
        "    \"\"\"\n",
        "    # Extract Parameters\n",
        "    econ_config = study_config[\"economic_analysis\"]\n",
        "    horizons = econ_config[\"local_projections\"].get(\"horizons_h\", 12)\n",
        "\n",
        "    # Prepare Data\n",
        "    mspi_col = \"MSPI\" if \"MSPI\" in mspi_forecasts.columns else \"p_mspi\"\n",
        "    mspi_series = mspi_forecasts[mspi_col]\n",
        "\n",
        "    # Intersection of indices\n",
        "    common_idx = mspi_series.index.intersection(market_aggregates.index)\n",
        "    mspi_series = mspi_series.loc[common_idx]\n",
        "    market_df = market_aggregates.loc[common_idx]\n",
        "\n",
        "    # 1. Predictive Regression\n",
        "    pred_vol_results = _estimate_volatility_prediction(\n",
        "        mspi_series, market_df, market_df[\"sigma_mkt\"]\n",
        "    )\n",
        "\n",
        "    # 2. Innovations\n",
        "    innovations = _construct_innovations(mspi_series, market_df)\n",
        "\n",
        "    # 3. Local Projections\n",
        "    # Outcome: Realized Volatility\n",
        "    lp_vol = _estimate_local_projections(\n",
        "        innovations, market_df[\"sigma_mkt\"], market_df, horizons\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"predictive_regression_vol\": pred_vol_results,\n",
        "        \"innovations\": innovations,\n",
        "        \"irf_volatility\": lp_vol\n",
        "    }\n"
      ],
      "metadata": {
        "id": "lgCjUYIxThz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestor Callable\n",
        "\n",
        "# ==============================================================================\n",
        "# Final Top-Level Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def execute_complete_mspi_research_pipeline(\n",
        "    df_crsp_daily: pd.DataFrame,\n",
        "    df_crsp_index: pd.DataFrame,\n",
        "    raw_study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete Market Stress Probability Index (MSPI) research pipeline,\n",
        "    from raw data ingestion to final economic analysis and inference.\n",
        "\n",
        "    This orchestrator enforces the strict sequential dependency graph required to\n",
        "    reproduce the results of 'Algorithmic Monitoring' (Schmitt, 2026). It manages\n",
        "    data flow from raw CRSP inputs through feature engineering, labeling,\n",
        "    hyperparameter tuning, out-of-sample forecasting, robustness checks,\n",
        "    performance evaluation, and economic impact analysis.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_crsp_daily : pd.DataFrame\n",
        "        Raw CRSP Daily Stock File (micro-data).\n",
        "    df_crsp_index : pd.DataFrame\n",
        "        Raw CRSP Daily Index File (market aggregate data).\n",
        "    raw_study_config : Dict[str, Any]\n",
        "        Configuration dictionary specifying all study parameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing the following keys:\n",
        "        - 'monthly_features': pd.DataFrame of X_t features.\n",
        "        - 'market_aggregates': pd.DataFrame of R_mkt and sigma_mkt.\n",
        "        - 'labels': pd.DataFrame of S_t and Y_next.\n",
        "        - 'mspi_forecasts': pd.DataFrame of MSPI out-of-sample forecasts.\n",
        "        - 'benchmark_forecasts': pd.DataFrame of Benchmark out-of-sample forecasts.\n",
        "        - 'unified_evaluation_panel': pd.DataFrame of all model forecasts (MSPI, Bench, RF, GB).\n",
        "        - 'discrimination_metrics': pd.DataFrame of AUC/PR-AUC.\n",
        "        - 'probability_metrics': pd.DataFrame of Brier/LogLoss/ECE.\n",
        "        - 'bootstrap_inference': pd.DataFrame of statistical significance tests.\n",
        "        - 'economic_analysis': Dict of predictive regressions, innovations, and IRFs.\n",
        "        - 'frozen_hyperparams': Dict of tuned hyperparameters.\n",
        "        - 'audit_log': Dict of detailed execution logs and statistics.\n",
        "    \"\"\"\n",
        "    # Initialize Audit Log\n",
        "    audit_log = {}\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 1: Configuration and Validation\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 1: Configuration and Validation\")\n",
        "\n",
        "    # Task 1: Validate Config\n",
        "    config_res = validate_study_config(raw_study_config)\n",
        "    study_config = config_res[\"validated_config\"]\n",
        "    audit_log[\"config_validation\"] = config_res[\"validation_report\"]\n",
        "\n",
        "    # Task 2: Validate Micro Data\n",
        "    val_daily = validate_crsp_daily(df_crsp_daily, study_config)\n",
        "    audit_log[\"validation_daily\"] = val_daily\n",
        "\n",
        "    # Task 3: Validate Index Data\n",
        "    val_index = validate_crsp_index(df_crsp_daily, df_crsp_index, study_config)\n",
        "    audit_log[\"validation_index\"] = val_index\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 2: Data Cleansing and Universe Construction\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 2: Data Cleansing and Universe Construction\")\n",
        "\n",
        "    # Task 4: Cleanse Micro Data\n",
        "    clean_daily_res = cleanse_crsp_daily(df_crsp_daily, study_config)\n",
        "    df_daily_clean = clean_daily_res[\"df_clean\"]\n",
        "    audit_log[\"cleansing_daily\"] = clean_daily_res[\"audit_log\"]\n",
        "\n",
        "    # Task 5: Cleanse Index Data\n",
        "    clean_index_res = cleanse_crsp_index(df_daily_clean, df_crsp_index, study_config)\n",
        "    df_index_clean = clean_index_res[\"df_index_clean\"]\n",
        "    audit_log[\"cleansing_index\"] = clean_index_res[\"audit_log\"]\n",
        "\n",
        "    # Task 6: Construct Eligible Universe\n",
        "    universe_res = construct_eligible_universe(df_daily_clean, study_config)\n",
        "    df_eligible = universe_res[\"df_eligible\"]\n",
        "    nd_series = universe_res[\"nd_series\"]\n",
        "    audit_log[\"universe_construction\"] = universe_res[\"audit_log\"]\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 3: Feature Engineering (Daily Signals)\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 3: Feature Engineering (Daily Signals)\")\n",
        "\n",
        "    # Task 7: Return Moments\n",
        "    moments = compute_daily_return_moments(df_eligible, study_config)\n",
        "\n",
        "    # Task 8: Tail Measures\n",
        "    tails = compute_daily_tail_measures(df_eligible, study_config)\n",
        "\n",
        "    # Task 9: Trading Proxies\n",
        "    proxies = compute_daily_trading_proxies(df_eligible, study_config)\n",
        "\n",
        "    # Merge Daily Statistics\n",
        "    # We align on DATE index. All should have same index from df_eligible.\n",
        "    daily_stats_all = pd.concat([moments, tails, proxies], axis=1)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 4: Monthly Aggregation and Labeling\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 4: Monthly Aggregation and Labeling\")\n",
        "\n",
        "    # Task 10: Aggregate Monthly Features\n",
        "    agg_res = aggregate_monthly_features(daily_stats_all, nd_series, study_config)\n",
        "    X_t = agg_res[\"X_t\"]\n",
        "    audit_log[\"monthly_aggregation\"] = agg_res[\"audit_log\"]\n",
        "\n",
        "    # Task 11: Construct Market Aggregates\n",
        "    market_res = construct_market_aggregates(df_index_clean, X_t, study_config)\n",
        "    aligned_panel = market_res[\"aligned_panel\"]\n",
        "    audit_log[\"market_aggregates\"] = market_res[\"audit_log\"]\n",
        "\n",
        "    # Task 12: Construct Stress Labels\n",
        "    label_res = construct_stress_labels(aligned_panel, study_config)\n",
        "    modeling_panel = label_res[\"modeling_panel\"]\n",
        "    audit_log[\"label_construction\"] = label_res[\"audit_log\"]\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 5: Model Training and Forecasting\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 5: Model Training and Forecasting\")\n",
        "\n",
        "    # Task 14: Tune Hyperparameters\n",
        "    # We pass the expanding_window_standardizer callable explicitly\n",
        "    tune_res = tune_hyperparameters(\n",
        "        modeling_panel,\n",
        "        expanding_window_standardizer,\n",
        "        study_config\n",
        "    )\n",
        "    frozen_params = tune_res\n",
        "    audit_log[\"hyperparameter_tuning\"] = tune_res[\"tuning_audit\"]\n",
        "\n",
        "    # Task 15: Forecast MSPI\n",
        "    mspi_forecasts = forecast_mspi(\n",
        "        modeling_panel,\n",
        "        frozen_params,\n",
        "        expanding_window_standardizer,\n",
        "        study_config\n",
        "    )\n",
        "\n",
        "    # Task 16: Forecast Benchmark\n",
        "    bench_forecasts = forecast_benchmark(\n",
        "        modeling_panel,\n",
        "        frozen_params,\n",
        "        expanding_window_standardizer,\n",
        "        study_config\n",
        "    )\n",
        "\n",
        "    # Task 18: Robustness Horse Race (RF/GB)\n",
        "    unified_evaluation_panel = run_robustness_horse_race(\n",
        "        modeling_panel,\n",
        "        mspi_forecasts,\n",
        "        bench_forecasts,\n",
        "        frozen_params,\n",
        "        expanding_window_standardizer,\n",
        "        study_config\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 6: Evaluation and Inference\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 6: Evaluation and Inference\")\n",
        "\n",
        "    # Task 19: Discrimination Metrics\n",
        "    discrim_metrics = compute_discrimination_metrics(unified_evaluation_panel)\n",
        "\n",
        "    # Task 20: Probability Metrics\n",
        "    prob_metrics = compute_probability_metrics(\n",
        "        unified_evaluation_panel, discrim_metrics, study_config\n",
        "    )\n",
        "\n",
        "    # Task 21: Bootstrap Inference\n",
        "    bootstrap_res = perform_bootstrap_inference(\n",
        "        unified_evaluation_panel, study_config\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 7: Economic Analysis\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 7: Economic Analysis\")\n",
        "\n",
        "    # Task 22: Economic Analysis\n",
        "    # We need market aggregates aligned with forecasts\n",
        "    # modeling_panel has R_mkt, sigma_mkt aligned with X_t\n",
        "    # We use the intersection of indices\n",
        "    econ_res = perform_economic_analysis(\n",
        "        mspi_forecasts,\n",
        "        modeling_panel[[\"R_mkt\", \"sigma_mkt\"]],\n",
        "        study_config\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Phase 8: Finalization\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"Phase 8: Finalization and Safety Checks\")\n",
        "\n",
        "    # Run Anti-Look-Ahead Checks\n",
        "    _run_anti_look_ahead_checks(mspi_forecasts, modeling_panel, audit_log)\n",
        "\n",
        "    # Compile Final Results\n",
        "    results = {\n",
        "        \"monthly_features\": X_t,\n",
        "        \"market_aggregates\": aligned_panel[[\"R_mkt\", \"sigma_mkt\"]],\n",
        "        \"labels\": modeling_panel[[\"S_t\", \"Y_next\"]],\n",
        "        \"mspi_forecasts\": mspi_forecasts,\n",
        "        \"benchmark_forecasts\": bench_forecasts,\n",
        "        \"unified_evaluation_panel\": unified_evaluation_panel,\n",
        "        \"discrimination_metrics\": discrim_metrics,\n",
        "        \"probability_metrics\": prob_metrics,\n",
        "        \"bootstrap_inference\": bootstrap_res,\n",
        "        \"economic_analysis\": econ_res,\n",
        "        \"frozen_hyperparams\": frozen_params,\n",
        "        \"audit_log\": audit_log\n",
        "    }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "LVk1P0_HYIa1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}